{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings('ignore','.*encoding is deprecated, Use raw=False instead..*')\n",
    "warnings.filterwarnings('ignore','.*the matrix subclass is not the recommended way to represent matrices or deal with linear algebra.*')\n",
    "warnings.filterwarnings('ignore','.*Precision and F-score are ill-defined.*')\n",
    "warnings.filterwarnings('ignore','.*Data with input dtype int64 was converted to float64 by StandardScaler.*')\n",
    "\n",
    "# importing required packages\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from spacy import displacy\n",
    "#from spacy.lang.en import English\n",
    "#parser = English()\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "#from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up display area to show dataframe in jupyter qtconsole\n",
    "\n",
    "#pd.set_option('display.height', 1000)\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "#pd.set_option('display.width', 1000)\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "#pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devanshi\\Desktop\\finalProject\\data\\asap-sas\n"
     ]
    }
   ],
   "source": [
    "myDir = Path.cwd().parents[0]\n",
    "dataFolder = myDir / 'data/asap-sas'\n",
    "ratingsFolder = myDir / 'data/ratings'\n",
    "figureFolder = myDir / 'figures'\n",
    "\n",
    "print(dataFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(dataFolder/'training.csv', header=0)  #read data into dataframe\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10895, 19)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only essay set 1\n",
    "df = df[~(df.subject == 'Biology')]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X & Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10895, 1) (10895,)\n"
     ]
    }
   ],
   "source": [
    "X = df[['EssayText','Score1']].copy()\n",
    "X.reset_index(drop=True,inplace=True)\n",
    "y= X.pop('Score1')\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, cohen_kappa_score \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "#from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, cohen_kappa_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_metrics(y_test, y_pred):\n",
    "    \n",
    "    #macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally)\n",
    "    macro_precision, macro_recall, macro_fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "\n",
    "    #micro-average will aggregate the contributions of all classes to compute the average metric\n",
    "    micro_precision, micro_recall, micro_fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "\n",
    "    #print(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "    cohens_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    return {'Macro_Precision':macro_precision, 'Macro_Recall': macro_recall,'Macro_fScore':macro_fscore,\n",
    "             'Micro_Precision':micro_precision, 'Micro_Recall':micro_recall,'Micro_fScore':micro_fscore,\n",
    "             'Cohens_Kappa':cohens_kappa}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Simple Baseline: Predict the most popular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (4296, 1) (4296,)\n",
      "Cross Validating\n",
      "Evaluating results\n",
      "1 (3666, 1) (3666,)\n",
      "Cross Validating\n",
      "Evaluating results\n",
      "1 (2933, 1) (2933,)\n",
      "Cross Validating\n",
      "Evaluating results\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Results\n",
      "Train: 2933 Test: 2933\n",
      "Evaluating results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Model</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Macro_Precision</th>\n",
       "      <th>Macro_Recall</th>\n",
       "      <th>Macro_fScore</th>\n",
       "      <th>Micro_Precision</th>\n",
       "      <th>Micro_Recall</th>\n",
       "      <th>Micro_fScore</th>\n",
       "      <th>Cohens_Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>None</td>\n",
       "      <td>Baseline_Content</td>\n",
       "      <td>0.117086</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.173299</td>\n",
       "      <td>0.351257</td>\n",
       "      <td>0.351257</td>\n",
       "      <td>0.351257</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Science</td>\n",
       "      <td>None</td>\n",
       "      <td>Baseline_Content</td>\n",
       "      <td>0.163939</td>\n",
       "      <td>0.242137</td>\n",
       "      <td>0.193528</td>\n",
       "      <td>0.327605</td>\n",
       "      <td>0.327605</td>\n",
       "      <td>0.327605</td>\n",
       "      <td>-0.016077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>None</td>\n",
       "      <td>Baseline_Content</td>\n",
       "      <td>0.178543</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.232534</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Overall</td>\n",
       "      <td>None</td>\n",
       "      <td>Baseline_Content</td>\n",
       "      <td>0.178543</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.232534</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Subject Model        Experiment  Macro_Precision  \\\n",
       "0                English  None  Baseline_Content         0.117086   \n",
       "1                Science  None  Baseline_Content         0.163939   \n",
       "2  English Language Arts  None  Baseline_Content         0.178543   \n",
       "3                Overall  None  Baseline_Content         0.178543   \n",
       "\n",
       "   Macro_Recall  Macro_fScore  Micro_Precision  Micro_Recall  Micro_fScore  \\\n",
       "0      0.333333      0.173299         0.351257      0.351257      0.351257   \n",
       "1      0.242137      0.193528         0.327605      0.327605      0.327605   \n",
       "2      0.333333      0.232534         0.535629      0.535629      0.535629   \n",
       "3      0.333333      0.232534         0.535629      0.535629      0.535629   \n",
       "\n",
       "   Cohens_Kappa  \n",
       "0      0.000000  \n",
       "1     -0.016077  \n",
       "2      0.000000  \n",
       "3      0.000000  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Empty array to store results\n",
    "tmp_results = []\n",
    "results_columns = ['Subject','Model','Experiment',\n",
    "                    'Macro_Precision','Macro_Recall','Macro_fScore',\n",
    "                    'Micro_Precision','Micro_Recall','Micro_fScore',\n",
    "                    'Cohens_Kappa']\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#Per Subject experiment\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "experiment_name = 'Baseline_Content'\n",
    "text_column = 'EssayText'\n",
    "\n",
    "\n",
    "for subject in list(df.subject.unique()):\n",
    "    \n",
    "    X = df[(df['subject'] == subject)][[text_column,'Score1']].copy()\n",
    "    X.reset_index(drop=True,inplace=True)\n",
    "    y= X.pop('Score1')\n",
    "    print(essay_set, X.shape, y.shape)\n",
    "    target_names = [str(i) for i in sorted(y.unique())] #Convert to string\n",
    "    \n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "\n",
    "    print('Cross Validating')\n",
    "    # data is an array with our already pre-processed dataset examples\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "        #print(\"Train:\", len(train_index), \"Test:\", len(test_index))\n",
    "        fold_X_train, fold_X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "        fold_y_train, fold_y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "        \n",
    "        most_common_class = Counter(fold_y_train).most_common(1)[0][0]\n",
    "        \n",
    "        ##For each fold, predict most common class as Baseline\n",
    "        fold_y_pred = [most_common_class]* len(fold_y_test)\n",
    "\n",
    "        y_test.extend(fold_y_test)\n",
    "        y_pred.extend(fold_y_pred)\n",
    "\n",
    "\n",
    "    print('Evaluating results')    \n",
    "    \n",
    "    myrow = {'Subject':subject, 'Model':None,'Experiment':experiment_name}\n",
    "    myrow.update(get_evaluation_metrics(y_test, y_pred))\n",
    "   \n",
    "    tmp_results.append(myrow)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#Repeat experiment for overall\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "print('Overall Results')\n",
    "print(\"Train:\", len(X), \"Test:\", len(y))\n",
    "# data is an array with our already pre-processed dataset examples\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "    \n",
    "    fold_X_train, fold_X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "    fold_y_train, fold_y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "\n",
    "    most_common_class = Counter(fold_y_train).most_common(1)[0][0]\n",
    "\n",
    "    ##For each fold, predict most common class as Baseline\n",
    "    fold_y_pred = [most_common_class]* len(fold_y_test)\n",
    "\n",
    "    y_test.extend(fold_y_test)\n",
    "    y_pred.extend(fold_y_pred)\n",
    "\n",
    "\n",
    "print('Evaluating results')    \n",
    "\n",
    "myrow = {'Subject':'Overall', 'Model':None,'Experiment':experiment_name}\n",
    "myrow.update(get_evaluation_metrics(y_test, y_pred))\n",
    "\n",
    "tmp_results.append(myrow)\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Get results, in a  DataFrame\n",
    "simple_baseline_results = pd.DataFrame(tmp_results, columns=results_columns)\n",
    "simple_baseline_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline per Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm  import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "random_state = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========English===========\n",
      "1 (4296, 1) (4296,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "==========Science===========\n",
      "1 (3666, 1) (3666,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "==========English Language Arts===========\n",
      "1 (2933, 1) (2933,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Model</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Macro_Precision</th>\n",
       "      <th>Macro_Recall</th>\n",
       "      <th>Macro_fScore</th>\n",
       "      <th>Micro_Precision</th>\n",
       "      <th>Micro_Recall</th>\n",
       "      <th>Micro_fScore</th>\n",
       "      <th>Cohens_Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.640662</td>\n",
       "      <td>0.642357</td>\n",
       "      <td>0.640420</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.469684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>NB</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.521068</td>\n",
       "      <td>0.520370</td>\n",
       "      <td>0.515569</td>\n",
       "      <td>0.524907</td>\n",
       "      <td>0.524907</td>\n",
       "      <td>0.524907</td>\n",
       "      <td>0.284147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.607577</td>\n",
       "      <td>0.608557</td>\n",
       "      <td>0.607713</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.418433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>RF</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.578719</td>\n",
       "      <td>0.579514</td>\n",
       "      <td>0.576081</td>\n",
       "      <td>0.585661</td>\n",
       "      <td>0.585661</td>\n",
       "      <td>0.585661</td>\n",
       "      <td>0.375639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Science</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.570430</td>\n",
       "      <td>0.560935</td>\n",
       "      <td>0.565255</td>\n",
       "      <td>0.573923</td>\n",
       "      <td>0.573923</td>\n",
       "      <td>0.573923</td>\n",
       "      <td>0.403480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Science</td>\n",
       "      <td>NB</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.578638</td>\n",
       "      <td>0.515187</td>\n",
       "      <td>0.456149</td>\n",
       "      <td>0.456356</td>\n",
       "      <td>0.456356</td>\n",
       "      <td>0.456356</td>\n",
       "      <td>0.297568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Science</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.528553</td>\n",
       "      <td>0.526025</td>\n",
       "      <td>0.527136</td>\n",
       "      <td>0.534643</td>\n",
       "      <td>0.534643</td>\n",
       "      <td>0.534643</td>\n",
       "      <td>0.350862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Science</td>\n",
       "      <td>RF</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.514673</td>\n",
       "      <td>0.480506</td>\n",
       "      <td>0.488441</td>\n",
       "      <td>0.524550</td>\n",
       "      <td>0.524550</td>\n",
       "      <td>0.524550</td>\n",
       "      <td>0.322536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.532803</td>\n",
       "      <td>0.505893</td>\n",
       "      <td>0.512391</td>\n",
       "      <td>0.596659</td>\n",
       "      <td>0.596659</td>\n",
       "      <td>0.596659</td>\n",
       "      <td>0.291037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>NB</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.527080</td>\n",
       "      <td>0.527960</td>\n",
       "      <td>0.503232</td>\n",
       "      <td>0.528128</td>\n",
       "      <td>0.528128</td>\n",
       "      <td>0.528128</td>\n",
       "      <td>0.261999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.498826</td>\n",
       "      <td>0.494209</td>\n",
       "      <td>0.496090</td>\n",
       "      <td>0.557791</td>\n",
       "      <td>0.557791</td>\n",
       "      <td>0.557791</td>\n",
       "      <td>0.249021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>RF</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.560171</td>\n",
       "      <td>0.497152</td>\n",
       "      <td>0.501398</td>\n",
       "      <td>0.616434</td>\n",
       "      <td>0.616434</td>\n",
       "      <td>0.616434</td>\n",
       "      <td>0.300562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Subject Model                 Experiment  Macro_Precision  \\\n",
       "0                 English  LogR  Baseline_perModel_Content         0.640662   \n",
       "1                 English    NB  Baseline_perModel_Content         0.521068   \n",
       "2                 English   SVM  Baseline_perModel_Content         0.607577   \n",
       "3                 English    RF  Baseline_perModel_Content         0.578719   \n",
       "4                 Science  LogR  Baseline_perModel_Content         0.570430   \n",
       "5                 Science    NB  Baseline_perModel_Content         0.578638   \n",
       "6                 Science   SVM  Baseline_perModel_Content         0.528553   \n",
       "7                 Science    RF  Baseline_perModel_Content         0.514673   \n",
       "8   English Language Arts  LogR  Baseline_perModel_Content         0.532803   \n",
       "9   English Language Arts    NB  Baseline_perModel_Content         0.527080   \n",
       "10  English Language Arts   SVM  Baseline_perModel_Content         0.498826   \n",
       "11  English Language Arts    RF  Baseline_perModel_Content         0.560171   \n",
       "\n",
       "    Macro_Recall  Macro_fScore  Micro_Precision  Micro_Recall  Micro_fScore  \\\n",
       "0       0.642357      0.640420         0.647579      0.647579      0.647579   \n",
       "1       0.520370      0.515569         0.524907      0.524907      0.524907   \n",
       "2       0.608557      0.607713         0.613128      0.613128      0.613128   \n",
       "3       0.579514      0.576081         0.585661      0.585661      0.585661   \n",
       "4       0.560935      0.565255         0.573923      0.573923      0.573923   \n",
       "5       0.515187      0.456149         0.456356      0.456356      0.456356   \n",
       "6       0.526025      0.527136         0.534643      0.534643      0.534643   \n",
       "7       0.480506      0.488441         0.524550      0.524550      0.524550   \n",
       "8       0.505893      0.512391         0.596659      0.596659      0.596659   \n",
       "9       0.527960      0.503232         0.528128      0.528128      0.528128   \n",
       "10      0.494209      0.496090         0.557791      0.557791      0.557791   \n",
       "11      0.497152      0.501398         0.616434      0.616434      0.616434   \n",
       "\n",
       "    Cohens_Kappa  \n",
       "0       0.469684  \n",
       "1       0.284147  \n",
       "2       0.418433  \n",
       "3       0.375639  \n",
       "4       0.403480  \n",
       "5       0.297568  \n",
       "6       0.350862  \n",
       "7       0.322536  \n",
       "8       0.291037  \n",
       "9       0.261999  \n",
       "10      0.249021  \n",
       "11      0.300562  "
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#======================EXPERIMENT CONFIG===================\n",
    "experiment_name = 'Baseline_perModel_Content'\n",
    "text_column = 'EssayText'\n",
    "\n",
    "#Empty array to store results\n",
    "tmp_results = []\n",
    "results_columns = ['Subject','Model','Experiment',\n",
    "                    'Macro_Precision','Macro_Recall','Macro_fScore',\n",
    "                    'Micro_Precision','Micro_Recall','Micro_fScore',\n",
    "                    'Cohens_Kappa']\n",
    "\n",
    "#Models and Vectorisers\n",
    "count_vectorizer = CountVectorizer()\n",
    "models = [('LogR', LogisticRegression()),\n",
    "         ('NB', MultinomialNB()),\n",
    "          ('SVM',LinearSVC(random_state=random_state)),\n",
    "         ('RF',RandomForestClassifier(random_state=random_state))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#Per Subject experiment\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for subject in list(df.subject.unique()):\n",
    "    print('\\n=========={}==========='.format(subject))\n",
    "    \n",
    "    X = df[(df['subject'] == subject)][[text_column,'Score1']].copy()\n",
    "    X.reset_index(drop=True,inplace=True)\n",
    "    y= X.pop('Score1')\n",
    "    print(essay_set, X.shape, y.shape)\n",
    "    target_names = [str(i) for i in sorted(y.unique())] #Convert to string\n",
    "    \n",
    "    # For each model\n",
    "    for model_name, clf in models:\n",
    "        print('\\n------------{}-------------'.format(model_name))\n",
    "\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "\n",
    "        print('Cross Validating...')\n",
    "        # data is an array with our already pre-processed dataset examples\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "            #print(\"Train:\", len(train_index), \"Test:\", len(test_index))\n",
    "            fold_X_train, fold_X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "            fold_y_train, fold_y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "\n",
    "            fold_X_train = fold_X_train[text_column]\n",
    "            fold_X_test =  fold_X_test[text_column]\n",
    "\n",
    "            #Make pipeline to train classifier\n",
    "            pipe = make_pipeline(count_vectorizer, clf)\n",
    "            pipe.fit(fold_X_train, fold_y_train)\n",
    "\n",
    "            ##For each fold, predict most common class as Baseline\n",
    "            fold_y_pred = pipe.predict(fold_X_test)\n",
    "\n",
    "            y_test.extend(fold_y_test)\n",
    "            y_pred.extend(fold_y_pred)\n",
    "\n",
    "\n",
    "        print('Evaluating results...')    \n",
    "\n",
    "        myrow = {'Subject':subject, 'Model':model_name, 'Experiment':experiment_name}\n",
    "        myrow.update(get_evaluation_metrics(y_test, y_pred))\n",
    "\n",
    "        tmp_results.append(myrow)\n",
    "\n",
    "    #     conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    #     fig, ax = plt.subplots(figsize=(5,5))\n",
    "    #     sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap=\"YlGnBu\")\n",
    "\n",
    "    #     plt.title('Set {}: {}'.format(str(essay_set),model_name))\n",
    "    #     plt.ylabel('Actual Class')\n",
    "    #     plt.xlabel('Predicted Class')\n",
    "\n",
    "    #     print(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set)))\n",
    "    #     plt.savefig(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set))\", \n",
    "    #                 format=\"svg\", bbox_inches='tight')\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Get results, in a  DataFrame\n",
    "baseline_mdl_results = pd.DataFrame(tmp_results, columns=results_columns)\n",
    "baseline_mdl_results              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Make plots\n",
    "g = sns.catplot(x=\"Model\", y=\"Micro_fScore\", data=baseline_mdl_results, \n",
    "                col='Subject',hue=\"Model\", kind=\"bar\", \n",
    "                palette=\"husl\", dodge=False, height=4)\n",
    "g.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle('Micro f-score of Models (Baseline)') # can also get the figure from plt.gcf()\n",
    "\n",
    "plt.savefig(figureFolder/\"baseline_content/Baseline_perModel_Content.svg\", format=\"svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========English===========\n",
      "1 (4296, 1) (4296,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.63      0.63      1509\n",
      "          1       0.43      0.45      0.44      1340\n",
      "          2       0.58      0.55      0.56      1447\n",
      "\n",
      "avg / total       0.55      0.55      0.55      4296\n",
      "\n",
      "accuracy: 0.549\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.49      0.53      1509\n",
      "          1       0.42      0.37      0.39      1340\n",
      "          2       0.49      0.62      0.54      1447\n",
      "\n",
      "avg / total       0.50      0.49      0.49      4296\n",
      "\n",
      "accuracy: 0.494\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.62      0.62      1509\n",
      "          1       0.41      0.43      0.42      1340\n",
      "          2       0.54      0.50      0.52      1447\n",
      "\n",
      "avg / total       0.52      0.52      0.52      4296\n",
      "\n",
      "accuracy: 0.521\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.76      0.69      1509\n",
      "          1       0.52      0.45      0.48      1340\n",
      "          2       0.66      0.60      0.63      1447\n",
      "\n",
      "avg / total       0.60      0.61      0.60      4296\n",
      "\n",
      "accuracy: 0.608\n",
      "\n",
      "==========Science===========\n",
      "1 (3666, 1) (3666,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.41      0.44       665\n",
      "          1       0.48      0.52      0.50      1236\n",
      "          2       0.48      0.48      0.48      1246\n",
      "          3       0.42      0.41      0.42       519\n",
      "\n",
      "avg / total       0.47      0.47      0.47      3666\n",
      "\n",
      "accuracy: 0.471\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.35      0.38       665\n",
      "          1       0.49      0.36      0.42      1236\n",
      "          2       0.46      0.38      0.42      1246\n",
      "          3       0.30      0.66      0.41       519\n",
      "\n",
      "avg / total       0.44      0.41      0.41      3666\n",
      "\n",
      "accuracy: 0.409\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.37      0.39       665\n",
      "          1       0.46      0.49      0.47      1236\n",
      "          2       0.46      0.45      0.46      1246\n",
      "          3       0.36      0.39      0.38       519\n",
      "\n",
      "avg / total       0.44      0.44      0.44      3666\n",
      "\n",
      "accuracy: 0.439\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.54      0.56       665\n",
      "          1       0.57      0.61      0.59      1236\n",
      "          2       0.54      0.60      0.57      1246\n",
      "          3       0.47      0.29      0.36       519\n",
      "\n",
      "avg / total       0.55      0.55      0.55      3666\n",
      "\n",
      "accuracy: 0.551\n",
      "\n",
      "==========English Language Arts===========\n",
      "1 (2933, 1) (2933,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.48      0.47       902\n",
      "          1       0.61      0.59      0.60      1571\n",
      "          2       0.29      0.31      0.30       460\n",
      "\n",
      "avg / total       0.52      0.51      0.51      2933\n",
      "\n",
      "accuracy: 0.510\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.42      0.47       902\n",
      "          1       0.62      0.49      0.55      1571\n",
      "          2       0.24      0.52      0.33       460\n",
      "\n",
      "avg / total       0.54      0.47      0.49      2933\n",
      "\n",
      "accuracy: 0.473\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.47      0.46       902\n",
      "          1       0.61      0.54      0.57      1571\n",
      "          2       0.27      0.35      0.31       460\n",
      "\n",
      "avg / total       0.51      0.49      0.50      2933\n",
      "\n",
      "accuracy: 0.491\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.59      0.59       902\n",
      "          1       0.64      0.78      0.70      1571\n",
      "          2       0.51      0.15      0.24       460\n",
      "\n",
      "avg / total       0.61      0.62      0.60      2933\n",
      "\n",
      "accuracy: 0.621\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Model</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Macro_Precision</th>\n",
       "      <th>Macro_Recall</th>\n",
       "      <th>Macro_fScore</th>\n",
       "      <th>Micro_Precision</th>\n",
       "      <th>Micro_Recall</th>\n",
       "      <th>Micro_fScore</th>\n",
       "      <th>Cohens_Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.545938</td>\n",
       "      <td>0.544969</td>\n",
       "      <td>0.545306</td>\n",
       "      <td>0.548650</td>\n",
       "      <td>0.548650</td>\n",
       "      <td>0.548650</td>\n",
       "      <td>0.322357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>NB</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.492797</td>\n",
       "      <td>0.491123</td>\n",
       "      <td>0.487746</td>\n",
       "      <td>0.494181</td>\n",
       "      <td>0.494181</td>\n",
       "      <td>0.494181</td>\n",
       "      <td>0.240044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.519280</td>\n",
       "      <td>0.517690</td>\n",
       "      <td>0.518098</td>\n",
       "      <td>0.521415</td>\n",
       "      <td>0.521415</td>\n",
       "      <td>0.521415</td>\n",
       "      <td>0.281662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>RF</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.602201</td>\n",
       "      <td>0.602172</td>\n",
       "      <td>0.598936</td>\n",
       "      <td>0.608240</td>\n",
       "      <td>0.608240</td>\n",
       "      <td>0.608240</td>\n",
       "      <td>0.409845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Science</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.463775</td>\n",
       "      <td>0.454899</td>\n",
       "      <td>0.458491</td>\n",
       "      <td>0.470813</td>\n",
       "      <td>0.470813</td>\n",
       "      <td>0.470813</td>\n",
       "      <td>0.258076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Science</td>\n",
       "      <td>NB</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.414924</td>\n",
       "      <td>0.438011</td>\n",
       "      <td>0.404991</td>\n",
       "      <td>0.408620</td>\n",
       "      <td>0.408620</td>\n",
       "      <td>0.408620</td>\n",
       "      <td>0.209068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Science</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.424567</td>\n",
       "      <td>0.423259</td>\n",
       "      <td>0.423348</td>\n",
       "      <td>0.438625</td>\n",
       "      <td>0.438625</td>\n",
       "      <td>0.438625</td>\n",
       "      <td>0.216866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Science</td>\n",
       "      <td>RF</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.538449</td>\n",
       "      <td>0.512707</td>\n",
       "      <td>0.519824</td>\n",
       "      <td>0.551009</td>\n",
       "      <td>0.551009</td>\n",
       "      <td>0.551009</td>\n",
       "      <td>0.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.454674</td>\n",
       "      <td>0.458528</td>\n",
       "      <td>0.456362</td>\n",
       "      <td>0.510058</td>\n",
       "      <td>0.510058</td>\n",
       "      <td>0.510058</td>\n",
       "      <td>0.183961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>NB</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.466218</td>\n",
       "      <td>0.478122</td>\n",
       "      <td>0.450467</td>\n",
       "      <td>0.473236</td>\n",
       "      <td>0.473236</td>\n",
       "      <td>0.473236</td>\n",
       "      <td>0.186364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.443836</td>\n",
       "      <td>0.453887</td>\n",
       "      <td>0.446750</td>\n",
       "      <td>0.490624</td>\n",
       "      <td>0.490624</td>\n",
       "      <td>0.490624</td>\n",
       "      <td>0.168972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>RF</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.582598</td>\n",
       "      <td>0.506011</td>\n",
       "      <td>0.510237</td>\n",
       "      <td>0.620866</td>\n",
       "      <td>0.620866</td>\n",
       "      <td>0.620866</td>\n",
       "      <td>0.312198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Subject Model                         Experiment  \\\n",
       "0                 English  LogR  Removed_Stopword_perModel_Content   \n",
       "1                 English    NB  Removed_Stopword_perModel_Content   \n",
       "2                 English   SVM  Removed_Stopword_perModel_Content   \n",
       "3                 English    RF  Removed_Stopword_perModel_Content   \n",
       "4                 Science  LogR  Removed_Stopword_perModel_Content   \n",
       "5                 Science    NB  Removed_Stopword_perModel_Content   \n",
       "6                 Science   SVM  Removed_Stopword_perModel_Content   \n",
       "7                 Science    RF  Removed_Stopword_perModel_Content   \n",
       "8   English Language Arts  LogR  Removed_Stopword_perModel_Content   \n",
       "9   English Language Arts    NB  Removed_Stopword_perModel_Content   \n",
       "10  English Language Arts   SVM  Removed_Stopword_perModel_Content   \n",
       "11  English Language Arts    RF  Removed_Stopword_perModel_Content   \n",
       "\n",
       "    Macro_Precision  Macro_Recall  Macro_fScore  Micro_Precision  \\\n",
       "0          0.545938      0.544969      0.545306         0.548650   \n",
       "1          0.492797      0.491123      0.487746         0.494181   \n",
       "2          0.519280      0.517690      0.518098         0.521415   \n",
       "3          0.602201      0.602172      0.598936         0.608240   \n",
       "4          0.463775      0.454899      0.458491         0.470813   \n",
       "5          0.414924      0.438011      0.404991         0.408620   \n",
       "6          0.424567      0.423259      0.423348         0.438625   \n",
       "7          0.538449      0.512707      0.519824         0.551009   \n",
       "8          0.454674      0.458528      0.456362         0.510058   \n",
       "9          0.466218      0.478122      0.450467         0.473236   \n",
       "10         0.443836      0.453887      0.446750         0.490624   \n",
       "11         0.582598      0.506011      0.510237         0.620866   \n",
       "\n",
       "    Micro_Recall  Micro_fScore  Cohens_Kappa  \n",
       "0       0.548650      0.548650      0.322357  \n",
       "1       0.494181      0.494181      0.240044  \n",
       "2       0.521415      0.521415      0.281662  \n",
       "3       0.608240      0.608240      0.409845  \n",
       "4       0.470813      0.470813      0.258076  \n",
       "5       0.408620      0.408620      0.209068  \n",
       "6       0.438625      0.438625      0.216866  \n",
       "7       0.551009      0.551009      0.364000  \n",
       "8       0.510058      0.510058      0.183961  \n",
       "9       0.473236      0.473236      0.186364  \n",
       "10      0.490624      0.490624      0.168972  \n",
       "11      0.620866      0.620866      0.312198  "
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#======================EXPERIMENT CONFIG===================\n",
    "experiment_name = 'Removed_Stopword_perModel_Content'\n",
    "text_column = 'EssayText'\n",
    "\n",
    "#Empty array to store results\n",
    "tmp_results = []\n",
    "results_columns = ['Subject','Model','Experiment',\n",
    "                    'Macro_Precision','Macro_Recall','Macro_fScore',\n",
    "                    'Micro_Precision','Micro_Recall','Micro_fScore',\n",
    "                    'Cohens_Kappa']\n",
    "\n",
    "#Models and Vectorisers\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "models = [('LogR', LogisticRegression()),\n",
    "         ('NB', MultinomialNB()),\n",
    "          ('SVM',LinearSVC(random_state=random_state)),\n",
    "         ('RF',RandomForestClassifier(random_state=random_state))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#Per Subject experiment\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for subject in list(df.subject.unique()):\n",
    "    print('\\n=========={}==========='.format(subject))\n",
    "    \n",
    "    X = df[(df['subject'] == subject)][[text_column,'Score1']].copy()\n",
    "    X.reset_index(drop=True,inplace=True)\n",
    "    y= X.pop('Score1')\n",
    "    print(essay_set, X.shape, y.shape)\n",
    "    target_names = [str(i) for i in sorted(y.unique())] #Convert to string\n",
    "    \n",
    "    \n",
    "    # For each model\n",
    "    for model_name, clf in models:\n",
    "        print('\\n------------{}-------------'.format(model_name))\n",
    "\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "\n",
    "        print('Cross Validating...')\n",
    "        # data is an array with our already pre-processed dataset examples\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "            #print(\"Train:\", len(train_index), \"Test:\", len(test_index))\n",
    "            fold_X_train, fold_X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "            fold_y_train, fold_y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "\n",
    "            fold_X_train = fold_X_train[text_column]\n",
    "            fold_X_test =  fold_X_test[text_column]\n",
    "\n",
    "            #Make pipeline to train classifier\n",
    "            pipe = make_pipeline(count_vectorizer,scaler, clf)\n",
    "            pipe.fit(fold_X_train, fold_y_train)\n",
    "\n",
    "            ##For each fold, predict most common class as Baseline\n",
    "            fold_y_pred = pipe.predict(fold_X_test)\n",
    "\n",
    "            y_test.extend(fold_y_test)\n",
    "            y_pred.extend(fold_y_pred)\n",
    "\n",
    "\n",
    "        print('Evaluating results...')    \n",
    "\n",
    "        myrow = {'Subject':subject, 'Model':model_name, 'Experiment':experiment_name}\n",
    "        myrow.update(get_evaluation_metrics(y_test, y_pred))\n",
    "\n",
    "        tmp_results.append(myrow)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "        print(report)\n",
    "        print(\"accuracy: {:0.3f}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "    #     conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    #     fig, ax = plt.subplots(figsize=(5,5))\n",
    "    #     sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap=\"YlGnBu\")\n",
    "\n",
    "    #     plt.title('Set {}: {}'.format(str(essay_set),model_name))\n",
    "    #     plt.ylabel('Actual Class')\n",
    "    #     plt.xlabel('Predicted Class')\n",
    "\n",
    "    #     print(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set)))\n",
    "    #     plt.savefig(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set))\", \n",
    "    #                 format=\"svg\", bbox_inches='tight')\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Get results, in a  DataFrame\n",
    "no_stopword_results = pd.DataFrame(tmp_results, columns=results_columns)\n",
    "no_stopword_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw plots\n",
    "g = sns.catplot(x=\"Model\", y=\"Micro_fScore\", data=no_stopword_results, \n",
    "                col='Subject',hue=\"Model\", kind=\"bar\", \n",
    "                palette=\"husl\", dodge=False, height=4)\n",
    "g.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "\n",
    "plt.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle('Micro f-score of Models after Stopword Removal') # can also get the figure from plt.gcf()\n",
    "\n",
    "plt.savefig(figureFolder/\"baseline_content/Stopword_Removal_Content.svg\", format=\"svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========English===========\n",
      "1 (4296, 1) (4296,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "==========Science===========\n",
      "1 (3666, 1) (3666,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "==========English Language Arts===========\n",
      "1 (2933, 1) (2933,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Smote Sampling...\n",
      "Evaluating results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Model</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Macro_Precision</th>\n",
       "      <th>Macro_Recall</th>\n",
       "      <th>Macro_fScore</th>\n",
       "      <th>Micro_Precision</th>\n",
       "      <th>Micro_Recall</th>\n",
       "      <th>Micro_fScore</th>\n",
       "      <th>Cohens_Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>LogR</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.640518</td>\n",
       "      <td>0.642417</td>\n",
       "      <td>0.640940</td>\n",
       "      <td>0.647346</td>\n",
       "      <td>0.647346</td>\n",
       "      <td>0.647346</td>\n",
       "      <td>0.469643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>NB</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.533516</td>\n",
       "      <td>0.532202</td>\n",
       "      <td>0.527143</td>\n",
       "      <td>0.536546</td>\n",
       "      <td>0.536546</td>\n",
       "      <td>0.536546</td>\n",
       "      <td>0.302019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>SVM</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.599997</td>\n",
       "      <td>0.601075</td>\n",
       "      <td>0.600411</td>\n",
       "      <td>0.605447</td>\n",
       "      <td>0.605447</td>\n",
       "      <td>0.605447</td>\n",
       "      <td>0.407059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>RF</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.619587</td>\n",
       "      <td>0.617187</td>\n",
       "      <td>0.615812</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>0.431234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Science</td>\n",
       "      <td>LogR</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.549107</td>\n",
       "      <td>0.578572</td>\n",
       "      <td>0.557481</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>0.399908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Science</td>\n",
       "      <td>NB</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.559225</td>\n",
       "      <td>0.518859</td>\n",
       "      <td>0.454564</td>\n",
       "      <td>0.450082</td>\n",
       "      <td>0.450082</td>\n",
       "      <td>0.450082</td>\n",
       "      <td>0.296352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Science</td>\n",
       "      <td>SVM</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.505794</td>\n",
       "      <td>0.520664</td>\n",
       "      <td>0.511841</td>\n",
       "      <td>0.515003</td>\n",
       "      <td>0.515003</td>\n",
       "      <td>0.515003</td>\n",
       "      <td>0.331538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Science</td>\n",
       "      <td>RF</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.514152</td>\n",
       "      <td>0.511597</td>\n",
       "      <td>0.512561</td>\n",
       "      <td>0.528914</td>\n",
       "      <td>0.528914</td>\n",
       "      <td>0.528914</td>\n",
       "      <td>0.342516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>LogR</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.515213</td>\n",
       "      <td>0.537382</td>\n",
       "      <td>0.518998</td>\n",
       "      <td>0.555063</td>\n",
       "      <td>0.555063</td>\n",
       "      <td>0.555063</td>\n",
       "      <td>0.292701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>NB</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.532736</td>\n",
       "      <td>0.541782</td>\n",
       "      <td>0.475811</td>\n",
       "      <td>0.474599</td>\n",
       "      <td>0.474599</td>\n",
       "      <td>0.474599</td>\n",
       "      <td>0.249609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>SVM</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.485768</td>\n",
       "      <td>0.500217</td>\n",
       "      <td>0.489727</td>\n",
       "      <td>0.532561</td>\n",
       "      <td>0.532561</td>\n",
       "      <td>0.532561</td>\n",
       "      <td>0.243436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>RF</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.533460</td>\n",
       "      <td>0.505246</td>\n",
       "      <td>0.509233</td>\n",
       "      <td>0.591204</td>\n",
       "      <td>0.591204</td>\n",
       "      <td>0.591204</td>\n",
       "      <td>0.285827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Subject Model              Experiment  Macro_Precision  \\\n",
       "0                 English  LogR  SMOTE_perModel_Content         0.640518   \n",
       "1                 English    NB  SMOTE_perModel_Content         0.533516   \n",
       "2                 English   SVM  SMOTE_perModel_Content         0.599997   \n",
       "3                 English    RF  SMOTE_perModel_Content         0.619587   \n",
       "4                 Science  LogR  SMOTE_perModel_Content         0.549107   \n",
       "5                 Science    NB  SMOTE_perModel_Content         0.559225   \n",
       "6                 Science   SVM  SMOTE_perModel_Content         0.505794   \n",
       "7                 Science    RF  SMOTE_perModel_Content         0.514152   \n",
       "8   English Language Arts  LogR  SMOTE_perModel_Content         0.515213   \n",
       "9   English Language Arts    NB  SMOTE_perModel_Content         0.532736   \n",
       "10  English Language Arts   SVM  SMOTE_perModel_Content         0.485768   \n",
       "11  English Language Arts    RF  SMOTE_perModel_Content         0.533460   \n",
       "\n",
       "    Macro_Recall  Macro_fScore  Micro_Precision  Micro_Recall  Micro_fScore  \\\n",
       "0       0.642417      0.640940         0.647346      0.647346      0.647346   \n",
       "1       0.532202      0.527143         0.536546      0.536546      0.536546   \n",
       "2       0.601075      0.600411         0.605447      0.605447      0.605447   \n",
       "3       0.617187      0.615812         0.622207      0.622207      0.622207   \n",
       "4       0.578572      0.557481         0.559193      0.559193      0.559193   \n",
       "5       0.518859      0.454564         0.450082      0.450082      0.450082   \n",
       "6       0.520664      0.511841         0.515003      0.515003      0.515003   \n",
       "7       0.511597      0.512561         0.528914      0.528914      0.528914   \n",
       "8       0.537382      0.518998         0.555063      0.555063      0.555063   \n",
       "9       0.541782      0.475811         0.474599      0.474599      0.474599   \n",
       "10      0.500217      0.489727         0.532561      0.532561      0.532561   \n",
       "11      0.505246      0.509233         0.591204      0.591204      0.591204   \n",
       "\n",
       "    Cohens_Kappa  \n",
       "0       0.469643  \n",
       "1       0.302019  \n",
       "2       0.407059  \n",
       "3       0.431234  \n",
       "4       0.399908  \n",
       "5       0.296352  \n",
       "6       0.331538  \n",
       "7       0.342516  \n",
       "8       0.292701  \n",
       "9       0.249609  \n",
       "10      0.243436  \n",
       "11      0.285827  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#======================EXPERIMENT CONFIG===================\n",
    "experiment_name = 'SMOTE_perModel_Content'\n",
    "text_column = 'EssayText'\n",
    "target_names = [str(i) for i in sorted(y.unique())] #Convert to string\n",
    "\n",
    "#Empty array to store results\n",
    "tmp_results = []\n",
    "results_columns = ['Subject','Model','Experiment',\n",
    "                    'Macro_Precision','Macro_Recall','Macro_fScore',\n",
    "                    'Micro_Precision','Micro_Recall','Micro_fScore',\n",
    "                    'Cohens_Kappa']\n",
    "\n",
    "#Models and Vectorisers\n",
    "smote = SMOTE(ratio='minority',random_state=42)\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "models = [('LogR', LogisticRegression()),\n",
    "         ('NB', MultinomialNB()),\n",
    "          ('SVM',LinearSVC(random_state=random_state)),\n",
    "         ('RF',RandomForestClassifier(random_state=random_state))]\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#Per Subject experiment\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for subject in list(df.subject.unique()):\n",
    "    print('\\n=========={}==========='.format(subject))\n",
    "    \n",
    "    X = df[(df['subject'] == subject)][[text_column,'Score1']].copy()\n",
    "    X.reset_index(drop=True,inplace=True)\n",
    "    y= X.pop('Score1')\n",
    "    print(essay_set, X.shape, y.shape)\n",
    "    \n",
    "    \n",
    "    # For each model\n",
    "    for model_name, clf in models:\n",
    "        print('\\n------------{}-------------'.format(model_name))\n",
    "\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "\n",
    "        print('Cross Validating...')\n",
    "        # data is an array with our already pre-processed dataset examples\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "#             print(\"Train:\", len(train_index), \"Test:\", len(test_index))\n",
    "            fold_X_train, fold_X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "            fold_y_train, fold_y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "\n",
    "            #Apply tokeniser\n",
    "            fold_X_train =  count_vectorizer.fit_transform(fold_X_train['EssayText']).toarray()\n",
    "            #print(len(X_train))\n",
    "\n",
    "#             if fig_show :        \n",
    "#                 #Make plots\n",
    "#                 fig, axs = plt.subplots(1, len(target_names), figsize=(9, 3), sharey=True)\n",
    "#                 sns.countplot(x=fold_y_train, ax= axs[0], palette='RdYlGn')\n",
    "\n",
    "            #Apply SMOTE iteratively for n-1 classes to upsample minority class\n",
    "            for i in range(0,len(target_names)-1):\n",
    "                \n",
    "                print('Smote Sampling...')\n",
    "                fold_X_train, fold_y_train = smote.fit_sample(fold_X_train, fold_y_train)\n",
    "#                 print(\"After sampling:\", len(fold_X_train))\n",
    "\n",
    "#                 if fig_show:\n",
    "#                     sns.countplot(x=fold_y_train, ax= axs[i+1], palette='RdYlGn')\n",
    "                    \n",
    "           \n",
    "#             if fig_show:\n",
    "#                 fig.suptitle('Iterative SMOTE sampling of Classes in Set 1')\n",
    "#                 plt.savefig(figureFolder/\"SMOTE_upsampling_Set1.svg\", format=\"svg\", bbox_inches='tight')\n",
    "#                 plt.show()\n",
    "\n",
    "#             fig_show=False\n",
    "\n",
    "            #Finally, train on classifier\n",
    "            mdl = clf.fit(fold_X_train, fold_y_train)\n",
    "\n",
    "            #Make predictions on test data\n",
    "            fold_X_test = count_vectorizer.transform(fold_X_test['EssayText']).toarray()\n",
    "            fold_y_pred = mdl.predict(fold_X_test)\n",
    "\n",
    "            y_test.extend(fold_y_test)\n",
    "            y_pred.extend(fold_y_pred)\n",
    "            \n",
    "            \n",
    "\n",
    "        print('Evaluating results...')    \n",
    "\n",
    "        myrow = {'Subject':subject, 'Model':model_name, 'Experiment':experiment_name}\n",
    "        myrow.update(get_evaluation_metrics(y_test, y_pred))\n",
    "\n",
    "        tmp_results.append(myrow)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "        print(report)\n",
    "        print(\"accuracy: {:0.3f}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "    #     conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    #     fig, ax = plt.subplots(figsize=(5,5))\n",
    "    #     sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap=\"YlGnBu\")\n",
    "\n",
    "    #     plt.title('Set {}: {}'.format(str(essay_set),model_name))\n",
    "    #     plt.ylabel('Actual Class')\n",
    "    #     plt.xlabel('Predicted Class')\n",
    "\n",
    "    #     print(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set)))\n",
    "    #     plt.savefig(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set))\", \n",
    "    #                 format=\"svg\", bbox_inches='tight')\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Get results, in a  DataFrame\n",
    "smote_sampling_results = pd.DataFrame(tmp_results, columns=results_columns)\n",
    "smote_sampling_results              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results to Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(figureFolder/'baseline_content/Baseline_Content_Words.xlsx')\n",
    "simple_baseline_results.to_excel(writer,'Simple_Baseline',index=False)\n",
    "baseline_mdl_results.to_excel(writer,'Baseline_per_model',index=False)\n",
    "no_stopword_results.to_excel(writer,'Removed_Stopwords',index=False)\n",
    "smote_sampling_results.to_excel(writer,'Smote_perModel', index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments around Features and Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Custom features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) BoW (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (5,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) TF.IDF (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION BASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1339, 1) (1339,)\n",
      "Train: 1069 Test: 270\n",
      "(0.4535262298428251, 0.4392591015541835, 0.4329341058694658, None)\n",
      "(0.44814814814814813, 0.44814814814814813, 0.44814814814814813, None)\n",
      "(0.44540888919129373, 0.44814814814814813, 0.43417279772617495, None)\n",
      "0.24774210437741917\n",
      "Train: 1070 Test: 269\n",
      "(0.3831232492997199, 0.3890151515151515, 0.38478021978021976, None)\n",
      "(0.3940520446096654, 0.3940520446096654, 0.3940520446096654, None)\n",
      "(0.386872221007362, 0.3940520446096654, 0.3891923689693206, None)\n",
      "0.1832845940358001\n",
      "Train: 1071 Test: 268\n",
      "(0.42422500425821835, 0.41681792103478854, 0.4096899819898355, None)\n",
      "(0.4291044776119403, 0.4291044776119403, 0.4291044776119403, None)\n",
      "(0.4235424182104297, 0.4291044776119403, 0.4164891921006229, None)\n",
      "0.2258283772302463\n",
      "Train: 1073 Test: 266\n",
      "(0.4558498399359744, 0.46317904468114, 0.4547386851944863, None)\n",
      "(0.46616541353383456, 0.46616541353383456, 0.46616541353383456, None)\n",
      "(0.45407817262243244, 0.46616541353383456, 0.45534104598512853, None)\n",
      "0.27851317020992117\n",
      "Train: 1073 Test: 266\n",
      "(0.4527128427128427, 0.4645752090487554, 0.4433417023580958, None)\n",
      "(0.46616541353383456, 0.46616541353383456, 0.46616541353383456, None)\n",
      "(0.4493721316277707, 0.46616541353383456, 0.44332097412339, None)\n",
      "0.27825123246837624\n"
     ]
    }
   ],
   "source": [
    "X = set_1[['function_based_text','Score1']].copy()\n",
    "X.reset_index(drop=True,inplace=True)\n",
    "y= X.pop('Score1')\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "#Initialise vectoriser\n",
    "count_vectorizer = CountVectorizer()\n",
    "clf = LogisticRegression() #OvR classifier\n",
    "target_names = [str(i) for i in sorted(y.unique())] #Convert to string\n",
    "\n",
    "\n",
    "# data is an array with our already pre-processed dataset examples\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    print(\"Train:\", len(train_index), \"Test:\", len(test_index))\n",
    "    X_train, X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "    y_train, y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "    \n",
    "    #Make pipeline to train classifier\n",
    "    pipe = make_pipeline(count_vectorizer, clf)\n",
    "    pipe.fit(X_train['function_based_text'], y_train)\n",
    "    \n",
    "    #Make predictions on test data\n",
    "    y_pred = pipe.predict(X_test['function_based_text'])\n",
    "    \n",
    "    print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
    "    print(precision_recall_fscore_support(y_test, y_pred, average='micro'))\n",
    "    print(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "    print(cohen_kappa_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.68      0.58       301\n",
      "          1       0.32      0.25      0.28       348\n",
      "          2       0.42      0.31      0.36       417\n",
      "          3       0.38      0.47      0.42       273\n",
      "\n",
      "avg / total       0.40      0.41      0.40      1339\n",
      "\n",
      "accuracy: 0.412\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "print(report)\n",
    "print(\"accuracy: {:0.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap=\"YlGnBu\")\n",
    "\n",
    "plt.title('Set {}: {}'.format('1','LogR'))\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Features & Prepare X & Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.SparseDataFrame(data=merged_2, columns=feature_names)\n",
    "X_train.head()\n",
    "\n",
    "#X_train = csr_matrix(merged_2) \n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1170,)\n"
     ]
    }
   ],
   "source": [
    "#y_train_tmp = pd.DataFrame(y_train_tmp)\n",
    "# y_train_scaled = StandardScaler().fit_transform(y_train_tmp.astype(float))\n",
    "# y_train_scaled = pd.DataFrame(y_train_scaled, index=y_train_tmp.index, columns=['Score1'])\n",
    "# y_train = np.squeeze(y_train_scaled)\n",
    "\n",
    "\n",
    "y_train = y_train_tmp\n",
    "\n",
    "#print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1170, 264)\n"
     ]
    }
   ],
   "source": [
    "print(text_ngrams_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION\n",
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\stylistics\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif, mutual_info_classif\n",
    "#from sklearn.linear_model import Lasso\n",
    "#from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "\n",
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "\n",
    "from yellowbrick.features.importances import FeatureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Selection\n",
    "* Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "kBestmodel = SelectKBest(mutual_info_classif, k='all')\n",
    "fit_data = kBestmodel.fit(text_ngrams_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>kBest_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>__VERB__</td>\n",
       "      <td>0.087464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>__NOUN__</td>\n",
       "      <td>0.078108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>to</td>\n",
       "      <td>0.070309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>__ADV__</td>\n",
       "      <td>0.057497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>of</td>\n",
       "      <td>0.052992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  kBest_weight\n",
       "108  __VERB__  0.087464    \n",
       "106  __NOUN__  0.078108    \n",
       "238  to        0.070309    \n",
       "105  __ADV__   0.057497    \n",
       "191  of        0.052992    "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = list(zip(text_ngrams_columns, fit_data.scores_))\n",
    "kBest_feature_importance = pd.DataFrame(feature_importance, columns=['feature','kBest_weight'])\n",
    "kBest_feature_importance.sort_values(by='kBest_weight', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>kBest_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>zero</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>inside</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>it</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>materialinto</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>me</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>60minsand</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4s</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  kBest_weight\n",
       "263  zero          0.0         \n",
       "176  inside        0.0         \n",
       "178  it            0.0         \n",
       "92   8             0.0         \n",
       "181  materialinto  0.0         \n",
       "182  me            0.0         \n",
       "87   60minsand     0.0         \n",
       "188  no            0.0         \n",
       "84   5             0.0         \n",
       "83   4s            0.0         "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kBest_feature_importance.sort_values(by='kBest_weight').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Baseline Model\n",
    "* Logitistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(pipe, X, y, target_names):\n",
    "    y_test = y\n",
    "    y_pred = pipe.predict(X)\n",
    "    report = metrics.classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print(report)\n",
    "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ...2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Were using LogisticRegressionCV here to adjust regularization parameter C automatically. \n",
    "# It allows to compare different vectorizers - optimal C value could be different for different input features \n",
    "# (e.g. for bigrams or for character-level input).\n",
    "# An alternative would be to use GridSearchCV or RandomizedSearchCV.\n",
    "\n",
    "\n",
    "#Initialise vectoriser\n",
    "count_vectorizer = CountVectorizer()\n",
    "clf = LogisticRegressionCV()\n",
    "target_names = [str(i) for i in sorted(y_train_tmp.unique())] #Convert to string\n",
    "\n",
    "pipe = make_pipeline(count_vectorizer, clf)\n",
    "pipe.fit(train['EssayText'], y_train_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.95      0.83       268\n",
      "          1       0.94      0.43      0.59       309\n",
      "          2       0.84      0.97      0.90       593\n",
      "\n",
      "avg / total       0.84      0.82      0.80      1170\n",
      "\n",
      "accuracy: 0.824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.89      0.84       112\n",
      "          1       0.71      0.30      0.42       120\n",
      "          2       0.78      0.94      0.85       270\n",
      "\n",
      "avg / total       0.76      0.77      0.75       502\n",
      "\n",
      "accuracy: 0.775\n"
     ]
    }
   ],
   "source": [
    "print_report(pipe,train['EssayText'], y_train_tmp, target_names)\n",
    "\n",
    "print_report(pipe,test['EssayText'], y_test_tmp, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights-wrapper\" style=\"border-collapse: collapse; border: none; margin-bottom: 1.5em;\">\n",
       "            <tr>\n",
       "                \n",
       "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
       "                        <b>\n",
       "    \n",
       "        y=0\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "                    </td>\n",
       "                \n",
       "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
       "                        <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "                    </td>\n",
       "                \n",
       "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
       "                        <b>\n",
       "    \n",
       "        y=2\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "                    </td>\n",
       "                \n",
       "            </tr>\n",
       "            <tr>\n",
       "                \n",
       "                    \n",
       "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
       "                            \n",
       "                                \n",
       "                                    \n",
       "                                    \n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.19%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.191\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 94.77%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.422\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        mass\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.22%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.266\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        it\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.28%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.260\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        will\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.45%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.243\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        then\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.63%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.226\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        them\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.63%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 682 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 96.56%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 1240 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 96.56%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.232\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        size\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 96.18%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.269\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        temperature\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 95.75%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.314\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        are\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 95.18%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.376\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        what\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 94.21%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.489\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        amount\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 93.74%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.546\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        used\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 93.32%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.599\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        how\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 93.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.640\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        much\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 92.76%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.671\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        vinegar\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "                                \n",
       "                            \n",
       "                        </td>\n",
       "                    \n",
       "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
       "                            \n",
       "                                \n",
       "                                    \n",
       "                                    \n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.82%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        is\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.82%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        they\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        you\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.83%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 685 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.85%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 1237 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.85%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        size\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.84%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        would\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        should\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        the\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        what\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        need\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        containers\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.80%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        be\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.79%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        samples\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.79%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        container\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.79%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        know\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 90.56%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.982\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "                                \n",
       "                            \n",
       "                        </td>\n",
       "                    \n",
       "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
       "                            \n",
       "                                \n",
       "                                    \n",
       "                                    \n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.10%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.533\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        size\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.16%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.196\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        long\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.065\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        temperature\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.14%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.044\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        where\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.002\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        type\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.64%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.969\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        big\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.91%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.930\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        kind\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.03%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.913\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        vinegar\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.51%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.844\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        container\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.84%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.797\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        rinse\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 92.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.702\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        must\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 92.53%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 920 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 92.25%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 1002 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 92.25%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.740\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        hypothesis\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.818\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        many\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.917\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        from\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -2.869\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "                                \n",
       "                            \n",
       "                        </td>\n",
       "                    \n",
       "                \n",
       "            </tr>\n",
       "        </table>\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intercept (bias) feature is shown as <BIAS> in the same table. \n",
    "#We can inspect features and weights because were using a bag-of-words vectorizer and a linear classifier\n",
    "#(so there is a direct mapping between individual words and classifier coefficients)\n",
    "\n",
    "#eli5.show_weights(clf, top=10)\n",
    "eli5.show_weights(clf, vec=count_vectorizer, top=15,  target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=0\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.126</b>, score <b>-2.409</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.78%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.191\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -3.600\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 87.36%); opacity: 0.84\" title=\"-0.176\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.16%); opacity: 0.81\" title=\"-0.032\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.57%); opacity: 0.82\" title=\"0.099\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.06%); opacity: 0.82\" title=\"0.075\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.96%); opacity: 0.83\" title=\"-0.145\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.93%); opacity: 0.80\" title=\"-0.005\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.73%); opacity: 0.82\" title=\"-0.113\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.29%); opacity: 0.80\" title=\"-0.010\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.57%); opacity: 0.82\" title=\"0.099\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.13%); opacity: 0.81\" title=\"-0.059\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.06%); opacity: 0.81\" title=\"-0.060\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.42%); opacity: 0.80\" title=\"0.002\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.16%); opacity: 0.91\" title=\"-0.489\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.33%); opacity: 0.83\" title=\"-0.157\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 67.74%); opacity: 0.95\" title=\"-0.671\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.97%); opacity: 0.84\" title=\"-0.164\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.38%); opacity: 0.83\" title=\"-0.156\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.62%); opacity: 0.82\" title=\"-0.082\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.79%); opacity: 0.83\" title=\"-0.148\">container</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 96.16%); opacity: 0.81\" title=\"-0.032\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.53%); opacity: 0.81\" title=\"0.028\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.03%); opacity: 0.82\" title=\"0.108\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.16%); opacity: 0.91\" title=\"-0.489\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.33%); opacity: 0.83\" title=\"-0.157\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 76.68%); opacity: 0.89\" title=\"0.422\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.87%); opacity: 0.81\" title=\"-0.024\">lost</span><span style=\"opacity: 0.80\"> fom </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.35%); opacity: 0.84\" title=\"-0.176\">samples</span><span style=\"opacity: 0.80\">. i </span><span style=\"background-color: hsl(0, 100.00%, 92.16%); opacity: 0.82\" title=\"-0.089\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.65%); opacity: 0.84\" title=\"0.170\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.86%); opacity: 0.81\" title=\"0.036\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.97%); opacity: 0.85\" title=\"0.226\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.36%); opacity: 0.84\" title=\"-0.176\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.64%); opacity: 0.80\" title=\"0.016\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.52%); opacity: 0.80\" title=\"0.017\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.35%); opacity: 0.83\" title=\"0.157\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.51%); opacity: 0.80\" title=\"-0.002\">different</span><span style=\"opacity: 0.80\"> temparatures </span><span style=\"background-color: hsl(120, 100.00%, 97.72%); opacity: 0.80\" title=\"0.015\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 70.23%); opacity: 0.93\" title=\"-0.599\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 68.80%); opacity: 0.94\" title=\"-0.640\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.35%); opacity: 0.84\" title=\"-0.176\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.13%); opacity: 0.81\" title=\"-0.059\">would</span><span style=\"opacity: 0.80\"> loose </span><span style=\"background-color: hsl(120, 100.00%, 76.68%); opacity: 0.89\" title=\"0.422\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.404</b>, score <b>-1.027</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.07%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.045\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.94%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.982\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 99.61%); opacity: 0.80\" title=\"-0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.71%); opacity: 0.80\" title=\"0.001\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.69%); opacity: 0.80\" title=\"0.001\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.94%); opacity: 0.80\" title=\"-0.000\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.91%); opacity: 0.80\" title=\"0.000\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.95%); opacity: 0.80\" title=\"0.000\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.54%); opacity: 0.80\" title=\"-0.002\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.92%); opacity: 0.80\" title=\"0.000\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.69%); opacity: 0.80\" title=\"0.001\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.30%); opacity: 0.80\" title=\"-0.003\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.11%); opacity: 0.80\" title=\"-0.004\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.67%); opacity: 0.80\" title=\"0.001\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.68%); opacity: 0.80\" title=\"0.001\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.32%); opacity: 0.80\" title=\"-0.003\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.54%); opacity: 0.80\" title=\"-0.002\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.19%); opacity: 0.80\" title=\"0.003\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.87%); opacity: 0.80\" title=\"0.000\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.82%); opacity: 0.80\" title=\"-0.000\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.07%); opacity: 0.80\" title=\"-0.004\">container</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 99.71%); opacity: 0.80\" title=\"0.001\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.79%); opacity: 0.80\" title=\"0.000\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.83%); opacity: 0.80\" title=\"-0.000\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.68%); opacity: 0.80\" title=\"0.001\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.32%); opacity: 0.80\" title=\"-0.003\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.86%); opacity: 0.80\" title=\"-0.000\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.84%); opacity: 0.80\" title=\"0.000\">lost</span><span style=\"opacity: 0.80\"> fom </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.09%); opacity: 0.80\" title=\"-0.004\">samples</span><span style=\"opacity: 0.80\">. i </span><span style=\"background-color: hsl(0, 100.00%, 99.52%); opacity: 0.80\" title=\"-0.002\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.87%); opacity: 0.80\" title=\"-0.000\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.91%); opacity: 0.80\" title=\"-0.000\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.69%); opacity: 0.80\" title=\"-0.001\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.61%); opacity: 0.80\" title=\"-0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.92%); opacity: 0.80\" title=\"0.000\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.97%); opacity: 0.80\" title=\"-0.000\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.85%); opacity: 0.80\" title=\"-0.000\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.97%); opacity: 0.80\" title=\"-0.000\">different</span><span style=\"opacity: 0.80\"> temparatures </span><span style=\"background-color: hsl(0, 100.00%, 99.96%); opacity: 0.80\" title=\"-0.000\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.96%); opacity: 0.80\" title=\"-0.000\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.86%); opacity: 0.80\" title=\"0.000\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.09%); opacity: 0.80\" title=\"-0.004\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.30%); opacity: 0.80\" title=\"-0.003\">would</span><span style=\"opacity: 0.80\"> loose </span><span style=\"background-color: hsl(0, 100.00%, 99.86%); opacity: 0.80\" title=\"-0.000\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=2\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.470</b>, score <b>-0.814</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.49%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +2.055\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.94%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -2.869\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 98.25%); opacity: 0.80\" title=\"0.010\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.40%); opacity: 0.84\" title=\"0.195\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.81%); opacity: 0.81\" title=\"-0.049\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.98%); opacity: 0.81\" title=\"-0.047\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.46%); opacity: 0.85\" title=\"0.236\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.67%); opacity: 0.81\" title=\"-0.038\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.00%); opacity: 0.83\" title=\"-0.126\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.65%); opacity: 0.81\" title=\"0.038\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.81%); opacity: 0.81\" title=\"-0.049\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.56%); opacity: 0.83\" title=\"-0.116\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.43%); opacity: 0.86\" title=\"0.282\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.99%); opacity: 0.83\" title=\"-0.126\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.38%); opacity: 0.85\" title=\"0.238\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.68%); opacity: 0.80\" title=\"-0.016\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.913\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.49%); opacity: 0.86\" title=\"-0.258\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.73%); opacity: 0.82\" title=\"0.080\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.31%); opacity: 0.82\" title=\"-0.087\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 62.14%); opacity: 0.98\" title=\"0.844\">container</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 86.40%); opacity: 0.84\" title=\"0.195\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.75%); opacity: 0.90\" title=\"-0.473\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 77.14%); opacity: 0.89\" title=\"0.410\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.38%); opacity: 0.85\" title=\"0.238\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.68%); opacity: 0.80\" title=\"-0.016\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 73.82%); opacity: 0.91\" title=\"-0.498\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.48%); opacity: 0.85\" title=\"-0.236\">lost</span><span style=\"opacity: 0.80\"> fom </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 73.68%); opacity: 0.91\" title=\"0.502\">samples</span><span style=\"opacity: 0.80\">. i </span><span style=\"background-color: hsl(120, 100.00%, 88.96%); opacity: 0.83\" title=\"0.145\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.19%); opacity: 0.84\" title=\"-0.200\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.09%); opacity: 0.81\" title=\"-0.033\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.67%); opacity: 0.83\" title=\"-0.114\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.25%); opacity: 0.80\" title=\"0.010\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.76%); opacity: 0.85\" title=\"-0.209\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.74%); opacity: 0.81\" title=\"-0.037\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.43%); opacity: 0.84\" title=\"-0.175\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.75%); opacity: 0.80\" title=\"0.006\">different</span><span style=\"opacity: 0.80\"> temparatures </span><span style=\"background-color: hsl(0, 100.00%, 96.82%); opacity: 0.81\" title=\"-0.024\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.27%); opacity: 0.83\" title=\"0.121\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.27%); opacity: 0.85\" title=\"0.219\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 73.68%); opacity: 0.91\" title=\"0.502\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.56%); opacity: 0.83\" title=\"-0.116\">would</span><span style=\"opacity: 0.80\"> loose </span><span style=\"background-color: hsl(0, 100.00%, 73.82%); opacity: 0.91\" title=\"-0.498\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_prediction(clf, test['EssayText'][438], vec=count_vectorizer, target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def test_print_report(pipe):\n",
    "#     y_test = y_test_tmp\n",
    "#     y_pred = pipe.predict(test['EssayText'])\n",
    "#     report = metrics.classification_report(y_test, y_pred,\n",
    "#         target_names=target_names)\n",
    "#     print(report)\n",
    "#     print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise vectoriser\n",
    "clf = MultinomialNB()\n",
    "pipe = make_pipeline(count_vectorizer, clf)\n",
    "pipe.fit(train['EssayText'], y_train_tmp);\n",
    "#MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.75      0.84       268\n",
      "          1       0.81      0.58      0.68       309\n",
      "          2       0.78      0.97      0.87       593\n",
      "\n",
      "avg / total       0.83      0.82      0.81      1170\n",
      "\n",
      "accuracy: 0.819\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.50      0.63       112\n",
      "          1       0.42      0.23      0.30       120\n",
      "          2       0.70      0.96      0.81       270\n",
      "\n",
      "avg / total       0.67      0.68      0.65       502\n",
      "\n",
      "accuracy: 0.681\n"
     ]
    }
   ],
   "source": [
    "print_report(pipe,train['EssayText'], y_train_tmp, target_names)\n",
    "\n",
    "print_report(pipe,test['EssayText'], y_test_tmp, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top10(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (class_label, \" \".join(feature_names[j] for j in top10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: experiment in would need mass of you and to the\n",
      "1: and experiment how you in need would of to the\n",
      "2: what vinegar know how in need would of to the\n"
     ]
    }
   ],
   "source": [
    "print_top10(count_vectorizer, clf, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-9.2963\t100            \t\t-2.6005\tthe            \n",
      "\t-9.2963\t100g           \t\t-3.1849\tto             \n",
      "\t-9.2963\t100ml          \t\t-3.7590\tand            \n",
      "\t-9.2963\t10cm           \t\t-3.8847\tyou            \n",
      "\t-9.2963\t10g            \t\t-3.8847\tof             \n",
      "\t-9.2963\t11grams        \t\t-3.9118\tmass           \n",
      "\t-9.2963\t1day           \t\t-3.9350\tneed           \n",
      "\t-9.2963\t1st            \t\t-3.9980\twould          \n",
      "\t-9.2963\t1telling       \t\t-4.0285\tin             \n",
      "\t-9.2963\t2days          \t\t-4.2725\texperiment     \n",
      "\t-9.2963\t30min          \t\t-4.4923\tsamples        \n",
      "\t-9.2963\t3rd            \t\t-4.5601\tthey           \n",
      "\t-9.2963\t50g            \t\t-4.6142\tit             \n",
      "\t-9.2963\t6th            \t\t-4.6424\tis             \n",
      "\t-9.2963\t7grams         \t\t-4.7320\tfor            \n",
      "\t-9.2963\tabove          \t\t-4.7530\twhat           \n",
      "\t-9.2963\tabsolutely     \t\t-4.7745\tsample         \n",
      "\t-9.2963\tabsorb         \t\t-4.7965\teach           \n",
      "\t-9.2963\tabsorbed       \t\t-4.8304\tinformation    \n",
      "\t-9.2963\taccuracy       \t\t-4.8420\tthis           \n"
     ]
    }
   ],
   "source": [
    "# It means that higher values mean more important features for the positive class.\n",
    "# The above print shows the top 20 lowest values (less predictive features) in the first column \n",
    "#and the top 20 high values (highest predictive features) in the second column.\n",
    "\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_informative_features(count_vectorizer, clf, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise vectoriser\n",
    "clf = BernoulliNB()\n",
    "pipe = make_pipeline(count_vectorizer, clf)\n",
    "pipe.fit(train['EssayText'], y_train);\n",
    "#MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.61      0.63       268\n",
      "          1       0.53      0.27      0.36       309\n",
      "          2       0.68      0.86      0.76       593\n",
      "\n",
      "avg / total       0.63      0.65      0.62      1170\n",
      "\n",
      "accuracy: 0.649\n"
     ]
    }
   ],
   "source": [
    "print_report(pipe,train['EssayText'], y_train, target_names)\n",
    "\n",
    "print_report(pipe,test['EssayText'], y_test_tmp, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.55      0.59       112\n",
      "          1       0.41      0.16      0.23       120\n",
      "          2       0.65      0.86      0.74       270\n",
      "\n",
      "avg / total       0.59      0.63      0.59       502\n",
      "\n",
      "accuracy: 0.625\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf = svm.SVC(gamma='scale', decision_function_shape='ovo')\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "pipe = make_pipeline(count_vectorizer, clf)\n",
    "pipe.fit(train['EssayText'], y_train_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98       268\n",
      "          1       0.99      0.97      0.98       309\n",
      "          2       0.99      0.98      0.98       593\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1170\n",
      "\n",
      "accuracy: 0.982\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.71      0.71       112\n",
      "          1       0.46      0.49      0.48       120\n",
      "          2       0.81      0.79      0.80       270\n",
      "\n",
      "avg / total       0.70      0.70      0.70       502\n",
      "\n",
      "accuracy: 0.699\n"
     ]
    }
   ],
   "source": [
    "print_report(pipe,train['EssayText'], y_train_tmp, target_names)\n",
    "\n",
    "print_report(pipe,test['EssayText'], y_test_tmp, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=0\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.290</b>, score <b>-0.712</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 94.85%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.163\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 83.34%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.875\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 95.29%); opacity: 0.81\" title=\"-0.038\">in</span><span style=\"opacity: 0.80\"> this </span><span style=\"background-color: hsl(120, 100.00%, 97.64%); opacity: 0.80\" title=\"0.014\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.20%); opacity: 0.90\" title=\"0.410\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.49%); opacity: 0.87\" title=\"-0.291\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.07%); opacity: 0.81\" title=\"-0.029\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.66%); opacity: 0.82\" title=\"-0.086\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.65%); opacity: 0.80\" title=\"0.014\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.86%); opacity: 0.81\" title=\"0.056\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.86%); opacity: 0.81\" title=\"0.056\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.37%); opacity: 0.85\" title=\"0.193\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.74%); opacity: 0.85\" title=\"-0.205\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.46%); opacity: 0.80\" title=\"-0.016\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.90%); opacity: 0.81\" title=\"0.021\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 66.09%); opacity: 0.96\" title=\"-0.641\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.03%); opacity: 0.80\" title=\"-0.020\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.90%); opacity: 0.90\" title=\"-0.417\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.90%); opacity: 0.81\" title=\"0.055\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.60%); opacity: 0.86\" title=\"-0.227\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 83.91%); opacity: 0.85\" title=\"0.221\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.15%); opacity: 0.82\" title=\"-0.065\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.41%); opacity: 0.88\" title=\"-0.314\">container</span><span style=\"opacity: 0.80\">. this </span><span style=\"background-color: hsl(120, 100.00%, 77.81%); opacity: 0.89\" title=\"0.350\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.79%); opacity: 0.85\" title=\"0.185\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.75%); opacity: 0.81\" title=\"-0.057\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 66.09%); opacity: 0.96\" title=\"-0.641\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.03%); opacity: 0.80\" title=\"-0.020\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.68%); opacity: 0.88\" title=\"0.308\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.90%); opacity: 0.81\" title=\"-0.031\">lost</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.53%); opacity: 0.80\" title=\"0.007\">fom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.49%); opacity: 0.81\" title=\"-0.036\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.81%); opacity: 0.86\" title=\"-0.243\">samples</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 92.51%); opacity: 0.82\" title=\"-0.074\">i</span><span style=\"opacity: 0.80\"> also </span><span style=\"background-color: hsl(120, 100.00%, 74.16%); opacity: 0.91\" title=\"0.435\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.96%); opacity: 0.83\" title=\"0.129\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 69.68%); opacity: 0.93\" title=\"0.546\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.46%); opacity: 0.81\" title=\"0.025\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.49%); opacity: 0.84\" title=\"0.154\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.52%); opacity: 0.87\" title=\"0.269\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.02%); opacity: 0.81\" title=\"0.041\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.96%); opacity: 0.84\" title=\"-0.146\">different</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.56%); opacity: 0.80\" title=\"-0.007\">temparatures</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.46%); opacity: 0.80\" title=\"-0.008\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.32%); opacity: 0.87\" title=\"-0.295\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-0.811\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.49%); opacity: 0.81\" title=\"-0.036\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.68%); opacity: 0.84\" title=\"-0.169\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.37%); opacity: 0.85\" title=\"0.193\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.54%); opacity: 0.80\" title=\"-0.015\">loose</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.38%); opacity: 0.87\" title=\"0.293\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.175</b>, score <b>-1.396</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 90.61%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.386\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 81.57%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.011\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 91.88%); opacity: 0.82\" title=\"-0.083\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.72%); opacity: 0.81\" title=\"-0.023\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.10%); opacity: 0.80\" title=\"-0.004\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.76%); opacity: 0.83\" title=\"-0.132\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.08%); opacity: 0.82\" title=\"0.095\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.14%); opacity: 0.80\" title=\"-0.010\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.19%); opacity: 0.81\" title=\"0.039\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.44%); opacity: 0.82\" title=\"-0.090\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.50%); opacity: 0.82\" title=\"-0.089\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.60%); opacity: 0.81\" title=\"-0.046\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.86%); opacity: 0.82\" title=\"0.069\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.96%); opacity: 0.82\" title=\"-0.082\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.32%); opacity: 0.83\" title=\"0.140\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.02%); opacity: 0.80\" title=\"0.011\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.12%); opacity: 0.87\" title=\"0.299\">amount</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 96.12%); opacity: 0.81\" title=\"0.029\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.29%); opacity: 0.83\" title=\"0.124\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.14%); opacity: 0.81\" title=\"-0.040\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.11%); opacity: 0.82\" title=\"0.066\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.11%); opacity: 0.82\" title=\"0.066\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.71%); opacity: 0.86\" title=\"-0.225\">container</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 93.89%); opacity: 0.81\" title=\"-0.055\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.99%); opacity: 0.80\" title=\"0.020\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 68.42%); opacity: 0.94\" title=\"-0.579\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.00%); opacity: 0.80\" title=\"0.004\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.12%); opacity: 0.87\" title=\"0.299\">amount</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 93.21%); opacity: 0.82\" title=\"0.064\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.96%); opacity: 0.85\" title=\"0.201\">lost</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.34%); opacity: 0.82\" title=\"0.063\">fom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.85%); opacity: 0.81\" title=\"0.043\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.98%); opacity: 0.82\" title=\"-0.082\">samples</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 93.13%); opacity: 0.82\" title=\"0.065\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.55%); opacity: 0.81\" title=\"-0.024\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.64%); opacity: 0.86\" title=\"-0.226\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.04%); opacity: 0.83\" title=\"-0.111\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.05%); opacity: 0.85\" title=\"-0.199\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.53%); opacity: 0.81\" title=\"-0.047\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.92%); opacity: 0.80\" title=\"0.005\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.74%); opacity: 0.84\" title=\"-0.168\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.47%); opacity: 0.82\" title=\"-0.089\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.16%); opacity: 0.84\" title=\"0.178\">different</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.43%); opacity: 0.81\" title=\"0.049\">temparatures</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.12%); opacity: 0.81\" title=\"0.040\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.13%); opacity: 0.84\" title=\"0.161\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.31%); opacity: 0.87\" title=\"0.274\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.69%); opacity: 0.81\" title=\"0.023\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.74%); opacity: 0.83\" title=\"-0.116\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.34%); opacity: 0.82\" title=\"0.063\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.42%); opacity: 0.82\" title=\"0.062\">loose</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.99%); opacity: 0.81\" title=\"0.042\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=2\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.535</b>, score <b>0.435</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.136\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.74%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.701\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 86.08%); opacity: 0.84\" title=\"0.180\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.52%); opacity: 0.81\" title=\"0.047\">this</span><span style=\"opacity: 0.80\"> experiment </span><span style=\"background-color: hsl(0, 100.00%, 79.90%); opacity: 0.87\" title=\"-0.304\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.76%); opacity: 0.85\" title=\"0.204\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.34%); opacity: 0.83\" title=\"0.123\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.17%); opacity: 0.82\" title=\"0.094\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.23%); opacity: 0.81\" title=\"0.039\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.59%); opacity: 0.81\" title=\"0.047\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.52%); opacity: 0.81\" title=\"0.036\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.73%); opacity: 0.83\" title=\"-0.133\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.16%); opacity: 0.88\" title=\"0.342\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.16%); opacity: 0.81\" title=\"-0.029\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.84%); opacity: 0.82\" title=\"0.084\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.41%); opacity: 0.88\" title=\"0.336\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.05%); opacity: 0.80\" title=\"0.011\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 67.07%); opacity: 0.95\" title=\"0.615\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.78%); opacity: 0.81\" title=\"-0.057\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 77.01%); opacity: 0.89\" title=\"0.368\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.84%); opacity: 0.86\" title=\"-0.242\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.87%); opacity: 0.83\" title=\"0.114\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 65.86%); opacity: 0.96\" title=\"0.647\">container</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 90.82%); opacity: 0.82\" title=\"0.099\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.97%); opacity: 0.87\" title=\"-0.281\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 76.25%); opacity: 0.89\" title=\"0.385\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.13%); opacity: 0.84\" title=\"0.161\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.41%); opacity: 0.88\" title=\"0.336\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.80%); opacity: 0.80\" title=\"-0.005\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.71%); opacity: 0.87\" title=\"-0.286\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.55%); opacity: 0.82\" title=\"-0.074\">lost</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.34%); opacity: 0.82\" title=\"0.063\">fom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.12%); opacity: 0.82\" title=\"0.066\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.94%); opacity: 0.87\" title=\"0.281\">samples</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 98.69%); opacity: 0.80\" title=\"-0.006\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.32%); opacity: 0.82\" title=\"0.091\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.87%); opacity: 0.85\" title=\"-0.202\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.26%); opacity: 0.81\" title=\"-0.039\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.67%); opacity: 0.87\" title=\"-0.287\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.97%); opacity: 0.84\" title=\"0.163\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.76%); opacity: 0.85\" title=\"-0.204\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.46%); opacity: 0.82\" title=\"-0.089\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.37%); opacity: 0.80\" title=\"-0.017\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.23%); opacity: 0.82\" title=\"-0.093\">different</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.72%); opacity: 0.82\" title=\"0.071\">temparatures</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.49%); opacity: 0.82\" title=\"0.074\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.94%); opacity: 0.86\" title=\"0.240\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 63.34%); opacity: 0.98\" title=\"0.716\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.68%); opacity: 0.82\" title=\"0.086\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.74%); opacity: 0.87\" title=\"0.265\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.04%); opacity: 0.84\" title=\"-0.180\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.85%); opacity: 0.81\" title=\"0.043\">loose</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.48%); opacity: 0.88\" title=\"-0.313\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = TextExplainer(random_state=42)\n",
    "te.fit(test['EssayText'][438], pipe.predict_proba)\n",
    "te.show_prediction(target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99       268\n",
      "          1       1.00      0.97      0.99       309\n",
      "          2       0.99      1.00      0.99       593\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1170\n",
      "\n",
      "accuracy: 0.991\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.49      0.57       112\n",
      "          1       0.24      0.20      0.22       120\n",
      "          2       0.66      0.79      0.72       270\n",
      "\n",
      "avg / total       0.56      0.58      0.56       502\n",
      "\n",
      "accuracy: 0.580\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "pipe = make_pipeline(count_vectorizer, clf)\n",
    "pipe.fit(train['EssayText'], y_train)\n",
    "\n",
    "print_report(pipe,train['EssayText'], y_train, target_names)\n",
    "\n",
    "print_report(pipe,test['EssayText'], y_test_tmp, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0732\n",
       "                \n",
       "                    &plusmn; 0.0558\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                __VERB__\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 83.61%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0551\n",
       "                \n",
       "                    &plusmn; 0.0457\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                __ADV__\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.44%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0512\n",
       "                \n",
       "                    &plusmn; 0.0356\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                __NOUN__\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.67%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0501\n",
       "                \n",
       "                    &plusmn; 0.0396\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                to\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.67%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0410\n",
       "                \n",
       "                    &plusmn; 0.0193\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                __ADJ__\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.26%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0385\n",
       "                \n",
       "                    &plusmn; 0.0211\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                the\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0374\n",
       "                \n",
       "                    &plusmn; 0.0178\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                of\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.34%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0339\n",
       "                \n",
       "                    &plusmn; 0.0179\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                each\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.82%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0319\n",
       "                \n",
       "                    &plusmn; 0.0210\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                in\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.61%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0287\n",
       "                \n",
       "                    &plusmn; 0.0173\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                .\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.05%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0270\n",
       "                \n",
       "                    &plusmn; 0.0179\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                i\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.11%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0268\n",
       "                \n",
       "                    &plusmn; 0.0176\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ,\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0261\n",
       "                \n",
       "                    &plusmn; 0.0134\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                and\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.44%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0255\n",
       "                \n",
       "                    &plusmn; 0.0108\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                you\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.96%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0236\n",
       "                \n",
       "                    &plusmn; 0.0150\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                they\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.96%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 249 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(clf, vec=count_vectorizer, top=15,  target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = SVC(kernel=\"linear\")\n",
    "# The \"accuracy\" scoring is proportional to the number of correct\n",
    "# classifications\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2), scoring='accuracy')\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Based \n",
    "* Random Forests\n",
    "* GradientBoosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "rf_features = RFECV(RandomForestClassifier(n_estimators=100), cv=StratifiedKFold(5), scoring='f1_weighted')\n",
    "rf_features.fit(X_train, y_train)\n",
    "rf_features.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new matplotlib figure\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot()\n",
    "\n",
    "gb_model = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "feature_importances = gb_model.feature_importances_\n",
    "\n",
    "eli5.show_weights(gb_model,feature_names=feature_names.tolist(), top=50, feature_filter=lambda x: x != '<BIAS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=150, gamma=2e-2, probability=True)\n",
    "\n",
    "pipe = make_pipeline(clf)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(doc):\n",
    "    y_pred = pipe.predict_proba([doc])[0]\n",
    "    #print(y_pred)\n",
    "    for target, prob in zip(y_train, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))\n",
    "\n",
    "doc = X_train.loc[0,]\n",
    "print_prediction(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_df = pd.SparseDataFrame(X_train)\n",
    "# sparse_df.head()\n",
    "\n",
    "sparse_df.fillna(0,inplace=True)\n",
    "sparse_df.loc[0,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TextExplainer(random_state=42)\n",
    "te.fit(doc, pipe.predict_proba)\n",
    "te.show_prediction(target_names=twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(gb_model, doc=train.iloc[1145,0], ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stylistics)",
   "language": "python",
   "name": "stylistics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
