{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings('ignore','.*encoding is deprecated, Use raw=False instead..*')\n",
    "warnings.filterwarnings('ignore','.*the matrix subclass is not the recommended way to represent matrices or deal with linear algebra.*')\n",
    "warnings.filterwarnings('ignore','.*ill-defined.*')\n",
    "warnings.filterwarnings('ignore','.*Data with input dtype int64 was converted to float64 by StandardScaler.*')\n",
    "\n",
    "# importing required packages\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from spacy import displacy\n",
    "#from spacy.lang.en import English\n",
    "#parser = English()\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "#from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up display area to show dataframe in jupyter qtconsole\n",
    "\n",
    "#pd.set_option('display.height', 1000)\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 50)\n",
    "#pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "# pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devanshi\\Desktop\\finalProject\\data\\asap-sas\n"
     ]
    }
   ],
   "source": [
    "myDir = Path.cwd().parents[0]\n",
    "dataFolder = myDir / 'data/asap-sas'\n",
    "ratingsFolder = myDir / 'data/ratings'\n",
    "figureFolder = myDir / 'figures'\n",
    "\n",
    "print(dataFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>subject</th>\n",
       "      <th>studentGrade</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>Score1</th>\n",
       "      <th>styleScore</th>\n",
       "      <th>totalChars</th>\n",
       "      <th>total_words</th>\n",
       "      <th>words_no_punct</th>\n",
       "      <th>words_no_punct_no_stop</th>\n",
       "      <th>count_content_words</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>maturity</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>content_only_text</th>\n",
       "      <th>function_based_text</th>\n",
       "      <th>function_only_text</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>English</td>\n",
       "      <td>10</td>\n",
       "      <td>One trait that describes rose is hard-working....</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>4.173913</td>\n",
       "      <td>5.487419</td>\n",
       "      <td>2.449177</td>\n",
       "      <td>trait  that  describes  rose  is  hard working...</td>\n",
       "      <td>one __NOUN__ __ADJ__ __VERB__ __VERB__ __VERB_...</td>\n",
       "      <td>One  - .  I  this  because  she  to  , but  sh...</td>\n",
       "      <td>NUM NOUN ADJ VERB VERB VERB ADV PUNCT VERB PUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>English</td>\n",
       "      <td>10</td>\n",
       "      <td>First the author has an introduction to grab t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>38</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>5.333607</td>\n",
       "      <td>2.461765</td>\n",
       "      <td>First  author  has  introduction  grab  reader...</td>\n",
       "      <td>__ADV__ the __NOUN__ __VERB__ an __NOUN__ to _...</td>\n",
       "      <td>the  an  to  the  's  .  ,  the  if  in  the  ...</td>\n",
       "      <td>ADV DET NOUN VERB DET NOUN PART VERB DET NOUN ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EssaySet  subject  studentGrade  \\\n",
       "0         7  English            10   \n",
       "1         9  English            10   \n",
       "\n",
       "                                           EssayText  Score1  styleScore  \\\n",
       "0  One trait that describes rose is hard-working....       1           1   \n",
       "1  First the author has an introduction to grab t...       1           1   \n",
       "\n",
       "   totalChars  total_words  words_no_punct  words_no_punct_no_stop  \\\n",
       "0         120           27              23                      12   \n",
       "1         190           38              33                      18   \n",
       "\n",
       "   count_content_words  count_stopwords  avg_word_len  maturity  concreteness  \\\n",
       "0                   15               11      4.173913  5.487419      2.449177   \n",
       "1                   20               15      4.666667  5.333607      2.461765   \n",
       "\n",
       "                                   content_only_text  \\\n",
       "0  trait  that  describes  rose  is  hard working...   \n",
       "1  First  author  has  introduction  grab  reader...   \n",
       "\n",
       "                                 function_based_text  \\\n",
       "0  one __NOUN__ __ADJ__ __VERB__ __VERB__ __VERB_...   \n",
       "1  __ADV__ the __NOUN__ __VERB__ an __NOUN__ to _...   \n",
       "\n",
       "                                  function_only_text  \\\n",
       "0  One  - .  I  this  because  she  to  , but  sh...   \n",
       "1  the  an  to  the  's  .  ,  the  if  in  the  ...   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  NUM NOUN ADJ VERB VERB VERB ADV PUNCT VERB PUN...  \n",
       "1  ADV DET NOUN VERB DET NOUN PART VERB DET NOUN ...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataFolder/'training.csv', header=0)  #read data into dataframe\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10895, 19)\n"
     ]
    }
   ],
   "source": [
    "# Take only essay set 1\n",
    "df = df[~(df.subject == 'Biology')]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X & Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10895, 1) (10895,)\n"
     ]
    }
   ],
   "source": [
    "X = df[['EssayText','Score1']].copy()\n",
    "X.reset_index(drop=True,inplace=True)\n",
    "y= X.pop('Score1')\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, cohen_kappa_score \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "#from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, cohen_kappa_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_metrics(y_test, y_pred):\n",
    "    \n",
    "    #macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally)\n",
    "    macro_precision, macro_recall, macro_fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "\n",
    "    #micro-average will aggregate the contributions of all classes to compute the average metric\n",
    "    micro_precision, micro_recall, micro_fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "\n",
    "    #print(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "    cohens_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    return {'Macro_Precision':macro_precision, 'Macro_Recall': macro_recall,'Macro_fScore':macro_fscore,\n",
    "             'Micro_Precision':micro_precision, 'Micro_Recall':micro_recall,'Micro_fScore':micro_fscore,\n",
    "             'Cohens_Kappa':cohens_kappa}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Simple Baseline: Predict the most popular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English (4296, 1) (4296,)\n",
      "Cross Validating\n",
      "Evaluating results\n",
      "Science (3666, 1) (3666,)\n",
      "Cross Validating\n",
      "Evaluating results\n",
      "English Language Arts (2933, 1) (2933,)\n",
      "Cross Validating\n",
      "Evaluating results\n",
      "\n",
      "\n",
      "Overall Results\n",
      "Train: 2933 Test: 2933\n",
      "Evaluating results\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       902\n",
      "          1       0.54      1.00      0.70      1571\n",
      "          2       0.00      0.00      0.00       460\n",
      "\n",
      "avg / total       0.29      0.54      0.37      2933\n",
      "\n",
      "f1-macro: 0.2325  f1-micro: 0.5356 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Model</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Macro_Precision</th>\n",
       "      <th>Macro_Recall</th>\n",
       "      <th>Macro_fScore</th>\n",
       "      <th>Micro_Precision</th>\n",
       "      <th>Micro_Recall</th>\n",
       "      <th>Micro_fScore</th>\n",
       "      <th>Cohens_Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>None</td>\n",
       "      <td>Baseline_Content</td>\n",
       "      <td>0.117086</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.173299</td>\n",
       "      <td>0.351257</td>\n",
       "      <td>0.351257</td>\n",
       "      <td>0.351257</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Science</td>\n",
       "      <td>None</td>\n",
       "      <td>Baseline_Content</td>\n",
       "      <td>0.166070</td>\n",
       "      <td>0.246778</td>\n",
       "      <td>0.179155</td>\n",
       "      <td>0.334970</td>\n",
       "      <td>0.334970</td>\n",
       "      <td>0.334970</td>\n",
       "      <td>-0.006606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>None</td>\n",
       "      <td>Baseline_Content</td>\n",
       "      <td>0.178543</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.232534</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Overall</td>\n",
       "      <td>None</td>\n",
       "      <td>Baseline_Content</td>\n",
       "      <td>0.178543</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.232534</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Subject Model        Experiment  Macro_Precision  \\\n",
       "0                English  None  Baseline_Content         0.117086   \n",
       "1                Science  None  Baseline_Content         0.166070   \n",
       "2  English Language Arts  None  Baseline_Content         0.178543   \n",
       "3                Overall  None  Baseline_Content         0.178543   \n",
       "\n",
       "   Macro_Recall  Macro_fScore  Micro_Precision  Micro_Recall  Micro_fScore  \\\n",
       "0      0.333333      0.173299         0.351257      0.351257      0.351257   \n",
       "1      0.246778      0.179155         0.334970      0.334970      0.334970   \n",
       "2      0.333333      0.232534         0.535629      0.535629      0.535629   \n",
       "3      0.333333      0.232534         0.535629      0.535629      0.535629   \n",
       "\n",
       "   Cohens_Kappa  \n",
       "0      0.000000  \n",
       "1     -0.006606  \n",
       "2      0.000000  \n",
       "3      0.000000  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Empty array to store results\n",
    "tmp_results = []\n",
    "results_columns = ['Subject','Model','Experiment',\n",
    "                    'Macro_Precision','Macro_Recall','Macro_fScore',\n",
    "                    'Micro_Precision','Micro_Recall','Micro_fScore',\n",
    "                    'Cohens_Kappa']\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#Per Subject experiment\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "experiment_name = 'Baseline_Content'\n",
    "text_column = 'EssayText'\n",
    "\n",
    "\n",
    "for subject in list(df.subject.unique()):\n",
    "    \n",
    "    X = df[(df['subject'] == subject)][[text_column,'Score1']].copy()\n",
    "    X.reset_index(drop=True,inplace=True)\n",
    "    y= X.pop('Score1')\n",
    "    print(subject, X.shape, y.shape)\n",
    "    target_names = [str(i) for i in sorted(y.unique())] #Convert to string\n",
    "    \n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "\n",
    "    print('Cross Validating')\n",
    "    # data is an array with our already pre-processed dataset examples\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "        #print(\"Train:\", len(train_index), \"Test:\", len(test_index))\n",
    "        fold_X_train, fold_X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "        fold_y_train, fold_y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "        \n",
    "        most_common_class = Counter(fold_y_train).most_common(1)[0][0]\n",
    "        \n",
    "        ##For each fold, predict most common class as Baseline\n",
    "        fold_y_pred = [most_common_class]* len(fold_y_test)\n",
    "\n",
    "        y_test.extend(fold_y_test)\n",
    "        y_pred.extend(fold_y_pred)\n",
    "\n",
    "\n",
    "    print('Evaluating results')    \n",
    "    \n",
    "    myrow = {'Subject':subject, 'Model':None,'Experiment':experiment_name}\n",
    "    myrow.update(get_evaluation_metrics(y_test, y_pred))\n",
    "   \n",
    "    tmp_results.append(myrow)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#Repeat experiment for overall\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "print('\\n')\n",
    "print('Overall Results')\n",
    "print(\"Train:\", len(X), \"Test:\", len(y))\n",
    "# data is an array with our already pre-processed dataset examples\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "    \n",
    "    fold_X_train, fold_X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "    fold_y_train, fold_y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "\n",
    "    most_common_class = Counter(fold_y_train).most_common(1)[0][0]\n",
    "\n",
    "    ##For each fold, predict most common class as Baseline\n",
    "    fold_y_pred = [most_common_class]* len(fold_y_test)\n",
    "\n",
    "    y_test.extend(fold_y_test)\n",
    "    y_pred.extend(fold_y_pred)\n",
    "\n",
    "\n",
    "print('Evaluating results')    \n",
    "\n",
    "myrow = {'Subject':'Overall', 'Model':None,'Experiment':experiment_name}\n",
    "myrow.update(get_evaluation_metrics(y_test, y_pred))\n",
    "\n",
    "tmp_results.append(myrow)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "print(report)\n",
    "print(\"f1-macro: {:0.4f}  f1-micro: {:0.4f} \\n\".\n",
    "              format(f1_score(y_test, y_pred, average='macro'),f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "###################################################################################################\n",
    "# Get results, in a  DataFrame\n",
    "simple_baseline_results = pd.DataFrame(tmp_results, columns=results_columns)\n",
    "simple_baseline_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline per Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm  import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "random_state = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========English===========\n",
      "English (4296, 1) (4296,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.75      0.72      1509\n",
      "          1       0.55      0.48      0.51      1340\n",
      "          2       0.70      0.69      0.69      1447\n",
      "\n",
      "avg / total       0.64      0.65      0.65      4296\n",
      "\n",
      "f1-macro: 0.6411  f1-micro: 0.6490\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.56      0.56      1509\n",
      "          1       0.49      0.36      0.41      1340\n",
      "          2       0.52      0.63      0.57      1447\n",
      "\n",
      "avg / total       0.52      0.52      0.52      4296\n",
      "\n",
      "f1-macro: 0.5135  f1-micro: 0.5228\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.70      0.68      1509\n",
      "          1       0.51      0.49      0.50      1340\n",
      "          2       0.66      0.65      0.66      1447\n",
      "\n",
      "avg / total       0.61      0.62      0.62      4296\n",
      "\n",
      "f1-macro: 0.6113  f1-micro: 0.6164\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.72      0.64      1509\n",
      "          1       0.51      0.42      0.46      1340\n",
      "          2       0.66      0.59      0.62      1447\n",
      "\n",
      "avg / total       0.58      0.58      0.58      4296\n",
      "\n",
      "f1-macro: 0.5746  f1-micro: 0.5840\n",
      "\n",
      "==========Science===========\n",
      "Science (3666, 1) (3666,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.56      0.59       665\n",
      "          1       0.56      0.61      0.58      1236\n",
      "          2       0.57      0.57      0.57      1246\n",
      "          3       0.51      0.47      0.49       519\n",
      "\n",
      "avg / total       0.57      0.57      0.57      3666\n",
      "\n",
      "f1-macro: 0.5601  f1-micro: 0.5685\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.31      0.44       665\n",
      "          1       0.61      0.45      0.52      1236\n",
      "          2       0.65      0.31      0.42      1246\n",
      "          3       0.27      0.97      0.42       519\n",
      "\n",
      "avg / total       0.60      0.45      0.46      3666\n",
      "\n",
      "f1-macro: 0.4514  f1-micro: 0.4534\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.54      0.55       665\n",
      "          1       0.53      0.55      0.54      1236\n",
      "          2       0.55      0.55      0.55      1246\n",
      "          3       0.47      0.46      0.46       519\n",
      "\n",
      "avg / total       0.53      0.53      0.53      3666\n",
      "\n",
      "f1-macro: 0.5263  f1-micro: 0.5344\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.48      0.53       665\n",
      "          1       0.53      0.62      0.57      1236\n",
      "          2       0.53      0.60      0.56      1246\n",
      "          3       0.45      0.24      0.31       519\n",
      "\n",
      "avg / total       0.53      0.53      0.52      3666\n",
      "\n",
      "f1-macro: 0.4951  f1-micro: 0.5338\n",
      "\n",
      "==========English Language Arts===========\n",
      "English Language Arts (2933, 1) (2933,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.55      0.55       902\n",
      "          1       0.63      0.71      0.67      1571\n",
      "          2       0.34      0.21      0.26       460\n",
      "\n",
      "avg / total       0.56      0.58      0.57      2933\n",
      "\n",
      "f1-macro: 0.4938  f1-micro: 0.5810\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.47      0.54       902\n",
      "          1       0.65      0.54      0.59      1571\n",
      "          2       0.27      0.55      0.36       460\n",
      "\n",
      "avg / total       0.59      0.52      0.54      2933\n",
      "\n",
      "f1-macro: 0.4981  f1-micro: 0.5227\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.55      0.54       902\n",
      "          1       0.63      0.63      0.63      1571\n",
      "          2       0.33      0.32      0.33       460\n",
      "\n",
      "avg / total       0.55      0.56      0.56      2933\n",
      "\n",
      "f1-macro: 0.4989  f1-micro: 0.5557\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.53      0.55       902\n",
      "          1       0.61      0.78      0.69      1571\n",
      "          2       0.50      0.11      0.18       460\n",
      "\n",
      "avg / total       0.58      0.60      0.57      2933\n",
      "\n",
      "f1-macro: 0.4738  f1-micro: 0.5980\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Model</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Macro_Precision</th>\n",
       "      <th>Macro_Recall</th>\n",
       "      <th>Macro_fScore</th>\n",
       "      <th>Micro_Precision</th>\n",
       "      <th>Micro_Recall</th>\n",
       "      <th>Micro_fScore</th>\n",
       "      <th>Cohens_Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.641345</td>\n",
       "      <td>0.643415</td>\n",
       "      <td>0.641070</td>\n",
       "      <td>0.648976</td>\n",
       "      <td>0.648976</td>\n",
       "      <td>0.648976</td>\n",
       "      <td>0.471675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>NB</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.518820</td>\n",
       "      <td>0.518251</td>\n",
       "      <td>0.513462</td>\n",
       "      <td>0.522812</td>\n",
       "      <td>0.522812</td>\n",
       "      <td>0.522812</td>\n",
       "      <td>0.281052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.610981</td>\n",
       "      <td>0.611997</td>\n",
       "      <td>0.611279</td>\n",
       "      <td>0.616387</td>\n",
       "      <td>0.616387</td>\n",
       "      <td>0.616387</td>\n",
       "      <td>0.423415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>RF</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.580503</td>\n",
       "      <td>0.577888</td>\n",
       "      <td>0.574563</td>\n",
       "      <td>0.584032</td>\n",
       "      <td>0.584032</td>\n",
       "      <td>0.584032</td>\n",
       "      <td>0.372814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Science</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.567756</td>\n",
       "      <td>0.554236</td>\n",
       "      <td>0.560133</td>\n",
       "      <td>0.568467</td>\n",
       "      <td>0.568467</td>\n",
       "      <td>0.568467</td>\n",
       "      <td>0.394681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Science</td>\n",
       "      <td>NB</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.566796</td>\n",
       "      <td>0.512939</td>\n",
       "      <td>0.451400</td>\n",
       "      <td>0.453355</td>\n",
       "      <td>0.453355</td>\n",
       "      <td>0.453355</td>\n",
       "      <td>0.293901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Science</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.528255</td>\n",
       "      <td>0.524543</td>\n",
       "      <td>0.526295</td>\n",
       "      <td>0.534370</td>\n",
       "      <td>0.534370</td>\n",
       "      <td>0.534370</td>\n",
       "      <td>0.349979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Science</td>\n",
       "      <td>RF</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.527735</td>\n",
       "      <td>0.484609</td>\n",
       "      <td>0.495060</td>\n",
       "      <td>0.533824</td>\n",
       "      <td>0.533824</td>\n",
       "      <td>0.533824</td>\n",
       "      <td>0.333047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.510419</td>\n",
       "      <td>0.488845</td>\n",
       "      <td>0.493796</td>\n",
       "      <td>0.580975</td>\n",
       "      <td>0.580975</td>\n",
       "      <td>0.580975</td>\n",
       "      <td>0.264035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>NB</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.520430</td>\n",
       "      <td>0.522358</td>\n",
       "      <td>0.498051</td>\n",
       "      <td>0.522673</td>\n",
       "      <td>0.522673</td>\n",
       "      <td>0.522673</td>\n",
       "      <td>0.253380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.499144</td>\n",
       "      <td>0.498767</td>\n",
       "      <td>0.498855</td>\n",
       "      <td>0.555745</td>\n",
       "      <td>0.555745</td>\n",
       "      <td>0.555745</td>\n",
       "      <td>0.251590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>RF</td>\n",
       "      <td>Baseline_perModel_Content</td>\n",
       "      <td>0.561430</td>\n",
       "      <td>0.473718</td>\n",
       "      <td>0.473794</td>\n",
       "      <td>0.598023</td>\n",
       "      <td>0.598023</td>\n",
       "      <td>0.598023</td>\n",
       "      <td>0.258468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Subject Model                 Experiment  Macro_Precision  \\\n",
       "0                 English  LogR  Baseline_perModel_Content         0.641345   \n",
       "1                 English    NB  Baseline_perModel_Content         0.518820   \n",
       "2                 English   SVM  Baseline_perModel_Content         0.610981   \n",
       "3                 English    RF  Baseline_perModel_Content         0.580503   \n",
       "4                 Science  LogR  Baseline_perModel_Content         0.567756   \n",
       "5                 Science    NB  Baseline_perModel_Content         0.566796   \n",
       "6                 Science   SVM  Baseline_perModel_Content         0.528255   \n",
       "7                 Science    RF  Baseline_perModel_Content         0.527735   \n",
       "8   English Language Arts  LogR  Baseline_perModel_Content         0.510419   \n",
       "9   English Language Arts    NB  Baseline_perModel_Content         0.520430   \n",
       "10  English Language Arts   SVM  Baseline_perModel_Content         0.499144   \n",
       "11  English Language Arts    RF  Baseline_perModel_Content         0.561430   \n",
       "\n",
       "    Macro_Recall  Macro_fScore  Micro_Precision  Micro_Recall  Micro_fScore  \\\n",
       "0       0.643415      0.641070         0.648976      0.648976      0.648976   \n",
       "1       0.518251      0.513462         0.522812      0.522812      0.522812   \n",
       "2       0.611997      0.611279         0.616387      0.616387      0.616387   \n",
       "3       0.577888      0.574563         0.584032      0.584032      0.584032   \n",
       "4       0.554236      0.560133         0.568467      0.568467      0.568467   \n",
       "5       0.512939      0.451400         0.453355      0.453355      0.453355   \n",
       "6       0.524543      0.526295         0.534370      0.534370      0.534370   \n",
       "7       0.484609      0.495060         0.533824      0.533824      0.533824   \n",
       "8       0.488845      0.493796         0.580975      0.580975      0.580975   \n",
       "9       0.522358      0.498051         0.522673      0.522673      0.522673   \n",
       "10      0.498767      0.498855         0.555745      0.555745      0.555745   \n",
       "11      0.473718      0.473794         0.598023      0.598023      0.598023   \n",
       "\n",
       "    Cohens_Kappa  \n",
       "0       0.471675  \n",
       "1       0.281052  \n",
       "2       0.423415  \n",
       "3       0.372814  \n",
       "4       0.394681  \n",
       "5       0.293901  \n",
       "6       0.349979  \n",
       "7       0.333047  \n",
       "8       0.264035  \n",
       "9       0.253380  \n",
       "10      0.251590  \n",
       "11      0.258468  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#======================EXPERIMENT CONFIG===================\n",
    "experiment_name = 'Baseline_perModel_Content'\n",
    "text_column = 'EssayText'\n",
    "\n",
    "#Empty array to store results\n",
    "tmp_results = []\n",
    "results_columns = ['Subject','Model','Experiment',\n",
    "                    'Macro_Precision','Macro_Recall','Macro_fScore',\n",
    "                    'Micro_Precision','Micro_Recall','Micro_fScore',\n",
    "                    'Cohens_Kappa']\n",
    "\n",
    "#Models and Vectorisers\n",
    "count_vectorizer = CountVectorizer()\n",
    "models = [('LogR', LogisticRegression(random_state=random_state)),\n",
    "         ('NB', MultinomialNB()),\n",
    "          ('SVM',LinearSVC(random_state=random_state)),\n",
    "         ('RF',RandomForestClassifier(random_state=random_state))]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#Per Subject experiment\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for subject in list(df.subject.unique()):\n",
    "    print('\\n=========={}==========='.format(subject))\n",
    "    \n",
    "    X = df[(df['subject'] == subject)][[text_column,'Score1']].copy()\n",
    "    X.reset_index(drop=True,inplace=True)\n",
    "    y= X.pop('Score1')\n",
    "    print(subject, X.shape, y.shape)\n",
    "    target_names = [str(i) for i in sorted(y.unique())] #Convert to string\n",
    "    \n",
    "    # For each model\n",
    "    for model_name, clf in models:\n",
    "        print('\\n------------{}-------------'.format(model_name))\n",
    "\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "\n",
    "        print('Cross Validating...')\n",
    "        # data is an array with our already pre-processed dataset examples\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "            #print(\"Train:\", len(train_index), \"Test:\", len(test_index))\n",
    "            fold_X_train, fold_X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "            fold_y_train, fold_y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "\n",
    "            fold_X_train = fold_X_train[text_column]\n",
    "            fold_X_test =  fold_X_test[text_column]\n",
    "\n",
    "            #Make pipeline to train classifier\n",
    "            pipe = make_pipeline(count_vectorizer, clf)\n",
    "            pipe.fit(fold_X_train, fold_y_train)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            fold_y_pred = pipe.predict(fold_X_test)\n",
    "\n",
    "            y_test.extend(fold_y_test)\n",
    "            y_pred.extend(fold_y_pred)\n",
    "\n",
    "\n",
    "        print('Evaluating results...')    \n",
    "\n",
    "        myrow = {'Subject':subject, 'Model':model_name, 'Experiment':experiment_name}\n",
    "        myrow.update(get_evaluation_metrics(y_test, y_pred))\n",
    "\n",
    "        tmp_results.append(myrow)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "        print(report)\n",
    "        print(\"f1-macro: {:0.4f}  f1-micro: {:0.4f}\".\n",
    "              format(f1_score(y_test, y_pred, average='macro'),f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "    #     conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    #     fig, ax = plt.subplots(figsize=(5,5))\n",
    "    #     sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap=\"YlGnBu\")\n",
    "\n",
    "    #     plt.title('Set {}: {}'.format(str(essay_set),model_name))\n",
    "    #     plt.ylabel('Actual Class')\n",
    "    #     plt.xlabel('Predicted Class')\n",
    "\n",
    "    #     print(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set)))\n",
    "    #     plt.savefig(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set))\", \n",
    "    #                 format=\"svg\", bbox_inches='tight')\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Get results, in a  DataFrame\n",
    "baseline_mdl_results = pd.DataFrame(tmp_results, columns=results_columns)\n",
    "baseline_mdl_results              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Make plots\n",
    "g = sns.catplot(x=\"Model\", y=\"Micro_fScore\", data=baseline_mdl_results, \n",
    "                col='Subject',hue=\"Model\", kind=\"bar\", \n",
    "                palette=\"husl\", dodge=False, height=4)\n",
    "g.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle('Micro f-score of Models (Baseline)') # can also get the figure from plt.gcf()\n",
    "\n",
    "plt.savefig(figureFolder/\"baseline_content/Baseline_perModel_Content.svg\", format=\"svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========English===========\n",
      "English (4296, 1) (4296,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.76      0.72      1509\n",
      "          1       0.55      0.47      0.51      1340\n",
      "          2       0.68      0.68      0.68      1447\n",
      "\n",
      "avg / total       0.64      0.64      0.64      4296\n",
      "\n",
      "f1-macro: 0.6349  f1-micro: 0.6439\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.58      0.58      1509\n",
      "          1       0.51      0.36      0.42      1340\n",
      "          2       0.53      0.67      0.59      1447\n",
      "\n",
      "avg / total       0.54      0.54      0.54      4296\n",
      "\n",
      "f1-macro: 0.5317  f1-micro: 0.5428\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.71      0.69      1509\n",
      "          1       0.51      0.48      0.50      1340\n",
      "          2       0.66      0.64      0.65      1447\n",
      "\n",
      "avg / total       0.61      0.62      0.61      4296\n",
      "\n",
      "f1-macro: 0.6105  f1-micro: 0.6162\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.74      0.68      1509\n",
      "          1       0.51      0.46      0.49      1340\n",
      "          2       0.68      0.61      0.64      1447\n",
      "\n",
      "avg / total       0.61      0.61      0.61      4296\n",
      "\n",
      "f1-macro: 0.6045  f1-micro: 0.6122\n",
      "\n",
      "==========Science===========\n",
      "Science (3666, 1) (3666,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.57      0.59       665\n",
      "          1       0.56      0.60      0.58      1236\n",
      "          2       0.58      0.57      0.58      1246\n",
      "          3       0.52      0.48      0.50       519\n",
      "\n",
      "avg / total       0.57      0.57      0.57      3666\n",
      "\n",
      "f1-macro: 0.5604  f1-micro: 0.5687\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.34      0.45       665\n",
      "          1       0.63      0.43      0.51      1236\n",
      "          2       0.65      0.33      0.43      1246\n",
      "          3       0.27      0.97      0.42       519\n",
      "\n",
      "avg / total       0.60      0.45      0.46      3666\n",
      "\n",
      "f1-macro: 0.4553  f1-micro: 0.4547\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.52      0.53       665\n",
      "          1       0.54      0.58      0.56      1236\n",
      "          2       0.54      0.53      0.54      1246\n",
      "          3       0.46      0.45      0.46       519\n",
      "\n",
      "avg / total       0.53      0.53      0.53      3666\n",
      "\n",
      "f1-macro: 0.5222  f1-micro: 0.5336\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.52      0.55       665\n",
      "          1       0.53      0.61      0.56      1236\n",
      "          2       0.53      0.57      0.55      1246\n",
      "          3       0.46      0.28      0.34       519\n",
      "\n",
      "avg / total       0.53      0.53      0.52      3666\n",
      "\n",
      "f1-macro: 0.5013  f1-micro: 0.5311\n",
      "\n",
      "==========English Language Arts===========\n",
      "English Language Arts (2933, 1) (2933,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.56      0.57       902\n",
      "          1       0.63      0.73      0.68      1571\n",
      "          2       0.37      0.19      0.25       460\n",
      "\n",
      "avg / total       0.57      0.59      0.58      2933\n",
      "\n",
      "f1-macro: 0.5009  f1-micro: 0.5946\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.47      0.55       902\n",
      "          1       0.63      0.63      0.63      1571\n",
      "          2       0.28      0.43      0.34       460\n",
      "\n",
      "avg / total       0.58      0.55      0.56      2933\n",
      "\n",
      "f1-macro: 0.5049  f1-micro: 0.5496\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.53      0.53       902\n",
      "          1       0.64      0.66      0.65      1571\n",
      "          2       0.33      0.28      0.30       460\n",
      "\n",
      "avg / total       0.55      0.56      0.56      2933\n",
      "\n",
      "f1-macro: 0.4917  f1-micro: 0.5602\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.57      0.57       902\n",
      "          1       0.63      0.76      0.69      1571\n",
      "          2       0.47      0.16      0.24       460\n",
      "\n",
      "avg / total       0.59      0.60      0.58      2933\n",
      "\n",
      "f1-macro: 0.4997  f1-micro: 0.6048\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Model</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Macro_Precision</th>\n",
       "      <th>Macro_Recall</th>\n",
       "      <th>Macro_fScore</th>\n",
       "      <th>Micro_Precision</th>\n",
       "      <th>Micro_Recall</th>\n",
       "      <th>Micro_fScore</th>\n",
       "      <th>Cohens_Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.635608</td>\n",
       "      <td>0.637949</td>\n",
       "      <td>0.634926</td>\n",
       "      <td>0.643855</td>\n",
       "      <td>0.643855</td>\n",
       "      <td>0.643855</td>\n",
       "      <td>0.463738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>NB</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.540228</td>\n",
       "      <td>0.537806</td>\n",
       "      <td>0.531736</td>\n",
       "      <td>0.542831</td>\n",
       "      <td>0.542831</td>\n",
       "      <td>0.542831</td>\n",
       "      <td>0.311022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.610594</td>\n",
       "      <td>0.611531</td>\n",
       "      <td>0.610543</td>\n",
       "      <td>0.616155</td>\n",
       "      <td>0.616155</td>\n",
       "      <td>0.616155</td>\n",
       "      <td>0.422856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>RF</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.607261</td>\n",
       "      <td>0.606664</td>\n",
       "      <td>0.604512</td>\n",
       "      <td>0.612197</td>\n",
       "      <td>0.612197</td>\n",
       "      <td>0.612197</td>\n",
       "      <td>0.416160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Science</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.565047</td>\n",
       "      <td>0.556521</td>\n",
       "      <td>0.560410</td>\n",
       "      <td>0.568740</td>\n",
       "      <td>0.568740</td>\n",
       "      <td>0.568740</td>\n",
       "      <td>0.396472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Science</td>\n",
       "      <td>NB</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.559098</td>\n",
       "      <td>0.516425</td>\n",
       "      <td>0.455340</td>\n",
       "      <td>0.454719</td>\n",
       "      <td>0.454719</td>\n",
       "      <td>0.454719</td>\n",
       "      <td>0.297216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Science</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.524618</td>\n",
       "      <td>0.520423</td>\n",
       "      <td>0.522237</td>\n",
       "      <td>0.533552</td>\n",
       "      <td>0.533552</td>\n",
       "      <td>0.533552</td>\n",
       "      <td>0.348562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Science</td>\n",
       "      <td>RF</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.522857</td>\n",
       "      <td>0.493062</td>\n",
       "      <td>0.501281</td>\n",
       "      <td>0.531097</td>\n",
       "      <td>0.531097</td>\n",
       "      <td>0.531097</td>\n",
       "      <td>0.334204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>LogR</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.526755</td>\n",
       "      <td>0.495593</td>\n",
       "      <td>0.500910</td>\n",
       "      <td>0.594613</td>\n",
       "      <td>0.594613</td>\n",
       "      <td>0.594613</td>\n",
       "      <td>0.280711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>NB</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.521697</td>\n",
       "      <td>0.510107</td>\n",
       "      <td>0.504907</td>\n",
       "      <td>0.549608</td>\n",
       "      <td>0.549608</td>\n",
       "      <td>0.549608</td>\n",
       "      <td>0.256706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.495724</td>\n",
       "      <td>0.489387</td>\n",
       "      <td>0.491727</td>\n",
       "      <td>0.560177</td>\n",
       "      <td>0.560177</td>\n",
       "      <td>0.560177</td>\n",
       "      <td>0.249518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>RF</td>\n",
       "      <td>Removed_Stopword_perModel_Content</td>\n",
       "      <td>0.559176</td>\n",
       "      <td>0.494586</td>\n",
       "      <td>0.499722</td>\n",
       "      <td>0.604841</td>\n",
       "      <td>0.604841</td>\n",
       "      <td>0.604841</td>\n",
       "      <td>0.285200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Subject Model                         Experiment  \\\n",
       "0                 English  LogR  Removed_Stopword_perModel_Content   \n",
       "1                 English    NB  Removed_Stopword_perModel_Content   \n",
       "2                 English   SVM  Removed_Stopword_perModel_Content   \n",
       "3                 English    RF  Removed_Stopword_perModel_Content   \n",
       "4                 Science  LogR  Removed_Stopword_perModel_Content   \n",
       "5                 Science    NB  Removed_Stopword_perModel_Content   \n",
       "6                 Science   SVM  Removed_Stopword_perModel_Content   \n",
       "7                 Science    RF  Removed_Stopword_perModel_Content   \n",
       "8   English Language Arts  LogR  Removed_Stopword_perModel_Content   \n",
       "9   English Language Arts    NB  Removed_Stopword_perModel_Content   \n",
       "10  English Language Arts   SVM  Removed_Stopword_perModel_Content   \n",
       "11  English Language Arts    RF  Removed_Stopword_perModel_Content   \n",
       "\n",
       "    Macro_Precision  Macro_Recall  Macro_fScore  Micro_Precision  \\\n",
       "0          0.635608      0.637949      0.634926         0.643855   \n",
       "1          0.540228      0.537806      0.531736         0.542831   \n",
       "2          0.610594      0.611531      0.610543         0.616155   \n",
       "3          0.607261      0.606664      0.604512         0.612197   \n",
       "4          0.565047      0.556521      0.560410         0.568740   \n",
       "5          0.559098      0.516425      0.455340         0.454719   \n",
       "6          0.524618      0.520423      0.522237         0.533552   \n",
       "7          0.522857      0.493062      0.501281         0.531097   \n",
       "8          0.526755      0.495593      0.500910         0.594613   \n",
       "9          0.521697      0.510107      0.504907         0.549608   \n",
       "10         0.495724      0.489387      0.491727         0.560177   \n",
       "11         0.559176      0.494586      0.499722         0.604841   \n",
       "\n",
       "    Micro_Recall  Micro_fScore  Cohens_Kappa  \n",
       "0       0.643855      0.643855      0.463738  \n",
       "1       0.542831      0.542831      0.311022  \n",
       "2       0.616155      0.616155      0.422856  \n",
       "3       0.612197      0.612197      0.416160  \n",
       "4       0.568740      0.568740      0.396472  \n",
       "5       0.454719      0.454719      0.297216  \n",
       "6       0.533552      0.533552      0.348562  \n",
       "7       0.531097      0.531097      0.334204  \n",
       "8       0.594613      0.594613      0.280711  \n",
       "9       0.549608      0.549608      0.256706  \n",
       "10      0.560177      0.560177      0.249518  \n",
       "11      0.604841      0.604841      0.285200  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#======================EXPERIMENT CONFIG===================\n",
    "experiment_name = 'Removed_Stopword_perModel_Content'\n",
    "text_column = 'EssayText'\n",
    "\n",
    "#Empty array to store results\n",
    "tmp_results = []\n",
    "results_columns = ['Subject','Model','Experiment',\n",
    "                    'Macro_Precision','Macro_Recall','Macro_fScore',\n",
    "                    'Micro_Precision','Micro_Recall','Micro_fScore',\n",
    "                    'Cohens_Kappa']\n",
    "\n",
    "#Models and Vectorisers\n",
    "# scaler = StandardScaler(with_mean=False)\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "models = [('LogR', LogisticRegression(random_state=random_state)),\n",
    "         ('NB', MultinomialNB()),\n",
    "          ('SVM',LinearSVC(random_state=random_state)),\n",
    "         ('RF',RandomForestClassifier(random_state=random_state))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#Per Subject experiment\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for subject in list(df.subject.unique()):\n",
    "    print('\\n=========={}==========='.format(subject))\n",
    "    \n",
    "    X = df[(df['subject'] == subject)][[text_column,'Score1']].copy()\n",
    "    X.reset_index(drop=True,inplace=True)\n",
    "    y= X.pop('Score1')\n",
    "    print(subject, X.shape, y.shape)\n",
    "    target_names = [str(i) for i in sorted(y.unique())] #Convert to string\n",
    "    \n",
    "    \n",
    "    # For each model\n",
    "    for model_name, clf in models:\n",
    "        print('\\n------------{}-------------'.format(model_name))\n",
    "\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "\n",
    "        print('Cross Validating...')\n",
    "        # data is an array with our already pre-processed dataset examples\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "            #print(\"Train:\", len(train_index), \"Test:\", len(test_index))\n",
    "            fold_X_train, fold_X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "            fold_y_train, fold_y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "\n",
    "            fold_X_train = fold_X_train[text_column]\n",
    "            fold_X_test =  fold_X_test[text_column]\n",
    "\n",
    "            # Make pipeline to train classifier\n",
    "            pipe = make_pipeline(count_vectorizer, clf)\n",
    "            pipe.fit(fold_X_train, fold_y_train)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            fold_y_pred = pipe.predict(fold_X_test)\n",
    "\n",
    "            y_test.extend(fold_y_test)\n",
    "            y_pred.extend(fold_y_pred)\n",
    "\n",
    "\n",
    "        print('Evaluating results...')    \n",
    "\n",
    "        myrow = {'Subject':subject, 'Model':model_name, 'Experiment':experiment_name}\n",
    "        myrow.update(get_evaluation_metrics(y_test, y_pred))\n",
    "\n",
    "        tmp_results.append(myrow)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "        print(report)\n",
    "        print(\"f1-macro: {:0.4f}  f1-micro: {:0.4f}\".\n",
    "              format(f1_score(y_test, y_pred, average='macro'),f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "    #     conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    #     fig, ax = plt.subplots(figsize=(5,5))\n",
    "    #     sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap=\"YlGnBu\")\n",
    "\n",
    "    #     plt.title('Set {}: {}'.format(str(essay_set),model_name))\n",
    "    #     plt.ylabel('Actual Class')\n",
    "    #     plt.xlabel('Predicted Class')\n",
    "\n",
    "    #     print(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set)))\n",
    "    #     plt.savefig(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set))\", \n",
    "    #                 format=\"svg\", bbox_inches='tight')\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Get results, in a  DataFrame\n",
    "no_stopword_results = pd.DataFrame(tmp_results, columns=results_columns)\n",
    "no_stopword_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw plots\n",
    "g = sns.catplot(x=\"Model\", y=\"Micro_fScore\", data=no_stopword_results, \n",
    "                col='Subject',hue=\"Model\", kind=\"bar\", \n",
    "                palette=\"husl\", dodge=False, height=4)\n",
    "g.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "\n",
    "plt.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle('Micro f-score of Models after Stopword Removal') # can also get the figure from plt.gcf()\n",
    "\n",
    "plt.savefig(figureFolder/\"baseline_content/Stopword_Removal_Content.svg\", format=\"svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE Sampling (with stopword removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as imb_pipeline\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========English===========\n",
      "English (4296, 1) (4296,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.75      0.72      1509\n",
      "          1       0.55      0.50      0.53      1340\n",
      "          2       0.68      0.68      0.68      1447\n",
      "\n",
      "avg / total       0.64      0.65      0.65      4296\n",
      "\n",
      "f1-macro: 0.6418  f1-micro: 0.6483\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.56      0.57      1509\n",
      "          1       0.50      0.38      0.43      1340\n",
      "          2       0.53      0.65      0.58      1447\n",
      "\n",
      "avg / total       0.53      0.54      0.53      4296\n",
      "\n",
      "f1-macro: 0.5277  f1-micro: 0.5363\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.70      0.68      1509\n",
      "          1       0.50      0.49      0.49      1340\n",
      "          2       0.66      0.65      0.65      1447\n",
      "\n",
      "avg / total       0.61      0.61      0.61      4296\n",
      "\n",
      "f1-macro: 0.6092  f1-micro: 0.6143\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.75      0.68      1509\n",
      "          1       0.52      0.47      0.49      1340\n",
      "          2       0.69      0.62      0.65      1447\n",
      "\n",
      "avg / total       0.61      0.62      0.61      4296\n",
      "\n",
      "f1-macro: 0.6076  f1-micro: 0.6152\n",
      "\n",
      "==========Science===========\n",
      "Science (3666, 1) (3666,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.65      0.60       665\n",
      "          1       0.60      0.53      0.56      1236\n",
      "          2       0.60      0.51      0.55      1246\n",
      "          3       0.42      0.60      0.50       519\n",
      "\n",
      "avg / total       0.57      0.56      0.56      3666\n",
      "\n",
      "f1-macro: 0.5538  f1-micro: 0.5562\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.37      0.47       665\n",
      "          1       0.66      0.41      0.51      1236\n",
      "          2       0.68      0.31      0.43      1246\n",
      "          3       0.26      0.98      0.41       519\n",
      "\n",
      "avg / total       0.61      0.45      0.46      3666\n",
      "\n",
      "f1-macro: 0.4540  f1-micro: 0.4501\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.59      0.56       665\n",
      "          1       0.54      0.50      0.52      1236\n",
      "          2       0.53      0.50      0.52      1246\n",
      "          3       0.41      0.49      0.45       519\n",
      "\n",
      "avg / total       0.52      0.52      0.52      3666\n",
      "\n",
      "f1-macro: 0.5124  f1-micro: 0.5164\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.56      0.55       665\n",
      "          1       0.54      0.58      0.56      1236\n",
      "          2       0.56      0.55      0.55      1246\n",
      "          3       0.46      0.39      0.42       519\n",
      "\n",
      "avg / total       0.54      0.54      0.54      3666\n",
      "\n",
      "f1-macro: 0.5222  f1-micro: 0.5382\n",
      "\n",
      "==========English Language Arts===========\n",
      "English Language Arts (2933, 1) (2933,)\n",
      "\n",
      "------------LogR-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.62      0.59       902\n",
      "          1       0.67      0.55      0.60      1571\n",
      "          2       0.29      0.41      0.34       460\n",
      "\n",
      "avg / total       0.58      0.55      0.56      2933\n",
      "\n",
      "f1-macro: 0.5094  f1-micro: 0.5476\n",
      "\n",
      "------------NB-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.51      0.56       902\n",
      "          1       0.72      0.37      0.48      1571\n",
      "          2       0.25      0.75      0.37       460\n",
      "\n",
      "avg / total       0.61      0.47      0.49      2933\n",
      "\n",
      "f1-macro: 0.4704  f1-micro: 0.4685\n",
      "\n",
      "------------SVM-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.58      0.54       902\n",
      "          1       0.65      0.54      0.59      1571\n",
      "          2       0.28      0.38      0.32       460\n",
      "\n",
      "avg / total       0.55      0.53      0.54      2933\n",
      "\n",
      "f1-macro: 0.4869  f1-micro: 0.5271\n",
      "\n",
      "------------RF-------------\n",
      "Cross Validating...\n",
      "Evaluating results...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.61      0.58       902\n",
      "          1       0.64      0.69      0.66      1571\n",
      "          2       0.37      0.21      0.27       460\n",
      "\n",
      "avg / total       0.57      0.59      0.58      2933\n",
      "\n",
      "f1-macro: 0.5037  f1-micro: 0.5885\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Model</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Macro_Precision</th>\n",
       "      <th>Macro_Recall</th>\n",
       "      <th>Macro_fScore</th>\n",
       "      <th>Micro_Precision</th>\n",
       "      <th>Micro_Recall</th>\n",
       "      <th>Micro_fScore</th>\n",
       "      <th>Cohens_Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>LogR</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.641619</td>\n",
       "      <td>0.643344</td>\n",
       "      <td>0.641765</td>\n",
       "      <td>0.648277</td>\n",
       "      <td>0.648277</td>\n",
       "      <td>0.648277</td>\n",
       "      <td>0.470931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>NB</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.533327</td>\n",
       "      <td>0.532080</td>\n",
       "      <td>0.527713</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.301769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>SVM</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.608938</td>\n",
       "      <td>0.609924</td>\n",
       "      <td>0.609245</td>\n",
       "      <td>0.614292</td>\n",
       "      <td>0.614292</td>\n",
       "      <td>0.614292</td>\n",
       "      <td>0.420298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>RF</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.612094</td>\n",
       "      <td>0.609638</td>\n",
       "      <td>0.607583</td>\n",
       "      <td>0.615223</td>\n",
       "      <td>0.615223</td>\n",
       "      <td>0.615223</td>\n",
       "      <td>0.420483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Science</td>\n",
       "      <td>LogR</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.545957</td>\n",
       "      <td>0.573939</td>\n",
       "      <td>0.553788</td>\n",
       "      <td>0.556192</td>\n",
       "      <td>0.556192</td>\n",
       "      <td>0.556192</td>\n",
       "      <td>0.395599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Science</td>\n",
       "      <td>NB</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.561699</td>\n",
       "      <td>0.517430</td>\n",
       "      <td>0.453971</td>\n",
       "      <td>0.450082</td>\n",
       "      <td>0.450082</td>\n",
       "      <td>0.450082</td>\n",
       "      <td>0.296828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Science</td>\n",
       "      <td>SVM</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.506394</td>\n",
       "      <td>0.521487</td>\n",
       "      <td>0.512390</td>\n",
       "      <td>0.516367</td>\n",
       "      <td>0.516367</td>\n",
       "      <td>0.516367</td>\n",
       "      <td>0.333936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Science</td>\n",
       "      <td>RF</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.525798</td>\n",
       "      <td>0.520029</td>\n",
       "      <td>0.522173</td>\n",
       "      <td>0.538189</td>\n",
       "      <td>0.538189</td>\n",
       "      <td>0.538189</td>\n",
       "      <td>0.354148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>LogR</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.505812</td>\n",
       "      <td>0.524874</td>\n",
       "      <td>0.509368</td>\n",
       "      <td>0.547562</td>\n",
       "      <td>0.547562</td>\n",
       "      <td>0.547562</td>\n",
       "      <td>0.277355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>NB</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.527080</td>\n",
       "      <td>0.539225</td>\n",
       "      <td>0.470389</td>\n",
       "      <td>0.468462</td>\n",
       "      <td>0.468462</td>\n",
       "      <td>0.468462</td>\n",
       "      <td>0.242549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>SVM</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.483511</td>\n",
       "      <td>0.499856</td>\n",
       "      <td>0.486903</td>\n",
       "      <td>0.527105</td>\n",
       "      <td>0.527105</td>\n",
       "      <td>0.527105</td>\n",
       "      <td>0.241085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>English Language Arts</td>\n",
       "      <td>RF</td>\n",
       "      <td>SMOTE_perModel_Content</td>\n",
       "      <td>0.522913</td>\n",
       "      <td>0.501458</td>\n",
       "      <td>0.503697</td>\n",
       "      <td>0.588476</td>\n",
       "      <td>0.588476</td>\n",
       "      <td>0.588476</td>\n",
       "      <td>0.282908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Subject Model              Experiment  Macro_Precision  \\\n",
       "0                 English  LogR  SMOTE_perModel_Content         0.641619   \n",
       "1                 English    NB  SMOTE_perModel_Content         0.533327   \n",
       "2                 English   SVM  SMOTE_perModel_Content         0.608938   \n",
       "3                 English    RF  SMOTE_perModel_Content         0.612094   \n",
       "4                 Science  LogR  SMOTE_perModel_Content         0.545957   \n",
       "5                 Science    NB  SMOTE_perModel_Content         0.561699   \n",
       "6                 Science   SVM  SMOTE_perModel_Content         0.506394   \n",
       "7                 Science    RF  SMOTE_perModel_Content         0.525798   \n",
       "8   English Language Arts  LogR  SMOTE_perModel_Content         0.505812   \n",
       "9   English Language Arts    NB  SMOTE_perModel_Content         0.527080   \n",
       "10  English Language Arts   SVM  SMOTE_perModel_Content         0.483511   \n",
       "11  English Language Arts    RF  SMOTE_perModel_Content         0.522913   \n",
       "\n",
       "    Macro_Recall  Macro_fScore  Micro_Precision  Micro_Recall  Micro_fScore  \\\n",
       "0       0.643344      0.641765         0.648277      0.648277      0.648277   \n",
       "1       0.532080      0.527713         0.536313      0.536313      0.536313   \n",
       "2       0.609924      0.609245         0.614292      0.614292      0.614292   \n",
       "3       0.609638      0.607583         0.615223      0.615223      0.615223   \n",
       "4       0.573939      0.553788         0.556192      0.556192      0.556192   \n",
       "5       0.517430      0.453971         0.450082      0.450082      0.450082   \n",
       "6       0.521487      0.512390         0.516367      0.516367      0.516367   \n",
       "7       0.520029      0.522173         0.538189      0.538189      0.538189   \n",
       "8       0.524874      0.509368         0.547562      0.547562      0.547562   \n",
       "9       0.539225      0.470389         0.468462      0.468462      0.468462   \n",
       "10      0.499856      0.486903         0.527105      0.527105      0.527105   \n",
       "11      0.501458      0.503697         0.588476      0.588476      0.588476   \n",
       "\n",
       "    Cohens_Kappa  \n",
       "0       0.470931  \n",
       "1       0.301769  \n",
       "2       0.420298  \n",
       "3       0.420483  \n",
       "4       0.395599  \n",
       "5       0.296828  \n",
       "6       0.333936  \n",
       "7       0.354148  \n",
       "8       0.277355  \n",
       "9       0.242549  \n",
       "10      0.241085  \n",
       "11      0.282908  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#======================EXPERIMENT CONFIG===================\n",
    "experiment_name = 'SMOTE_perModel_Content'\n",
    "text_column = 'EssayText'\n",
    "\n",
    "#Empty array to store results\n",
    "tmp_results = []\n",
    "results_columns = ['Subject','Model','Experiment',\n",
    "                    'Macro_Precision','Macro_Recall','Macro_fScore',\n",
    "                    'Micro_Precision','Micro_Recall','Micro_fScore',\n",
    "                    'Cohens_Kappa']\n",
    "\n",
    "#Models and Vectorisers\n",
    "smote = SMOTE(ratio='auto',random_state=random_state)\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "models = [('LogR', LogisticRegression(random_state=random_state)),\n",
    "         ('NB', MultinomialNB()),\n",
    "          ('SVM',LinearSVC(random_state=random_state)),\n",
    "         ('RF',RandomForestClassifier(random_state=random_state))]\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "#Per Subject experiment\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for subject in list(df.subject.unique()):\n",
    "    print('\\n=========={}==========='.format(subject))\n",
    "    \n",
    "    X = df[(df['subject'] == subject)][[text_column,'Score1']].copy()\n",
    "    X.reset_index(drop=True,inplace=True)\n",
    "    y= X.pop('Score1')\n",
    "    \n",
    "    target_names = [str(i) for i in sorted(y.unique())] #Convert to string\n",
    "\n",
    "    print(subject, X.shape, y.shape)\n",
    "    \n",
    "    \n",
    "    # For each model\n",
    "    for model_name, clf in models:\n",
    "        print('\\n------------{}-------------'.format(model_name))\n",
    "\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "\n",
    "        print('Cross Validating...')\n",
    "        # data is an array with our already pre-processed dataset examples\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "#             print(\"Train:\", len(train_index), \"Test:\", len(test_index))\n",
    "            fold_X_train, fold_X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "            fold_y_train, fold_y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "            \n",
    "            fold_X_train = fold_X_train[text_column]\n",
    "            fold_X_test =  fold_X_test[text_column]\n",
    "            \n",
    "            # Make pipeline\n",
    "            pipe = make_pipeline(count_vectorizer,smote, clf)\n",
    "            pipe.fit(fold_X_train, fold_y_train)\n",
    "            \n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            fold_y_pred = pipe.predict(fold_X_test)\n",
    "\n",
    "            y_test.extend(fold_y_test)\n",
    "            y_pred.extend(fold_y_pred)\n",
    "            \n",
    "\n",
    "        print('Evaluating results...')    \n",
    "\n",
    "        myrow = {'Subject':subject, 'Model':model_name, 'Experiment':experiment_name}\n",
    "        myrow.update(get_evaluation_metrics(y_test, y_pred))\n",
    "\n",
    "        tmp_results.append(myrow)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "        print(report)\n",
    "        print(\"f1-macro: {:0.4f}  f1-micro: {:0.4f}\".\n",
    "              format(f1_score(y_test, y_pred, average='macro'),f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "    #     conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    #     fig, ax = plt.subplots(figsize=(5,5))\n",
    "    #     sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap=\"YlGnBu\")\n",
    "\n",
    "    #     plt.title('Set {}: {}'.format(str(essay_set),model_name))\n",
    "    #     plt.ylabel('Actual Class')\n",
    "    #     plt.xlabel('Predicted Class')\n",
    "\n",
    "    #     print(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set)))\n",
    "    #     plt.savefig(figureFolder/\"baseline_content/{}_{}_Set{}.svg\".format(model_name, experiment_name, str(essay_set))\", \n",
    "    #                 format=\"svg\", bbox_inches='tight')\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Get results, in a  DataFrame\n",
    "smote_sampling_results = pd.DataFrame(tmp_results, columns=results_columns)\n",
    "smote_sampling_results              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results to Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(figureFolder/'baseline_content/Baseline_Content_Words_2.xlsx')\n",
    "simple_baseline_results.to_excel(writer,'Simple_Baseline',index=False)\n",
    "baseline_mdl_results.to_excel(writer,'Baseline_per_model',index=False)\n",
    "no_stopword_results.to_excel(writer,'Removed_Stopwords',index=False)\n",
    "smote_sampling_results.to_excel(writer,'Smote_perModel', index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION BASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1339, 1) (1339,)\n",
      "Train: 1069 Test: 270\n",
      "(0.4535262298428251, 0.4392591015541835, 0.4329341058694658, None)\n",
      "(0.44814814814814813, 0.44814814814814813, 0.44814814814814813, None)\n",
      "(0.44540888919129373, 0.44814814814814813, 0.43417279772617495, None)\n",
      "0.24774210437741917\n",
      "Train: 1070 Test: 269\n",
      "(0.3831232492997199, 0.3890151515151515, 0.38478021978021976, None)\n",
      "(0.3940520446096654, 0.3940520446096654, 0.3940520446096654, None)\n",
      "(0.386872221007362, 0.3940520446096654, 0.3891923689693206, None)\n",
      "0.1832845940358001\n",
      "Train: 1071 Test: 268\n",
      "(0.42422500425821835, 0.41681792103478854, 0.4096899819898355, None)\n",
      "(0.4291044776119403, 0.4291044776119403, 0.4291044776119403, None)\n",
      "(0.4235424182104297, 0.4291044776119403, 0.4164891921006229, None)\n",
      "0.2258283772302463\n",
      "Train: 1073 Test: 266\n",
      "(0.4558498399359744, 0.46317904468114, 0.4547386851944863, None)\n",
      "(0.46616541353383456, 0.46616541353383456, 0.46616541353383456, None)\n",
      "(0.45407817262243244, 0.46616541353383456, 0.45534104598512853, None)\n",
      "0.27851317020992117\n",
      "Train: 1073 Test: 266\n",
      "(0.4527128427128427, 0.4645752090487554, 0.4433417023580958, None)\n",
      "(0.46616541353383456, 0.46616541353383456, 0.46616541353383456, None)\n",
      "(0.4493721316277707, 0.46616541353383456, 0.44332097412339, None)\n",
      "0.27825123246837624\n"
     ]
    }
   ],
   "source": [
    "X = set_1[['function_based_text','Score1']].copy()\n",
    "X.reset_index(drop=True,inplace=True)\n",
    "y= X.pop('Score1')\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "#Initialise vectoriser\n",
    "count_vectorizer = CountVectorizer()\n",
    "clf = LogisticRegression() #OvR classifier\n",
    "target_names = [str(i) for i in sorted(y.unique())] #Convert to string\n",
    "\n",
    "\n",
    "# data is an array with our already pre-processed dataset examples\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    print(\"Train:\", len(train_index), \"Test:\", len(test_index))\n",
    "    X_train, X_test = X.reindex(train_index), X.reindex(test_index)\n",
    "    y_train, y_test = y.reindex(train_index), y.reindex(test_index)\n",
    "    \n",
    "    #Make pipeline to train classifier\n",
    "    pipe = make_pipeline(count_vectorizer, clf)\n",
    "    pipe.fit(X_train['function_based_text'], y_train)\n",
    "    \n",
    "    #Make predictions on test data\n",
    "    y_pred = pipe.predict(X_test['function_based_text'])\n",
    "    \n",
    "    print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
    "    print(precision_recall_fscore_support(y_test, y_pred, average='micro'))\n",
    "    print(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "    print(cohen_kappa_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.68      0.58       301\n",
      "          1       0.32      0.25      0.28       348\n",
      "          2       0.42      0.31      0.36       417\n",
      "          3       0.38      0.47      0.42       273\n",
      "\n",
      "avg / total       0.40      0.41      0.40      1339\n",
      "\n",
      "accuracy: 0.412\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "print(report)\n",
    "print(\"accuracy: {:0.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap=\"YlGnBu\")\n",
    "\n",
    "plt.title('Set {}: {}'.format('1','LogR'))\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Features & Prepare X & Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.SparseDataFrame(data=merged_2, columns=feature_names)\n",
    "X_train.head()\n",
    "\n",
    "#X_train = csr_matrix(merged_2) \n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1170,)\n"
     ]
    }
   ],
   "source": [
    "#y_train_tmp = pd.DataFrame(y_train_tmp)\n",
    "# y_train_scaled = StandardScaler().fit_transform(y_train_tmp.astype(float))\n",
    "# y_train_scaled = pd.DataFrame(y_train_scaled, index=y_train_tmp.index, columns=['Score1'])\n",
    "# y_train = np.squeeze(y_train_scaled)\n",
    "\n",
    "\n",
    "y_train = y_train_tmp\n",
    "\n",
    "#print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1170, 264)\n"
     ]
    }
   ],
   "source": [
    "print(text_ngrams_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION\n",
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\stylistics\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif, mutual_info_classif\n",
    "#from sklearn.linear_model import Lasso\n",
    "#from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "\n",
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "\n",
    "from yellowbrick.features.importances import FeatureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Selection\n",
    "* Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "kBestmodel = SelectKBest(mutual_info_classif, k='all')\n",
    "fit_data = kBestmodel.fit(text_ngrams_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>kBest_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>__VERB__</td>\n",
       "      <td>0.087464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>__NOUN__</td>\n",
       "      <td>0.078108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>to</td>\n",
       "      <td>0.070309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>__ADV__</td>\n",
       "      <td>0.057497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>of</td>\n",
       "      <td>0.052992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  kBest_weight\n",
       "108  __VERB__  0.087464    \n",
       "106  __NOUN__  0.078108    \n",
       "238  to        0.070309    \n",
       "105  __ADV__   0.057497    \n",
       "191  of        0.052992    "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = list(zip(text_ngrams_columns, fit_data.scores_))\n",
    "kBest_feature_importance = pd.DataFrame(feature_importance, columns=['feature','kBest_weight'])\n",
    "kBest_feature_importance.sort_values(by='kBest_weight', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>kBest_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>zero</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>inside</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>it</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>materialinto</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>me</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>60minsand</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4s</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  kBest_weight\n",
       "263  zero          0.0         \n",
       "176  inside        0.0         \n",
       "178  it            0.0         \n",
       "92   8             0.0         \n",
       "181  materialinto  0.0         \n",
       "182  me            0.0         \n",
       "87   60minsand     0.0         \n",
       "188  no            0.0         \n",
       "84   5             0.0         \n",
       "83   4s            0.0         "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kBest_feature_importance.sort_values(by='kBest_weight').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Baseline Model\n",
    "* Logitistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(pipe, X, y, target_names):\n",
    "    y_test = y\n",
    "    y_pred = pipe.predict(X)\n",
    "    report = metrics.classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print(report)\n",
    "    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ...2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Were using LogisticRegressionCV here to adjust regularization parameter C automatically. \n",
    "# It allows to compare different vectorizers - optimal C value could be different for different input features \n",
    "# (e.g. for bigrams or for character-level input).\n",
    "# An alternative would be to use GridSearchCV or RandomizedSearchCV.\n",
    "\n",
    "\n",
    "#Initialise vectoriser\n",
    "count_vectorizer = CountVectorizer()\n",
    "clf = LogisticRegressionCV()\n",
    "target_names = [str(i) for i in sorted(y_train_tmp.unique())] #Convert to string\n",
    "\n",
    "pipe = make_pipeline(count_vectorizer, clf)\n",
    "pipe.fit(train['EssayText'], y_train_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.95      0.83       268\n",
      "          1       0.94      0.43      0.59       309\n",
      "          2       0.84      0.97      0.90       593\n",
      "\n",
      "avg / total       0.84      0.82      0.80      1170\n",
      "\n",
      "accuracy: 0.824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.89      0.84       112\n",
      "          1       0.71      0.30      0.42       120\n",
      "          2       0.78      0.94      0.85       270\n",
      "\n",
      "avg / total       0.76      0.77      0.75       502\n",
      "\n",
      "accuracy: 0.775\n"
     ]
    }
   ],
   "source": [
    "print_report(pipe,train['EssayText'], y_train_tmp, target_names)\n",
    "\n",
    "print_report(pipe,test['EssayText'], y_test_tmp, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights-wrapper\" style=\"border-collapse: collapse; border: none; margin-bottom: 1.5em;\">\n",
       "            <tr>\n",
       "                \n",
       "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
       "                        <b>\n",
       "    \n",
       "        y=0\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "                    </td>\n",
       "                \n",
       "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
       "                        <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "                    </td>\n",
       "                \n",
       "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
       "                        <b>\n",
       "    \n",
       "        y=2\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "                    </td>\n",
       "                \n",
       "            </tr>\n",
       "            <tr>\n",
       "                \n",
       "                    \n",
       "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
       "                            \n",
       "                                \n",
       "                                    \n",
       "                                    \n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.19%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.191\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 94.77%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.422\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        mass\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.22%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.266\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        it\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.28%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.260\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        will\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.45%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.243\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        then\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.63%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.226\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        them\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 96.63%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 682 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 96.56%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 1240 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 96.56%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.232\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        size\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 96.18%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.269\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        temperature\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 95.75%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.314\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        are\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 95.18%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.376\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        what\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 94.21%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.489\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        amount\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 93.74%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.546\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        used\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 93.32%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.599\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        how\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 93.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.640\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        much\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 92.76%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.671\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        vinegar\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "                                \n",
       "                            \n",
       "                        </td>\n",
       "                    \n",
       "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
       "                            \n",
       "                                \n",
       "                                    \n",
       "                                    \n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.82%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        is\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.82%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        they\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        you\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.83%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 685 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.85%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 1237 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.85%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        size\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.84%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        would\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        should\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        the\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        what\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        need\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        containers\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.80%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        be\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.79%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        samples\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.79%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        container\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.79%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        know\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 90.56%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.982\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "                                \n",
       "                            \n",
       "                        </td>\n",
       "                    \n",
       "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
       "                            \n",
       "                                \n",
       "                                    \n",
       "                                    \n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 87.10%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.533\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        size\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.16%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.196\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        long\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.065\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        temperature\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.14%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.044\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        where\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.002\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        type\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.64%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.969\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        big\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.91%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.930\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        kind\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.03%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.913\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        vinegar\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.51%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.844\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        container\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.84%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.797\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        rinse\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 92.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.702\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        must\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 92.53%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 920 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 92.25%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 1002 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 92.25%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.740\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        hypothesis\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.818\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        many\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.917\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        from\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -2.869\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "                                \n",
       "                            \n",
       "                        </td>\n",
       "                    \n",
       "                \n",
       "            </tr>\n",
       "        </table>\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intercept (bias) feature is shown as <BIAS> in the same table. \n",
    "#We can inspect features and weights because were using a bag-of-words vectorizer and a linear classifier\n",
    "#(so there is a direct mapping between individual words and classifier coefficients)\n",
    "\n",
    "#eli5.show_weights(clf, top=10)\n",
    "eli5.show_weights(clf, vec=count_vectorizer, top=15,  target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=0\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.126</b>, score <b>-2.409</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.78%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.191\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -3.600\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 87.36%); opacity: 0.84\" title=\"-0.176\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.16%); opacity: 0.81\" title=\"-0.032\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.57%); opacity: 0.82\" title=\"0.099\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.06%); opacity: 0.82\" title=\"0.075\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.96%); opacity: 0.83\" title=\"-0.145\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.93%); opacity: 0.80\" title=\"-0.005\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.73%); opacity: 0.82\" title=\"-0.113\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.29%); opacity: 0.80\" title=\"-0.010\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.57%); opacity: 0.82\" title=\"0.099\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.13%); opacity: 0.81\" title=\"-0.059\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.06%); opacity: 0.81\" title=\"-0.060\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.42%); opacity: 0.80\" title=\"0.002\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.16%); opacity: 0.91\" title=\"-0.489\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.33%); opacity: 0.83\" title=\"-0.157\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 67.74%); opacity: 0.95\" title=\"-0.671\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.97%); opacity: 0.84\" title=\"-0.164\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.38%); opacity: 0.83\" title=\"-0.156\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.62%); opacity: 0.82\" title=\"-0.082\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.79%); opacity: 0.83\" title=\"-0.148\">container</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 96.16%); opacity: 0.81\" title=\"-0.032\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.53%); opacity: 0.81\" title=\"0.028\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.03%); opacity: 0.82\" title=\"0.108\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.16%); opacity: 0.91\" title=\"-0.489\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.33%); opacity: 0.83\" title=\"-0.157\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 76.68%); opacity: 0.89\" title=\"0.422\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.87%); opacity: 0.81\" title=\"-0.024\">lost</span><span style=\"opacity: 0.80\"> fom </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.35%); opacity: 0.84\" title=\"-0.176\">samples</span><span style=\"opacity: 0.80\">. i </span><span style=\"background-color: hsl(0, 100.00%, 92.16%); opacity: 0.82\" title=\"-0.089\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.65%); opacity: 0.84\" title=\"0.170\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.86%); opacity: 0.81\" title=\"0.036\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.97%); opacity: 0.85\" title=\"0.226\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.36%); opacity: 0.84\" title=\"-0.176\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.64%); opacity: 0.80\" title=\"0.016\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.52%); opacity: 0.80\" title=\"0.017\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.35%); opacity: 0.83\" title=\"0.157\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.51%); opacity: 0.80\" title=\"-0.002\">different</span><span style=\"opacity: 0.80\"> temparatures </span><span style=\"background-color: hsl(120, 100.00%, 97.72%); opacity: 0.80\" title=\"0.015\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 70.23%); opacity: 0.93\" title=\"-0.599\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 68.80%); opacity: 0.94\" title=\"-0.640\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.54%); opacity: 0.81\" title=\"-0.067\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.35%); opacity: 0.84\" title=\"-0.176\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.13%); opacity: 0.81\" title=\"-0.059\">would</span><span style=\"opacity: 0.80\"> loose </span><span style=\"background-color: hsl(120, 100.00%, 76.68%); opacity: 0.89\" title=\"0.422\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.404</b>, score <b>-1.027</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.07%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.045\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.94%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.982\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 99.61%); opacity: 0.80\" title=\"-0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.71%); opacity: 0.80\" title=\"0.001\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.69%); opacity: 0.80\" title=\"0.001\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.94%); opacity: 0.80\" title=\"-0.000\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.91%); opacity: 0.80\" title=\"0.000\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.95%); opacity: 0.80\" title=\"0.000\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.54%); opacity: 0.80\" title=\"-0.002\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.92%); opacity: 0.80\" title=\"0.000\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.69%); opacity: 0.80\" title=\"0.001\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.30%); opacity: 0.80\" title=\"-0.003\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.11%); opacity: 0.80\" title=\"-0.004\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.67%); opacity: 0.80\" title=\"0.001\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.68%); opacity: 0.80\" title=\"0.001\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.32%); opacity: 0.80\" title=\"-0.003\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.54%); opacity: 0.80\" title=\"-0.002\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.19%); opacity: 0.80\" title=\"0.003\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.87%); opacity: 0.80\" title=\"0.000\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.82%); opacity: 0.80\" title=\"-0.000\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.07%); opacity: 0.80\" title=\"-0.004\">container</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 99.71%); opacity: 0.80\" title=\"0.001\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.79%); opacity: 0.80\" title=\"0.000\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.83%); opacity: 0.80\" title=\"-0.000\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.68%); opacity: 0.80\" title=\"0.001\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.32%); opacity: 0.80\" title=\"-0.003\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.86%); opacity: 0.80\" title=\"-0.000\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.84%); opacity: 0.80\" title=\"0.000\">lost</span><span style=\"opacity: 0.80\"> fom </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.09%); opacity: 0.80\" title=\"-0.004\">samples</span><span style=\"opacity: 0.80\">. i </span><span style=\"background-color: hsl(0, 100.00%, 99.52%); opacity: 0.80\" title=\"-0.002\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.87%); opacity: 0.80\" title=\"-0.000\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.91%); opacity: 0.80\" title=\"-0.000\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.69%); opacity: 0.80\" title=\"-0.001\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.61%); opacity: 0.80\" title=\"-0.001\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.92%); opacity: 0.80\" title=\"0.000\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.97%); opacity: 0.80\" title=\"-0.000\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.85%); opacity: 0.80\" title=\"-0.000\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.97%); opacity: 0.80\" title=\"-0.000\">different</span><span style=\"opacity: 0.80\"> temparatures </span><span style=\"background-color: hsl(0, 100.00%, 99.96%); opacity: 0.80\" title=\"-0.000\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.96%); opacity: 0.80\" title=\"-0.000\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.86%); opacity: 0.80\" title=\"0.000\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.25%); opacity: 0.80\" title=\"-0.003\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.09%); opacity: 0.80\" title=\"-0.004\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.30%); opacity: 0.80\" title=\"-0.003\">would</span><span style=\"opacity: 0.80\"> loose </span><span style=\"background-color: hsl(0, 100.00%, 99.86%); opacity: 0.80\" title=\"-0.000\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=2\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.470</b>, score <b>-0.814</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 86.49%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +2.055\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 82.94%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -2.869\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 98.25%); opacity: 0.80\" title=\"0.010\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.40%); opacity: 0.84\" title=\"0.195\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.81%); opacity: 0.81\" title=\"-0.049\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.98%); opacity: 0.81\" title=\"-0.047\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.46%); opacity: 0.85\" title=\"0.236\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.67%); opacity: 0.81\" title=\"-0.038\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.00%); opacity: 0.83\" title=\"-0.126\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.65%); opacity: 0.81\" title=\"0.038\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.81%); opacity: 0.81\" title=\"-0.049\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.56%); opacity: 0.83\" title=\"-0.116\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.43%); opacity: 0.86\" title=\"0.282\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.99%); opacity: 0.83\" title=\"-0.126\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.38%); opacity: 0.85\" title=\"0.238\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.68%); opacity: 0.80\" title=\"-0.016\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.913\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.49%); opacity: 0.86\" title=\"-0.258\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.73%); opacity: 0.82\" title=\"0.080\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.31%); opacity: 0.82\" title=\"-0.087\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 62.14%); opacity: 0.98\" title=\"0.844\">container</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 86.40%); opacity: 0.84\" title=\"0.195\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.75%); opacity: 0.90\" title=\"-0.473\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 77.14%); opacity: 0.89\" title=\"0.410\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.38%); opacity: 0.85\" title=\"0.238\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.68%); opacity: 0.80\" title=\"-0.016\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 73.82%); opacity: 0.91\" title=\"-0.498\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.48%); opacity: 0.85\" title=\"-0.236\">lost</span><span style=\"opacity: 0.80\"> fom </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 73.68%); opacity: 0.91\" title=\"0.502\">samples</span><span style=\"opacity: 0.80\">. i </span><span style=\"background-color: hsl(120, 100.00%, 88.96%); opacity: 0.83\" title=\"0.145\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.19%); opacity: 0.84\" title=\"-0.200\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.09%); opacity: 0.81\" title=\"-0.033\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.67%); opacity: 0.83\" title=\"-0.114\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.25%); opacity: 0.80\" title=\"0.010\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.76%); opacity: 0.85\" title=\"-0.209\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.74%); opacity: 0.81\" title=\"-0.037\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.43%); opacity: 0.84\" title=\"-0.175\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.75%); opacity: 0.80\" title=\"0.006\">different</span><span style=\"opacity: 0.80\"> temparatures </span><span style=\"background-color: hsl(0, 100.00%, 96.82%); opacity: 0.81\" title=\"-0.024\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.27%); opacity: 0.83\" title=\"0.121\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.27%); opacity: 0.85\" title=\"0.219\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.50%); opacity: 0.81\" title=\"0.068\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 73.68%); opacity: 0.91\" title=\"0.502\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.56%); opacity: 0.83\" title=\"-0.116\">would</span><span style=\"opacity: 0.80\"> loose </span><span style=\"background-color: hsl(0, 100.00%, 73.82%); opacity: 0.91\" title=\"-0.498\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_prediction(clf, test['EssayText'][438], vec=count_vectorizer, target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def test_print_report(pipe):\n",
    "#     y_test = y_test_tmp\n",
    "#     y_pred = pipe.predict(test['EssayText'])\n",
    "#     report = metrics.classification_report(y_test, y_pred,\n",
    "#         target_names=target_names)\n",
    "#     print(report)\n",
    "#     print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise vectoriser\n",
    "clf = MultinomialNB()\n",
    "pipe = make_pipeline(count_vectorizer, clf)\n",
    "pipe.fit(train['EssayText'], y_train_tmp);\n",
    "#MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.75      0.84       268\n",
      "          1       0.81      0.58      0.68       309\n",
      "          2       0.78      0.97      0.87       593\n",
      "\n",
      "avg / total       0.83      0.82      0.81      1170\n",
      "\n",
      "accuracy: 0.819\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.50      0.63       112\n",
      "          1       0.42      0.23      0.30       120\n",
      "          2       0.70      0.96      0.81       270\n",
      "\n",
      "avg / total       0.67      0.68      0.65       502\n",
      "\n",
      "accuracy: 0.681\n"
     ]
    }
   ],
   "source": [
    "print_report(pipe,train['EssayText'], y_train_tmp, target_names)\n",
    "\n",
    "print_report(pipe,test['EssayText'], y_test_tmp, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top10(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (class_label, \" \".join(feature_names[j] for j in top10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: experiment in would need mass of you and to the\n",
      "1: and experiment how you in need would of to the\n",
      "2: what vinegar know how in need would of to the\n"
     ]
    }
   ],
   "source": [
    "print_top10(count_vectorizer, clf, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-9.2963\t100            \t\t-2.6005\tthe            \n",
      "\t-9.2963\t100g           \t\t-3.1849\tto             \n",
      "\t-9.2963\t100ml          \t\t-3.7590\tand            \n",
      "\t-9.2963\t10cm           \t\t-3.8847\tyou            \n",
      "\t-9.2963\t10g            \t\t-3.8847\tof             \n",
      "\t-9.2963\t11grams        \t\t-3.9118\tmass           \n",
      "\t-9.2963\t1day           \t\t-3.9350\tneed           \n",
      "\t-9.2963\t1st            \t\t-3.9980\twould          \n",
      "\t-9.2963\t1telling       \t\t-4.0285\tin             \n",
      "\t-9.2963\t2days          \t\t-4.2725\texperiment     \n",
      "\t-9.2963\t30min          \t\t-4.4923\tsamples        \n",
      "\t-9.2963\t3rd            \t\t-4.5601\tthey           \n",
      "\t-9.2963\t50g            \t\t-4.6142\tit             \n",
      "\t-9.2963\t6th            \t\t-4.6424\tis             \n",
      "\t-9.2963\t7grams         \t\t-4.7320\tfor            \n",
      "\t-9.2963\tabove          \t\t-4.7530\twhat           \n",
      "\t-9.2963\tabsolutely     \t\t-4.7745\tsample         \n",
      "\t-9.2963\tabsorb         \t\t-4.7965\teach           \n",
      "\t-9.2963\tabsorbed       \t\t-4.8304\tinformation    \n",
      "\t-9.2963\taccuracy       \t\t-4.8420\tthis           \n"
     ]
    }
   ],
   "source": [
    "# It means that higher values mean more important features for the positive class.\n",
    "# The above print shows the top 20 lowest values (less predictive features) in the first column \n",
    "#and the top 20 high values (highest predictive features) in the second column.\n",
    "\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_informative_features(count_vectorizer, clf, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise vectoriser\n",
    "clf = BernoulliNB()\n",
    "pipe = make_pipeline(count_vectorizer, clf)\n",
    "pipe.fit(train['EssayText'], y_train);\n",
    "#MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.61      0.63       268\n",
      "          1       0.53      0.27      0.36       309\n",
      "          2       0.68      0.86      0.76       593\n",
      "\n",
      "avg / total       0.63      0.65      0.62      1170\n",
      "\n",
      "accuracy: 0.649\n"
     ]
    }
   ],
   "source": [
    "print_report(pipe,train['EssayText'], y_train, target_names)\n",
    "\n",
    "print_report(pipe,test['EssayText'], y_test_tmp, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.55      0.59       112\n",
      "          1       0.41      0.16      0.23       120\n",
      "          2       0.65      0.86      0.74       270\n",
      "\n",
      "avg / total       0.59      0.63      0.59       502\n",
      "\n",
      "accuracy: 0.625\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf = svm.SVC(gamma='scale', decision_function_shape='ovo')\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "pipe = make_pipeline(count_vectorizer, clf)\n",
    "pipe.fit(train['EssayText'], y_train_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98       268\n",
      "          1       0.99      0.97      0.98       309\n",
      "          2       0.99      0.98      0.98       593\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1170\n",
      "\n",
      "accuracy: 0.982\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.71      0.71       112\n",
      "          1       0.46      0.49      0.48       120\n",
      "          2       0.81      0.79      0.80       270\n",
      "\n",
      "avg / total       0.70      0.70      0.70       502\n",
      "\n",
      "accuracy: 0.699\n"
     ]
    }
   ],
   "source": [
    "print_report(pipe,train['EssayText'], y_train_tmp, target_names)\n",
    "\n",
    "print_report(pipe,test['EssayText'], y_test_tmp, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=0\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.290</b>, score <b>-0.712</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 94.85%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.163\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 83.34%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.875\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 95.29%); opacity: 0.81\" title=\"-0.038\">in</span><span style=\"opacity: 0.80\"> this </span><span style=\"background-color: hsl(120, 100.00%, 97.64%); opacity: 0.80\" title=\"0.014\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.20%); opacity: 0.90\" title=\"0.410\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.49%); opacity: 0.87\" title=\"-0.291\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.07%); opacity: 0.81\" title=\"-0.029\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.66%); opacity: 0.82\" title=\"-0.086\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.65%); opacity: 0.80\" title=\"0.014\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.86%); opacity: 0.81\" title=\"0.056\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.86%); opacity: 0.81\" title=\"0.056\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.37%); opacity: 0.85\" title=\"0.193\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.74%); opacity: 0.85\" title=\"-0.205\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.46%); opacity: 0.80\" title=\"-0.016\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.90%); opacity: 0.81\" title=\"0.021\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 66.09%); opacity: 0.96\" title=\"-0.641\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.03%); opacity: 0.80\" title=\"-0.020\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.90%); opacity: 0.90\" title=\"-0.417\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.90%); opacity: 0.81\" title=\"0.055\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.60%); opacity: 0.86\" title=\"-0.227\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 83.91%); opacity: 0.85\" title=\"0.221\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.15%); opacity: 0.82\" title=\"-0.065\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.41%); opacity: 0.88\" title=\"-0.314\">container</span><span style=\"opacity: 0.80\">. this </span><span style=\"background-color: hsl(120, 100.00%, 77.81%); opacity: 0.89\" title=\"0.350\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.79%); opacity: 0.85\" title=\"0.185\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.75%); opacity: 0.81\" title=\"-0.057\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 66.09%); opacity: 0.96\" title=\"-0.641\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.03%); opacity: 0.80\" title=\"-0.020\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.68%); opacity: 0.88\" title=\"0.308\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.90%); opacity: 0.81\" title=\"-0.031\">lost</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.53%); opacity: 0.80\" title=\"0.007\">fom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.49%); opacity: 0.81\" title=\"-0.036\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.81%); opacity: 0.86\" title=\"-0.243\">samples</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 92.51%); opacity: 0.82\" title=\"-0.074\">i</span><span style=\"opacity: 0.80\"> also </span><span style=\"background-color: hsl(120, 100.00%, 74.16%); opacity: 0.91\" title=\"0.435\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.96%); opacity: 0.83\" title=\"0.129\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 69.68%); opacity: 0.93\" title=\"0.546\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.46%); opacity: 0.81\" title=\"0.025\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.49%); opacity: 0.84\" title=\"0.154\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.52%); opacity: 0.87\" title=\"0.269\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.02%); opacity: 0.81\" title=\"0.041\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.96%); opacity: 0.84\" title=\"-0.146\">different</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.56%); opacity: 0.80\" title=\"-0.007\">temparatures</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.46%); opacity: 0.80\" title=\"-0.008\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.32%); opacity: 0.87\" title=\"-0.295\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-0.811\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.49%); opacity: 0.81\" title=\"-0.036\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.68%); opacity: 0.84\" title=\"-0.169\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.37%); opacity: 0.85\" title=\"0.193\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.54%); opacity: 0.80\" title=\"-0.015\">loose</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.38%); opacity: 0.87\" title=\"0.293\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.175</b>, score <b>-1.396</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 90.61%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.386\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 81.57%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.011\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 91.88%); opacity: 0.82\" title=\"-0.083\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.72%); opacity: 0.81\" title=\"-0.023\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.10%); opacity: 0.80\" title=\"-0.004\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.76%); opacity: 0.83\" title=\"-0.132\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.08%); opacity: 0.82\" title=\"0.095\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.14%); opacity: 0.80\" title=\"-0.010\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.19%); opacity: 0.81\" title=\"0.039\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.44%); opacity: 0.82\" title=\"-0.090\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.50%); opacity: 0.82\" title=\"-0.089\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.60%); opacity: 0.81\" title=\"-0.046\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.86%); opacity: 0.82\" title=\"0.069\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.96%); opacity: 0.82\" title=\"-0.082\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.32%); opacity: 0.83\" title=\"0.140\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.02%); opacity: 0.80\" title=\"0.011\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.12%); opacity: 0.87\" title=\"0.299\">amount</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 96.12%); opacity: 0.81\" title=\"0.029\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.29%); opacity: 0.83\" title=\"0.124\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.14%); opacity: 0.81\" title=\"-0.040\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.11%); opacity: 0.82\" title=\"0.066\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.11%); opacity: 0.82\" title=\"0.066\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.71%); opacity: 0.86\" title=\"-0.225\">container</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 93.89%); opacity: 0.81\" title=\"-0.055\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.99%); opacity: 0.80\" title=\"0.020\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 68.42%); opacity: 0.94\" title=\"-0.579\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.00%); opacity: 0.80\" title=\"0.004\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.12%); opacity: 0.87\" title=\"0.299\">amount</span><span style=\"opacity: 0.80\"> of </span><span style=\"background-color: hsl(120, 100.00%, 93.21%); opacity: 0.82\" title=\"0.064\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.96%); opacity: 0.85\" title=\"0.201\">lost</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.34%); opacity: 0.82\" title=\"0.063\">fom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.85%); opacity: 0.81\" title=\"0.043\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.98%); opacity: 0.82\" title=\"-0.082\">samples</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 93.13%); opacity: 0.82\" title=\"0.065\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.55%); opacity: 0.81\" title=\"-0.024\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.64%); opacity: 0.86\" title=\"-0.226\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.04%); opacity: 0.83\" title=\"-0.111\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.05%); opacity: 0.85\" title=\"-0.199\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.53%); opacity: 0.81\" title=\"-0.047\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.92%); opacity: 0.80\" title=\"0.005\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.74%); opacity: 0.84\" title=\"-0.168\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.47%); opacity: 0.82\" title=\"-0.089\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.16%); opacity: 0.84\" title=\"0.178\">different</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.43%); opacity: 0.81\" title=\"0.049\">temparatures</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.12%); opacity: 0.81\" title=\"0.040\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.13%); opacity: 0.84\" title=\"0.161\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.31%); opacity: 0.87\" title=\"0.274\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.69%); opacity: 0.81\" title=\"0.023\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.74%); opacity: 0.83\" title=\"-0.116\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.34%); opacity: 0.82\" title=\"0.063\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.42%); opacity: 0.82\" title=\"0.062\">loose</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.99%); opacity: 0.81\" title=\"0.042\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=2\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.535</b>, score <b>0.435</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.136\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.74%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.701\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 86.08%); opacity: 0.84\" title=\"0.180\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.52%); opacity: 0.81\" title=\"0.047\">this</span><span style=\"opacity: 0.80\"> experiment </span><span style=\"background-color: hsl(0, 100.00%, 79.90%); opacity: 0.87\" title=\"-0.304\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.76%); opacity: 0.85\" title=\"0.204\">additional</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.34%); opacity: 0.83\" title=\"0.123\">iformation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.17%); opacity: 0.82\" title=\"0.094\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.23%); opacity: 0.81\" title=\"0.039\">perform</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.59%); opacity: 0.81\" title=\"0.047\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.52%); opacity: 0.81\" title=\"0.036\">experiment</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.73%); opacity: 0.83\" title=\"-0.133\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.16%); opacity: 0.88\" title=\"0.342\">be</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.16%); opacity: 0.81\" title=\"-0.029\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.84%); opacity: 0.82\" title=\"0.084\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.41%); opacity: 0.88\" title=\"0.336\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.05%); opacity: 0.80\" title=\"0.011\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 67.07%); opacity: 0.95\" title=\"0.615\">vinegar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.78%); opacity: 0.81\" title=\"-0.057\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 77.01%); opacity: 0.89\" title=\"0.368\">poured</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.84%); opacity: 0.86\" title=\"-0.242\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.87%); opacity: 0.83\" title=\"0.114\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 65.86%); opacity: 0.96\" title=\"0.647\">container</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 90.82%); opacity: 0.82\" title=\"0.099\">this</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.97%); opacity: 0.87\" title=\"-0.281\">may</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 76.25%); opacity: 0.89\" title=\"0.385\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.13%); opacity: 0.84\" title=\"0.161\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.41%); opacity: 0.88\" title=\"0.336\">amount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.80%); opacity: 0.80\" title=\"-0.005\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.71%); opacity: 0.87\" title=\"-0.286\">mass</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.55%); opacity: 0.82\" title=\"-0.074\">lost</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.34%); opacity: 0.82\" title=\"0.063\">fom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.12%); opacity: 0.82\" title=\"0.066\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.94%); opacity: 0.87\" title=\"0.281\">samples</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 98.69%); opacity: 0.80\" title=\"-0.006\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.32%); opacity: 0.82\" title=\"0.091\">also</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.87%); opacity: 0.85\" title=\"-0.202\">think</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.26%); opacity: 0.81\" title=\"-0.039\">placing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.67%); opacity: 0.87\" title=\"-0.287\">them</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.97%); opacity: 0.84\" title=\"0.163\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.76%); opacity: 0.85\" title=\"-0.204\">certain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.46%); opacity: 0.82\" title=\"-0.089\">places</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.37%); opacity: 0.80\" title=\"-0.017\">with</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.23%); opacity: 0.82\" title=\"-0.093\">different</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.72%); opacity: 0.82\" title=\"0.071\">temparatures</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.49%); opacity: 0.82\" title=\"0.074\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.94%); opacity: 0.86\" title=\"0.240\">how</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 63.34%); opacity: 0.98\" title=\"0.716\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.68%); opacity: 0.82\" title=\"0.086\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.74%); opacity: 0.87\" title=\"0.265\">samples</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.04%); opacity: 0.84\" title=\"-0.180\">would</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.85%); opacity: 0.81\" title=\"0.043\">loose</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.48%); opacity: 0.88\" title=\"-0.313\">mass</span><span style=\"opacity: 0.80\">.</span>\n",
       "    </p>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = TextExplainer(random_state=42)\n",
    "te.fit(test['EssayText'][438], pipe.predict_proba)\n",
    "te.show_prediction(target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99       268\n",
      "          1       1.00      0.97      0.99       309\n",
      "          2       0.99      1.00      0.99       593\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1170\n",
      "\n",
      "accuracy: 0.991\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.49      0.57       112\n",
      "          1       0.24      0.20      0.22       120\n",
      "          2       0.66      0.79      0.72       270\n",
      "\n",
      "avg / total       0.56      0.58      0.56       502\n",
      "\n",
      "accuracy: 0.580\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "pipe = make_pipeline(count_vectorizer, clf)\n",
    "pipe.fit(train['EssayText'], y_train)\n",
    "\n",
    "print_report(pipe,train['EssayText'], y_train, target_names)\n",
    "\n",
    "print_report(pipe,test['EssayText'], y_test_tmp, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0732\n",
       "                \n",
       "                    &plusmn; 0.0558\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                __VERB__\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 83.61%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0551\n",
       "                \n",
       "                    &plusmn; 0.0457\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                __ADV__\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.44%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0512\n",
       "                \n",
       "                    &plusmn; 0.0356\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                __NOUN__\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.67%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0501\n",
       "                \n",
       "                    &plusmn; 0.0396\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                to\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.67%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0410\n",
       "                \n",
       "                    &plusmn; 0.0193\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                __ADJ__\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.26%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0385\n",
       "                \n",
       "                    &plusmn; 0.0211\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                the\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0374\n",
       "                \n",
       "                    &plusmn; 0.0178\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                of\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.34%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0339\n",
       "                \n",
       "                    &plusmn; 0.0179\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                each\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.82%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0319\n",
       "                \n",
       "                    &plusmn; 0.0210\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                in\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.61%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0287\n",
       "                \n",
       "                    &plusmn; 0.0173\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                .\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.05%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0270\n",
       "                \n",
       "                    &plusmn; 0.0179\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                i\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.11%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0268\n",
       "                \n",
       "                    &plusmn; 0.0176\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ,\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0261\n",
       "                \n",
       "                    &plusmn; 0.0134\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                and\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.44%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0255\n",
       "                \n",
       "                    &plusmn; 0.0108\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                you\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.96%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0236\n",
       "                \n",
       "                    &plusmn; 0.0150\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                they\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.96%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 249 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(clf, vec=count_vectorizer, top=15,  target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = SVC(kernel=\"linear\")\n",
    "# The \"accuracy\" scoring is proportional to the number of correct\n",
    "# classifications\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2), scoring='accuracy')\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Based \n",
    "* Random Forests\n",
    "* GradientBoosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "rf_features = RFECV(RandomForestClassifier(n_estimators=100), cv=StratifiedKFold(5), scoring='f1_weighted')\n",
    "rf_features.fit(X_train, y_train)\n",
    "rf_features.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new matplotlib figure\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot()\n",
    "\n",
    "gb_model = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "feature_importances = gb_model.feature_importances_\n",
    "\n",
    "eli5.show_weights(gb_model,feature_names=feature_names.tolist(), top=50, feature_filter=lambda x: x != '<BIAS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=150, gamma=2e-2, probability=True)\n",
    "\n",
    "pipe = make_pipeline(clf)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(doc):\n",
    "    y_pred = pipe.predict_proba([doc])[0]\n",
    "    #print(y_pred)\n",
    "    for target, prob in zip(y_train, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))\n",
    "\n",
    "doc = X_train.loc[0,]\n",
    "print_prediction(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_df = pd.SparseDataFrame(X_train)\n",
    "# sparse_df.head()\n",
    "\n",
    "sparse_df.fillna(0,inplace=True)\n",
    "sparse_df.loc[0,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TextExplainer(random_state=42)\n",
    "te.fit(doc, pipe.predict_proba)\n",
    "te.show_prediction(target_names=twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(gb_model, doc=train.iloc[1145,0], ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stylistics)",
   "language": "python",
   "name": "stylistics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
