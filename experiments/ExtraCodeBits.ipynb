{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(doc):\n",
    "    content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "    doc_tokens = []\n",
    "\n",
    "    doc = nlp(doc, disable=['ner','textcat'])\n",
    "    return ['__{}__'.format(w.pos_) if w.pos_ in content_words else w.text.lower().strip() if w.pos_ == 'PRON' else w.lemma_.lower().strip() for w in doc]\n",
    "\n",
    "\n",
    "def spacy_pos_tagger(doc):\n",
    "    \n",
    "    doc = nlp(doc, disable=['ner','textcat'])\n",
    "    \n",
    "    return [w.pos_ for w in doc] \n",
    "\n",
    "def get_text_ngrams(col, tokenizer, ngram_range, max_features, scale_values):\n",
    "    \n",
    "    #Get text BoW n-grams\n",
    "\n",
    "    #Initialise vectoriser\n",
    "    count_vectorizer = CountVectorizer(tokenizer=tokenizer,  ngram_range=ngram_range, max_features=max_features)\n",
    "    \n",
    "    unscaled_text_ngrams = count_vectorizer.fit_transform(col)\n",
    "    \n",
    "    if scale_values:\n",
    "        print('Scaling values')\n",
    "        text_ngrams = StandardScaler().fit_transform(unscaled_text_ngrams.astype(float))\n",
    "    else:\n",
    "        text_ngrams = unscaled_text_ngrams\n",
    "    \n",
    "    ngram_columns = count_vectorizer.get_feature_names()\n",
    "    \n",
    "    print(text_ngrams.shape)\n",
    "    \n",
    "    return text_ngrams, ngram_columns\n",
    "\n",
    "def get_pos_ngrams(col, tokenizer, ngram_range, max_features, scale_values):\n",
    "    \n",
    "    #Get POS n-grams\n",
    "    pos_vectorizer = CountVectorizer(tokenizer=tokenizer, ngram_range=ngram_range, max_features=max_features)\n",
    "    \n",
    "    unscaled_pos_ngrams = pos_vectorizer.fit_transform(col)\n",
    "    \n",
    "    if scale_values:\n",
    "        print('Scaling values')\n",
    "        pos_ngrams = StandardScaler().fit_transform(pos_ngrams.astype(float))\n",
    "    else:\n",
    "        pos_ngrams = unscaled_pos_ngrams\n",
    "        \n",
    "    pos_ngram_columns = pos_vectorizer.get_feature_names()\n",
    "    \n",
    "    print(pos_ngrams.shape)\n",
    "    \n",
    "    return pos_ngrams, pos_ngram_columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_features(col, scale_values):\n",
    "    \n",
    "    features_df = pd.DataFrame(index=col.index)\n",
    "    content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "    \n",
    "    total_words = []\n",
    "    words_no_punct = []\n",
    "    words_no_punct_no_stop =[]\n",
    "    count_content_words=[]\n",
    "    count_stopwords=[]\n",
    "    avg_word_len = []\n",
    "    \n",
    "    \n",
    "    aoa_ratings_df = pd.read_csv(ratingsFolder/'AoA_Ratings.csv')\n",
    "    aoa_ratings = dict(zip(aoa_ratings_df.Word, aoa_ratings_df.AoA))\n",
    "    maturity_tokens = []\n",
    "    maturity = []\n",
    "\n",
    "    conc_ratings_df = pd.read_csv(ratingsFolder/'Concreteness_Ratings.csv')\n",
    "    conc_ratings = dict(zip(conc_ratings_df.Word, conc_ratings_df.Concreteness))\n",
    "    concreteness = []\n",
    "    conc_tokens = []    \n",
    "\n",
    "\n",
    "    for doc in nlp.pipe(col, batch_size=50, n_threads=4):\n",
    "\n",
    "        if doc.is_parsed:\n",
    "            \n",
    "            #Add placeholders for CONTENT words, else parse as usual\n",
    "            #tokens.append(['__{}__'.format(w.pos_) if w.pos_ in content_words else w.lemma_.lower() for w in doc])\n",
    "            \n",
    "            total_words.append(len([w.text for w in doc]))\n",
    "            words_no_punct.append(len([w.text for w in doc if not w.is_punct]))\n",
    "            words_no_punct_no_stop.append(len([w.text for w in doc if not w.is_punct if not w.is_stop]))\n",
    "            count_content_words.append(len([w.text for w in doc if w.pos_ in content_words]))\n",
    "            count_stopwords.append(len([w.text for w in doc if w.is_stop]))\n",
    "            avg_word_len.append(sum([len(w) for w in doc if not w.is_punct])/len([w for w in doc if not w.is_punct]))\n",
    "            \n",
    "            #-------------------Maturity\n",
    "            mat_tmp = []\n",
    "            maturity_tokens.append([w.text.lower().strip() if w.lemma_ == '-PRON-' else w.lemma_.lower().strip() for w in doc])\n",
    "\n",
    "            #maturity.append([value.get('name') for value in d.values()])\n",
    "            mat_tmp.append([aoa_ratings[t] for a in maturity_tokens for t in a if t in aoa_ratings])\n",
    "\n",
    "             #Now get avg maturity per doc\n",
    "            for j in mat_tmp:\n",
    "                maturity.append(sum(j)/len(j))\n",
    "\n",
    "            #----------Concreteness\n",
    "            conc_tmp = []\n",
    "            conc_tokens.append([w.text.lower().strip() for w in doc])\n",
    "            conc_tmp.append([conc_ratings[t] for a in conc_tokens for t in a if t in conc_ratings])\n",
    "\n",
    "            #Now get avg concreteness per doc\n",
    "            for k in conc_tmp:\n",
    "                concreteness.append(sum(k)/len(k))\n",
    "\n",
    "        else:\n",
    "            # We want to make sure that the lists of parsed results have the\n",
    "            # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "\n",
    "            #tokens.append(None)\n",
    "            total_words.append(None)\n",
    "            words_no_punct.append(None)\n",
    "            words_no_punct_no_stop.append(None)\n",
    "            count_content_words.append(None)\n",
    "            count_stopwords.append(None)\n",
    "            avg_word_len.append(None)\n",
    "            maturity.append(None)\n",
    "            concreteness.append(None)\n",
    "    \n",
    "    features_df['total_words'] = total_words\n",
    "    features_df['words_no_punct'] = words_no_punct\n",
    "    features_df['words_no_punct_no_stop'] = words_no_punct_no_stop\n",
    "    features_df['count_content_words'] = count_content_words\n",
    "    features_df['count_stopwords'] = count_stopwords\n",
    "    features_df['avg_word_len'] = avg_word_len\n",
    "    features_df['maturity'] = maturity\n",
    "    features_df['concreteness'] = concreteness\n",
    "    \n",
    "    col_names = ['total_words','words_no_punct','words_no_punct_no_stop','count_content_words',\n",
    "                 'count_stopwords','avg_word_len','maturity','concreteness']\n",
    "    \n",
    "    if scale_values:\n",
    "        scaled_features = StandardScaler().fit_transform(train[col_names].astype(float))\n",
    "        features_df = pd.DataFrame(scaled_features, index=col.index, columns=[col_names])\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get handcrafted features\n",
    "custom_features_df = get_custom_features(train['EssayText'], scale_values=False)\n",
    "custom_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get text based ngrams    \n",
    "text_ngrams, text_ngrams_columns = get_text_ngrams(train['EssayText'],spacy_tokenizer, (1,1), 10000, scale_values=False )    \n",
    "\n",
    "text_ngrams_df = pd.DataFrame(text_ngrams.toarray(), columns=text_ngrams_columns, index=train.index)\n",
    "text_ngrams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ngrams, pos_ngram_columns = get_pos_ngrams(train['EssayText'], spacy_pos_tagger, (1,1), 10000, scale_values=False)\n",
    "pos_ngrams_df = pd.DataFrame(pos_ngrams.toarray(), \n",
    "                             columns=pos_ngram_columns, \n",
    "                             index=train.index)\n",
    "\n",
    "pos_ngrams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the features\n",
    "merged_1 = pd.merge(custom_features_df, pos_ngrams_df, how='inner',left_index=True, right_index=True, copy=True)\n",
    "merged_2 = pd.merge(merged_1, text_ngrams_df, how='inner',left_index=True, right_index=True, copy=True)\n",
    "\n",
    "feature_names = merged_2.columns\n",
    "\n",
    "print(merged_2.shape)\n",
    "merged_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10,8))\n",
    "# ax = fig.add_subplot(111)\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "visualizer = FreqDistVisualizer(features=text_ngrams_columns, orient='v',n=30, color='green')\n",
    "visualizer.fit(text_ngrams)\n",
    "\n",
    "# Call finalize to draw the final yellowbrick-specific elements\n",
    "visualizer.finalize()\n",
    "#visualizer.poof()\n",
    "# Get access to the axes object and modify labels\n",
    "visualizer.ax.set_title(\"Frequency Distribution of Content Independent uni grams\")\n",
    "#visualizer.ax.set_xlabel(\"Xlabel\")\n",
    "#visualizer.ax.set_ylabel(\"yLabel\")\n",
    "\n",
    "#visualizer.ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(12,10))\n",
    "visualizer = FreqDistVisualizer(features=pos_ngram_columns, orient='v', n=16)\n",
    "visualizer.fit(pos_ngrams)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words per T-unit\n",
    "\n",
    "https://github.com/explosion/spaCy/issues/252\n",
    "\n",
    "http://mlreference.com/dependency-tree-spacy\n",
    "\n",
    "http://grammar.yourdictionary.com/grammar-rules-and-tips/grammar-clause.html\n",
    "\n",
    "https://shirishkadam.com/2016/12/23/dependency-parsing-in-nlp/\n",
    "\n",
    "https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc = (u'Apple is looking at buying U.K. startup for $1 billion. This is another sentence.')\n",
    "mydoc\n",
    "\n",
    "words_per_t_unit = []\n",
    "\n",
    "for doc in nlp.pipe(new_df['EssayText']):\n",
    "    tokens = []\n",
    "    words = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        #print(sent)\n",
    "        tokens.append([w.text.lower() for w in sent if w.pos_ not in ['PUNCT','SYM','X','SPACE']])\n",
    "        \n",
    "    \n",
    "    #Get number of words in a sentence\n",
    "    for i in tokens:\n",
    "        words.append(len(i))\n",
    "    \n",
    "    #Get avg words per sentence for the doc\n",
    "    words_per_t_unit.append(sum(words)/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stylistics)",
   "language": "python",
   "name": "stylistics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
