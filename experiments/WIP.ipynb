{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from spacy import displacy\n",
    "#from spacy.lang.en import English\n",
    "#parser = English()\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, cohen_kappa_score\n",
    "\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up display area to show dataframe in jupyter qtconsole\n",
    "\n",
    "#pd.set_option('display.height', 1000)\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "#pd.set_option('display.width', 1000)\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDir = Path.cwd().parents[0]\n",
    "dataFolder = myDir / 'data/asap-sas'\n",
    "ratingsFolder = myDir / 'data/ratings'\n",
    "\n",
    "print(dataFolder)\n",
    "\n",
    "gradeMap = {1: 10,\n",
    "                2: 10,\n",
    "                3: 10,\n",
    "                4:10,\n",
    "                5: 10,\n",
    "                6: 10,\n",
    "                7:10,\n",
    "                8:10,\n",
    "                9:10,\n",
    "                10:8}\n",
    "\n",
    "subjectMap = {1: 'Science',\n",
    "            2: 'Science',\n",
    "            3: 'English Language Arts',\n",
    "            4: 'English Language Arts',\n",
    "            5: 'Biology',\n",
    "            6: 'Biology',\n",
    "            7:'English',\n",
    "            8:'English',\n",
    "            9:'English',\n",
    "            10:'Science'}\n",
    "\n",
    "df = pd.read_csv(dataFolder/'train.tsv', sep='\\t', header=0)  #read data into dataframe\n",
    "df.drop('Score2', inplace=True, axis=1) #Score 2 is for inter-rate reliability only\n",
    "\n",
    "df['subject'] = df['EssaySet'].map(subjectMap)\n",
    "df['studentGrade'] = df['EssaySet'].map(gradeMap)\n",
    "\n",
    "df = df[['Id','EssaySet','subject','studentGrade','EssayText','Score1']] #rearrange columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only essay set 1\n",
    "set_1 = df[(df['EssaySet'] == 1)].copy()\n",
    "set_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EssayText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>You would need to know how much vinegar was put in to each sample, find the size and shape of the container so the same amount of vinegar was actually covering the sample. You would need to know the shape or volume of sample because the surface are has to be the same so the same amount is affected by the vinegar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>In order to replicate experiment I would need to know exactly how much vinegar to pour in each container, how much of each sample to put in the container, and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>The additional information you would need in order to replicate the experiment form a hypothesis. Draw a conclusion. Say what you are experimenting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>To replicate this experiment you would need to state you problem. What is the person for this lab also you need to indicate your independent and dependent variables.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1) I would need to know how much vinegar is being put into the sample. ^p 2) What are they trying to find out through pass is to amount. ^p 3) Was there a control group in this experiment.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                       EssayText\n",
       "1145  You would need to know how much vinegar was put in to each sample, find the size and shape of the container so the same amount of vinegar was actually covering the sample. You would need to know the shape or volume of sample because the surface are has to be the same so the same amount is affected by the vinegar.\n",
       "842   In order to replicate experiment I would need to know exactly how much vinegar to pour in each container, how much of each sample to put in the container, and                                                                                                                                                            \n",
       "1554  The additional information you would need in order to replicate the experiment form a hypothesis. Draw a conclusion. Say what you are experimenting.                                                                                                                                                                      \n",
       "1526  To replicate this experiment you would need to state you problem. What is the person for this lab also you need to indicate your independent and dependent variables.                                                                                                                                                     \n",
       "497   1) I would need to know how much vinegar is being put into the sample. ^p 2) What are they trying to find out through pass is to amount. ^p 3) Was there a control group in this experiment.                                                                                                                              "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = set_1[['EssayText','Score1']]\n",
    "y = X.pop('Score1')\n",
    "\n",
    "\n",
    "train, test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "#Adding the is_copy to False otherwise we get SettingWithCopyWarning\n",
    "train = train.copy()\n",
    "test = test.copy()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EssayText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>You would need to know how much vinegar was put in to each sample, find the size and shape of the container so the same amount of vinegar was actually covering the sample. You would need to know the shape or volume of sample because the surface are has to be the same so the same amount is affected by the vinegar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>In order to replicate experiment I would need to know exactly how much vinegar to pour in each container, how much of each sample to put in the container, and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>The additional information you would need in order to replicate the experiment form a hypothesis. Draw a conclusion. Say what you are experimenting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>To replicate this experiment you would need to state you problem. What is the person for this lab also you need to indicate your independent and dependent variables.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1) I would need to know how much vinegar is being put into the sample. ^p 2) What are they trying to find out through pass is to amount. ^p 3) Was there a control group in this experiment.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                       EssayText\n",
       "1145  You would need to know how much vinegar was put in to each sample, find the size and shape of the container so the same amount of vinegar was actually covering the sample. You would need to know the shape or volume of sample because the surface are has to be the same so the same amount is affected by the vinegar.\n",
       "842   In order to replicate experiment I would need to know exactly how much vinegar to pour in each container, how much of each sample to put in the container, and                                                                                                                                                            \n",
       "1554  The additional information you would need in order to replicate the experiment form a hypothesis. Draw a conclusion. Say what you are experimenting.                                                                                                                                                                      \n",
       "1526  To replicate this experiment you would need to state you problem. What is the person for this lab also you need to indicate your independent and dependent variables.                                                                                                                                                     \n",
       "497   1) I would need to know how much vinegar is being put into the sample. ^p 2) What are they trying to find out through pass is to amount. ^p 3) Was there a control group in this experiment.                                                                                                                              "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = train[['EssayText']][0:5]\n",
    "new_df = new_df.copy() \n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EssayText</th>\n",
       "      <th>total_words</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>You would need to know how much vinegar was put in to each sample, find the size and shape of the container so the same amount of vinegar was actually covering the sample. You would need to know the shape or volume of sample because the surface are has to be the same so the same amount is affected by the vinegar.</td>\n",
       "      <td>62</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>In order to replicate experiment I would need to know exactly how much vinegar to pour in each container, how much of each sample to put in the container, and</td>\n",
       "      <td>30</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>The additional information you would need in order to replicate the experiment form a hypothesis. Draw a conclusion. Say what you are experimenting.</td>\n",
       "      <td>23</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>To replicate this experiment you would need to state you problem. What is the person for this lab also you need to indicate your independent and dependent variables.</td>\n",
       "      <td>28</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1) I would need to know how much vinegar is being put into the sample. ^p 2) What are they trying to find out through pass is to amount. ^p 3) Was there a control group in this experiment.</td>\n",
       "      <td>39</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                       EssayText  \\\n",
       "1145  You would need to know how much vinegar was put in to each sample, find the size and shape of the container so the same amount of vinegar was actually covering the sample. You would need to know the shape or volume of sample because the surface are has to be the same so the same amount is affected by the vinegar.   \n",
       "842   In order to replicate experiment I would need to know exactly how much vinegar to pour in each container, how much of each sample to put in the container, and                                                                                                                                                               \n",
       "1554  The additional information you would need in order to replicate the experiment form a hypothesis. Draw a conclusion. Say what you are experimenting.                                                                                                                                                                         \n",
       "1526  To replicate this experiment you would need to state you problem. What is the person for this lab also you need to indicate your independent and dependent variables.                                                                                                                                                        \n",
       "497   1) I would need to know how much vinegar is being put into the sample. ^p 2) What are they trying to find out through pass is to amount. ^p 3) Was there a control group in this experiment.                                                                                                                                 \n",
       "\n",
       "      total_words  avg_word_length  \n",
       "1145  62           250              \n",
       "842   30           127              \n",
       "1554  23           123              \n",
       "1526  28           136              \n",
       "497   39           144              "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "tokens = []\n",
    "word_count = []\n",
    "tmp_word_len = []\n",
    "avg_word_len = []\n",
    "X_train = pd.DataFrame()\n",
    "\n",
    "\n",
    "for doc in nlp.pipe(new_df['EssayText'], batch_size=50, n_threads=4):\n",
    "        \n",
    "    if doc.is_parsed:\n",
    "        \n",
    "        #Add placeholders for CONTENT words, else parse as usual\n",
    "        tokens.append(['__{}__'.format(w.pos_) if w.pos_ in content_words else w.lemma_.lower() for w in doc])\n",
    "        \n",
    "        #Count words which are not punctuation\n",
    "        word_count.append(len([w for w in doc if not w.is_punct]))\n",
    "        \n",
    "        \n",
    "        #word_len.append(np.sum([len(w) for w in doc if not w.is_punct]))\n",
    "    \n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "\n",
    "        tokens.append(None)\n",
    "        word_count.append(None)\n",
    "        word_len.append(None)\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "new_df['total_words'] = word_count\n",
    "new_df['avg_word_length'] = word_len\n",
    "# X_train['lemmas'] = lemmas\n",
    "# X_train['sentences'] = sentences\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2580645161290325\n",
      "3.2\n",
      "3.3043478260869565\n",
      "3.4642857142857144\n",
      "3.051282051282051\n"
     ]
    }
   ],
   "source": [
    "for doc in nlp.pipe(new_df['EssayText'], batch_size=50, n_threads=4):\n",
    "    word_count =[]    \n",
    "    if doc.is_parsed:\n",
    "        \n",
    "        #Add placeholders for CONTENT words, else parse as usual\n",
    "        #tokens.append(['__{}__'.format(w.pos_) if w.pos_ in content_words else w.lemma_.lower() for w in doc])\n",
    "        \n",
    "        #Count words which are not punctuation\n",
    "        word_count.append([len(w.shape_) for w in doc if not w.is_punct])\n",
    "        \n",
    "        #print(word_count)\n",
    "        \n",
    "        for i in word_count:\n",
    "            print(sum(i)/len(i))\n",
    "        #word_len.append(np.sum([len(w) for w in doc if not w.is_punct]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "# new_df = train[['EssayText']][0:5]\n",
    "# new_df = new_df.copy() \n",
    "\n",
    "def get_maturity(col):\n",
    "    \n",
    "    aoa_ratings_df = pd.read_csv(ratingsFolder/'AoA_Ratings.csv')\n",
    "    aoa_ratings = dict(zip(aoa_ratings_df.Word, aoa_ratings_df.AoA))\n",
    "    \n",
    "    tokens = []\n",
    "    maturity = []\n",
    "    mat_tmp = []\n",
    "    \n",
    "    \n",
    "    for doc in nlp.pipe(col, batch_size=50, n_threads=4, disable=['ner']):\n",
    "\n",
    "        if doc.is_parsed:\n",
    "            #Add placeholders for CONTENT words, else parse as usual. If -PRON- then add actual word else lemma.\n",
    "            tokens.append([w.text.lower() if w.lemma_ == '-PRON-' else w.lemma_.lower() for w in doc])\n",
    "\n",
    "            #maturity.append([value.get('name') for value in d.values()])\n",
    "\n",
    "            mat_tmp.append([aoa_ratings[t] for a in tokens for t in a if t in aoa_ratings])\n",
    "    \n",
    "    #Now get avg maturity per doc\n",
    "    for i in mat_tmp:\n",
    "            avg = sum(i)/len(i)\n",
    "            maturity.append(avg)\n",
    "\n",
    "    return maturity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concreteness(col):\n",
    "    \n",
    "    conc_ratings_df = pd.read_csv(ratingsFolder/'Concreteness_Ratings.csv')\n",
    "    conc_ratings = dict(zip(conc_ratings_df.Word, conc_ratings_df.Concreteness))\n",
    "    \n",
    "    tokens = []\n",
    "    concreteness = []\n",
    "    conc_tmp = []\n",
    "    \n",
    "    \n",
    "    for doc in nlp.pipe(col, batch_size=50, n_threads=4, disable=['ner']):\n",
    "\n",
    "        if doc.is_parsed:\n",
    "            \n",
    "            tokens.append([w.text.lower() for w in doc])\n",
    "            conc_tmp.append([conc_ratings[t] for a in tokens for t in a if t in conc_ratings])\n",
    "    \n",
    "    #Now get avg concreteness per doc\n",
    "    for i in conc_tmp:\n",
    "        avg = sum(i)/len(i)\n",
    "        concreteness.append(avg)\n",
    "\n",
    "    return concreteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_t_unit = []\n",
    "\n",
    "for doc in nlp.pipe(new_df['EssayText']):\n",
    "    tokens = []\n",
    "    words = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        #print(sent)\n",
    "        tokens.append([w.text.lower() for w in sent if w.pos_ not in ['PUNCT','SYM','X','SPACE']])\n",
    "        \n",
    "    \n",
    "    #Get number of words in a sentence\n",
    "    for i in tokens:\n",
    "        words.append(len(i))\n",
    "    \n",
    "    #Get avg words per sentence for the doc\n",
    "    words_per_t_unit.append(sum(words)/len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW, N-grams (Tokens + POS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(doc):\n",
    "    content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "    \n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    #remove ^p (bullet points)\n",
    "    \n",
    "    return ['__{}__'.format(w.pos_) if w.pos_ in content_words else w.lemma_.lower() for w in doc]  \n",
    "    \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(2,3), max_features=2000)\n",
    "X_train_counts = vectorizer.fit_transform(text)\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "#print(count_vect.vocabulary_)\n",
    "pd.DataFrame(X_train_counts.toarray(), columns=vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos_tagger(doc):\n",
    "    content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "    \n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    #remove ^p (bullet points)\n",
    "    \n",
    "    return ['__{}__'.format(w.pos_) if w.pos_ in content_words else w.pos_ for w in doc]  \n",
    "    \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), max_features=200, tokenizer=spacy_pos_tagger)\n",
    "X_train_counts = vectorizer.fit_transform(train.clean_text)\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "pd.DataFrame(X_train_counts.toarray(), columns=vectorizer.get_feature_names()).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "tokens = []\n",
    "sentences = []\n",
    "word_count = []\n",
    "word_len = []\n",
    "tf_text = []\n",
    "\n",
    "for doc in nlp.pipe(new_df['EssayText'], batch_size=50, n_threads=4):\n",
    "        \n",
    "    if doc.is_parsed:\n",
    "        #Add placeholders for CONTENT words, else parse as usual\n",
    "        tokens.append(['__{}__'.format(w.pos_) if w.pos_ in content_words else w.lemma_.lower() for w in doc])\n",
    "        #tf_text.append((' '.join(t for t in tokens)))\n",
    "         \n",
    "        \n",
    "#         sentences.append([sent.text for sent in doc.sents])\n",
    "        #word_count.append(len([w for w in doc if not w.is_punct]))\n",
    "        word_len.append(np.sum([len(w) for w in doc if not w.is_punct]))\n",
    "    \n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        #pos.append(None)        \n",
    "#         sentences.append(None)\n",
    "\n",
    "    \n",
    "#train['tokens'] = tokens\n",
    "#train['pos'] = pos\n",
    "#train['total_words'] = word_count\n",
    "#train['avg_word_length'] = word_len\n",
    "# X_train['lemmas'] = lemmas\n",
    "# X_train['sentences'] = sentences\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_text'] = tf_text\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_features(df, col):\n",
    "    \n",
    "    content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "    tokens = []\n",
    "    pos = []\n",
    "    #sentences = []\n",
    "    word_count = []\n",
    "    word_len = []\n",
    "    #tf_text = []\n",
    "    \n",
    "    for doc in nlp.pipe(df[col], batch_size=50, n_threads=4, disable=['ner']):\n",
    "        \n",
    "        if doc.is_parsed:\n",
    "            #Add placeholders for CONTENT words, else parse as usual\n",
    "            tokens.append(['__{}__'.format(w.pos_) if w.pos_ in content_words else w.lemma_.lower() for w in doc])\n",
    "            \n",
    "            #tf_text.append('__{}__'.format(w.pos_) if w.pos_ in content_words else w.lemma_.lower() for w in doc)\n",
    "        \n",
    "            #pos.append([n.pos_ for n in doc])\n",
    "            #sentences.append([sent.text for sent in doc.sents])\n",
    "            \n",
    "            word_count.append(len([w for w in doc if not w.is_punct]))\n",
    "            word_len.append(np.sum([len(w) for w in doc if not w.is_punct]))\n",
    "    \n",
    "        else:\n",
    "            \n",
    "            # We want to make sure that the lists of parsed results have the\n",
    "            # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "\n",
    "            tokens.append(None)\n",
    "            pos.append(None)        \n",
    "            #sentences.append(None)\n",
    "            word_count.append(None)\n",
    "            word_len.append(None)\n",
    "    \n",
    "     \n",
    "    #df['tokens'] = tokens\n",
    "    #df['pos'] = pos\n",
    "    df['total_words'] = word_count\n",
    "    df['avg_word_length'] = word_len\n",
    "    #df['lemmas'] = lemmas\n",
    "    #df['sentences'] = sentences\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_numeric_features(train, \"EssayText\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(doc):\n",
    "    content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "    \n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    #remove ^p (bullet points)\n",
    "    \n",
    "    return ['__{}__'.format(w.pos_) if w.pos_ in content_words else w.lemma_.lower() for w in doc]  \n",
    "    \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(2,3), max_features=2000)\n",
    "X_train_counts = vectorizer.fit_transform(text)\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "#print(count_vect.vocabulary_)\n",
    "pd.DataFrame(X_train_counts.toarray(), columns=vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos_tagger(doc):\n",
    "    content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "    \n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    #remove ^p (bullet points)\n",
    "    \n",
    "    return ['__{}__'.format(w.pos_) if w.pos_ in content_words else w.pos_ for w in doc]  \n",
    "    \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), max_features=200, tokenizer=spacy_pos_tagger)\n",
    "X_train_counts = vectorizer.fit_transform(train.clean_text)\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "pd.DataFrame(X_train_counts.toarray(), columns=vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,3), max_features=200, tokenizer=spacy_pos_tagger)\n",
    "X_train_counts = vectorizer.fit_transform(train.clean_text)\n",
    "X_train_counts.shape\n",
    "\n",
    "#print(count_vect.vocabulary_)\n",
    "pd.DataFrame(X_train_counts.toarray(), columns=vectorizer.get_feature_names()).head(10)\n",
    "\n",
    "################################################\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(2,3), max_features=2000)\n",
    "X_train_counts = vectorizer.fit_transform(text)\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "#print(count_vect.vocabulary_)\n",
    "pd.DataFrame(X_train_counts.toarray(), columns=vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maturity, Concreteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content_words = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']\n",
    "# new_df = train[['EssayText']][0:5]\n",
    "# new_df = new_df.copy() \n",
    "\n",
    "def get_maturity(col):\n",
    "    \n",
    "    aoa_ratings_df = pd.read_csv(ratingsFolder/'AoA_Ratings.csv')\n",
    "    aoa_ratings = dict(zip(aoa_ratings_df.Word, aoa_ratings_df.AoA))\n",
    "    \n",
    "    tokens = []\n",
    "    maturity = []\n",
    "    mat_tmp = []\n",
    "    \n",
    "    \n",
    "    for doc in nlp.pipe(col, batch_size=50, n_threads=4, disable=['ner']):\n",
    "\n",
    "        if doc.is_parsed:\n",
    "            #Add placeholders for CONTENT words, else parse as usual. If -PRON- then add actual word else lemma.\n",
    "            tokens.append([w.text.lower() if w.lemma_ == '-PRON-' else w.lemma_.lower() for w in doc])\n",
    "\n",
    "            #maturity.append([value.get('name') for value in d.values()])\n",
    "\n",
    "            mat_tmp.append([aoa_ratings[t] for a in tokens for t in a if t in aoa_ratings])\n",
    "    \n",
    "    #Now get avg maturity per doc\n",
    "    for i in mat_tmp:\n",
    "            avg = sum(i)/len(i)\n",
    "            maturity.append(avg)\n",
    "\n",
    "    return maturity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_maturity(new_df['EssayText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concreteness(col):\n",
    "    \n",
    "    conc_ratings_df = pd.read_csv(ratingsFolder/'Concreteness_Ratings.csv')\n",
    "    conc_ratings = dict(zip(conc_ratings_df.Word, conc_ratings_df.Concreteness))\n",
    "    \n",
    "    tokens = []\n",
    "    concreteness = []\n",
    "    conc_tmp = []\n",
    "    \n",
    "    \n",
    "    for doc in nlp.pipe(col, batch_size=50, n_threads=4, disable=['ner']):\n",
    "\n",
    "        if doc.is_parsed:\n",
    "            \n",
    "            tokens.append([w.text.lower() for w in doc])\n",
    "            conc_tmp.append([conc_ratings[t] for a in tokens for t in a if t in conc_ratings])\n",
    "    \n",
    "    #Now get avg concreteness per doc\n",
    "    for i in conc_tmp:\n",
    "        avg = sum(i)/len(i)\n",
    "        concreteness.append(avg)\n",
    "\n",
    "    return concreteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_concreteness(new_df['EssayText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words per T-unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "word_count = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc = (u'Apple is looking at buying U.K. startup for $1 billion. This is another sentence.')\n",
    "mydoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "word_count = []\n",
    "\n",
    "for doc in nlp.pipe(new_df['EssayText']):\n",
    "    tokens = []\n",
    "    words = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        #print(sent)\n",
    "        tokens.append([w.text.lower() for w in sent if w.pos_ not in ['PUNCT','SYM','X','SPACE']])\n",
    "        \n",
    "    \n",
    "    #Get number of words in a sentence\n",
    "    for i in tokens:\n",
    "        words.append(len(i))\n",
    "    \n",
    "    #Get avg words per sentence for the doc\n",
    "    word_count.append(sum(words)/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stylistics)",
   "language": "python",
   "name": "stylistics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
